{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> configuring model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "class TripletEmbeddingCriterion(nn.Module):\n",
    "    def __init__(self, margin=0.5, gamma=2):\n",
    "        super(TripletEmbeddingCriterion, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        N = anchor.size(0)\n",
    "        \n",
    "        delta_pos = anchor - positive\n",
    "        delta_neg = anchor - negative\n",
    "\n",
    "        norm_delta_pos = torch.norm(delta_pos, p=2, dim=1)\n",
    "        norm_delta_neg = torch.norm(delta_neg, p=2, dim=1)\n",
    "\n",
    "        norm_delta_pos = norm_delta_pos * norm_delta_pos * self.gamma\n",
    "        norm_delta_neg = norm_delta_neg * norm_delta_neg\n",
    "\n",
    "        delta_pos_neg = norm_delta_pos - norm_delta_neg + self.margin\n",
    "\n",
    "        loss = F.relu(delta_pos_neg)\n",
    "        return loss.mean()\n",
    "\n",
    "    def backward(self, anchor, positive, negative):\n",
    "        N = anchor.size(0)\n",
    "        \n",
    "        delta_pos = anchor - positive\n",
    "        delta_neg = anchor - negative\n",
    "\n",
    "        norm_delta_pos = torch.norm(delta_pos, p=2, dim=1)\n",
    "        norm_delta_neg = torch.norm(delta_neg, p=2, dim=1)\n",
    "\n",
    "        norm_delta_pos = norm_delta_pos * norm_delta_pos * self.gamma\n",
    "        norm_delta_neg = norm_delta_neg * norm_delta_neg\n",
    "\n",
    "        delta_pos_neg = norm_delta_pos - norm_delta_neg + self.margin\n",
    "\n",
    "        mask = (delta_pos_neg > 0).float().view(-1, 1)\n",
    "\n",
    "        grad_anchor = mask * (delta_neg - delta_pos * self.gamma) * (2 / N)\n",
    "        grad_positive = mask * (delta_pos * self.gamma) * (-2 / N)\n",
    "        grad_negative = mask * delta_neg * (2 / N)\n",
    "\n",
    "        return grad_anchor, grad_positive, grad_negative\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "torch_fix_seed()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AggClustering:\n",
    "    def __init__(self):\n",
    "        self.K_c = 5\n",
    "\n",
    "    def init(self, indices):\n",
    "        # nsamples = indices.size(0)\n",
    "        # visited = torch.full((nsamples, 1), -1, dtype=torch.int32)\n",
    "        # count = 0\n",
    "        # for i in range(nsamples):\n",
    "        #     cur_idx = i\n",
    "        #     pos = []\n",
    "        #     while visited[cur_idx, 0] == -1:\n",
    "        #         pos.append(cur_idx)\n",
    "        #         neighbor = 0\n",
    "        #         for k in range(indices.size(0)):\n",
    "        #             neighbor = indices[cur_idx, k].item()\n",
    "        #             if cur_idx != neighbor:\n",
    "        #                 break\n",
    "        #         visited[cur_idx, 0] = -2\n",
    "        #         cur_idx = neighbor\n",
    "        #         if len(pos) > 50:\n",
    "        #             break\n",
    "        #     if visited[cur_idx, 0] < 0:\n",
    "        #         visited[cur_idx, 0] = count\n",
    "        #         count += 1\n",
    "        #     for p in pos:\n",
    "        #         visited[p, 0] = visited[cur_idx, 0]\n",
    "        # label_indice = [[] for _ in range(count)]\n",
    "        # for i in range(nsamples):\n",
    "        #     label_indice[visited[i, 0]].append(i)\n",
    "        # return label_indice\n",
    "\n",
    "        # # Initialize labels for input data given KNN indices\n",
    "        # nsamples = indices.size(0)\n",
    "        # k = indices.size(1)\n",
    "        # visited = torch.full((nsamples, 1), -1, dtype=torch.int)\n",
    "        # count = 0\n",
    "        \n",
    "        # for i in range(nsamples):\n",
    "        #     cur_idx = i\n",
    "        #     pos = []\n",
    "        #     while visited[cur_idx][0] == -1:\n",
    "        #         pos.append(cur_idx)\n",
    "        #         neighbor = 0\n",
    "        #         for k_idx in range(indices[cur_idx].size(0)):\n",
    "        #             neighbor = indices[cur_idx][k_idx].item()\n",
    "        #             if cur_idx != neighbor:\n",
    "        #                 break\n",
    "        #         visited[cur_idx][0] = -2\n",
    "        #         cur_idx = neighbor\n",
    "        #         if len(pos) > 50:\n",
    "        #             break\n",
    "            \n",
    "        #     if visited[cur_idx][0] < 0:\n",
    "        #         visited[cur_idx][0] = count\n",
    "        #         count += 1\n",
    "            \n",
    "        #     for j in pos:\n",
    "        #         visited[j][0] = visited[cur_idx][0]\n",
    "        \n",
    "        # label_indices = [[] for _ in range(count)]\n",
    "        \n",
    "        # for i in range(nsamples):\n",
    "        #     label_indices[visited[i][0]].append(i)\n",
    "        \n",
    "        # for i in range(count):\n",
    "        #     if len(label_indices[i]) == 0:\n",
    "        #         print(\"error\")\n",
    "        # print(label_indices)\n",
    "\n",
    "        # サンプル間の距離行列を計算\n",
    "        nsamples = indices\n",
    "        distances = squareform(pdist(nsamples, 'euclidean'))\n",
    "\n",
    "        # 距離の上三角行列をフラット化し、ソートして最も距離が短い3ペアを見つける\n",
    "        triu_indices = np.triu_indices_from(distances, k=1)\n",
    "        sorted_distances_indices = np.argsort(distances[triu_indices])\n",
    "        shortest_3_pairs_indices = sorted_distances_indices[:3]\n",
    "\n",
    "        # 最も距離が短い3ペアのインデックスを取得\n",
    "        shortest_3_pairs = list(zip(triu_indices[0][shortest_3_pairs_indices], triu_indices[1][shortest_3_pairs_indices]))\n",
    "\n",
    "        # 全サンプルのインデックスのセットを作成\n",
    "        all_indices = set(range(len(nsamples)))\n",
    "\n",
    "        # グループ化する\n",
    "        label_indices = []\n",
    "        grouped_indices = set()\n",
    "        \n",
    "        for pair in shortest_3_pairs:\n",
    "            if pair[0] not in grouped_indices and pair[1] not in grouped_indices:\n",
    "                group = list(pair)\n",
    "                label_indices.append(group)\n",
    "                grouped_indices.update(pair)\n",
    "            else:\n",
    "                for group in label_indices:\n",
    "                    if pair[0] in group or pair[1] in group:\n",
    "                        group.extend([idx for idx in pair if idx not in group])\n",
    "                        grouped_indices.update(pair)\n",
    "                        break\n",
    "\n",
    "        # 残りの単一要素をリストに追加\n",
    "        remaining_indices = all_indices - grouped_indices\n",
    "        for index in remaining_indices:\n",
    "            label_indices.append([index])\n",
    "\n",
    "        print(label_indices)\n",
    "        return label_indices\n",
    "\n",
    "    # def merge_two_clusters(self, W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "    #     A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "    #     nsamples_c_a = len(Y_t[idx_c_a])\n",
    "    #     nsamples_c_b = len(Y_t[idx_c_b])\n",
    "    #     ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "    #     A_us_t[idx_c_a, :] *= ratio\n",
    "    #     A_us_t[idx_c_b, :] *= (1 - ratio)\n",
    "    #     A_us_t[idx_c_a, :] += A_us_t[idx_c_b, :]\n",
    "    #     A_us_t[idx_c_a, idx_c_a] = 0\n",
    "    #     A_us_t[:, idx_c_b] = 0\n",
    "    #     A_us_t[idx_c_b, :] = 0\n",
    "    #     Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "    #     Y_t[idx_c_b] = []\n",
    "    #     for i in range(len(Y_t)):\n",
    "    #         if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = 0\n",
    "    #             A_s_t[idx_c_a, i] = 0\n",
    "    #         elif i < idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "    #         elif i > idx_c_a:\n",
    "    #             A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "    #     return A_s_t, A_us_t, Y_t\n",
    "\n",
    "\n",
    "    def merge_two_clusters(W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "        nclusters = len(Y_t)\n",
    "\n",
    "        idx_c_a_tensor = torch.tensor([idx_c_a], dtype=torch.long)\n",
    "        idx_c_b_tensor = torch.tensor([idx_c_b], dtype=torch.long)\n",
    "\n",
    "        A_us_t.index_add_(1, idx_c_a_tensor, A_us_t.index_select(1, idx_c_b_tensor))\n",
    "\n",
    "        nsamples_c_a = len(Y_t[idx_c_a])\n",
    "        nsamples_c_b = len(Y_t[idx_c_b])\n",
    "        ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "\n",
    "        A_us_t[idx_c_a, :] *= ratio\n",
    "        A_us_t[idx_c_b, :] *= (1 - ratio)\n",
    "        A_us_t.index_add_(0, idx_c_a_tensor, A_us_t.index_select(0, idx_c_b_tensor))\n",
    "\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    # def search_clusters(self, A_s_t):\n",
    "    #     A_sorted, idx_sort = torch.sort(A_s_t, dim=1, descending=True)\n",
    "    #     aff = torch.zeros(1, A_sorted.size(1))\n",
    "    #     for i in range(A_sorted.size(1)):\n",
    "    #         aff[0, i] = A_sorted[0, i]\n",
    "    #         if A_sorted.size(1) > 100:\n",
    "    #             for k in range(1, self.K_c):\n",
    "    #                 aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "    #     v_c, idx_c = torch.max(aff, 1)\n",
    "    #     idx_c_b = idx_c.item()\n",
    "    #     idx_c_a = idx_sort[0, idx_c_b].item()\n",
    "    #     if idx_c_a == idx_c_b:\n",
    "    #         raise ValueError(\"Error: idx_c_a == idx_c_b\")\n",
    "    #     if idx_c_a > idx_c_b:\n",
    "    #         idx_c_a, idx_c_b = idx_c_b, idx_c_a\n",
    "    #     return idx_c_a, idx_c_b\n",
    "\n",
    "    def search_clusters(self, A_s_t):\n",
    "        # print(\"cluster numbers:\", nclusters)\n",
    "        nclusters = A_s_t.size(0)  # クラスタの数を取得\n",
    "        A_sorted, idx_sort = torch.sort(A_s_t, dim=0, descending=True)\n",
    "        # print(\"A_s_t: \", A_s_t.size())\n",
    "        # print(\"nclusters: \", nclusters)\n",
    "        # print(\"A_sorted: \", A_sorted)\n",
    "        # print(\"idx_sort: \", idx_sort)\n",
    "        aff = torch.zeros(1, A_sorted.size(1), dtype=torch.float32)\n",
    "\n",
    "        for i in range(A_sorted.size(1)):\n",
    "            aff[0, i] = A_sorted[0, i]\n",
    "            if A_sorted.size(1) > 100:\n",
    "                for k in range(1, self.K_c):\n",
    "                    aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "\n",
    "        v_c, idx_c = torch.max(aff, dim=1)  # each row\n",
    "        # print(\"idx_c: \", idx_c)\n",
    "        # find corresponding cluster labels for two clusters\n",
    "        idx_c_b = idx_c[0].item()         # col\n",
    "        idx_c_a = idx_sort[0, idx_c_b].item()        # row\n",
    "\n",
    "        # インデックスの検証と調整\n",
    "        if idx_c_a >= nclusters or idx_c_b >= nclusters:\n",
    "            raise ValueError(f\"Error: idx_c_a ({idx_c_a}) or idx_c_b ({idx_c_b}) is out of range (nclusters: {nclusters})\")\n",
    "        if idx_c_a == idx_c_b:\n",
    "            print(\"error\")\n",
    "            raise ValueError(\"idx_c_a and idx_c_b are the same\")\n",
    "        elif idx_c_a > idx_c_b:\n",
    "            idx_c_a, idx_c_b = idx_c_b, idx_c_a\n",
    "\n",
    "        return idx_c_a, idx_c_b\n",
    "\n",
    "    # def search_clusters(self,A_s_t):\n",
    "    #     # Sort the tensor along the first dimension in descending order\n",
    "    #     A_sorted, Idx_sort = torch.sort(A_s_t, dim=0, descending=True)\n",
    "        \n",
    "    #     # Initialize affinity tensor\n",
    "    #     aff = torch.zeros(1, A_sorted.size(1))\n",
    "        \n",
    "    #     for i in range(A_sorted.size(1)):\n",
    "    #         aff[0, i] = A_sorted[0, i]\n",
    "    #         if A_sorted.size(1) > 100:\n",
    "    #             for k in range(1, self.K_c ):  # Adjusting index for Python's 0-based indexing\n",
    "    #                 aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "        \n",
    "    #     # Find the maximum value in the affinity tensor along the second dimension\n",
    "    #     v_c, idx_c = torch.max(aff, dim=1)\n",
    "        \n",
    "    #     # Find corresponding cluster labels for two clusters\n",
    "    #     idx_c_b = idx_c.item()  # Converting tensor to integer\n",
    "    #     idx_c_a = Idx_sort[0, idx_c_b].item()  # Converting tensor to integer\n",
    "        \n",
    "    #     if idx_c_a == idx_c_b:\n",
    "    #         print(\"error\")\n",
    "    #         raise ValueError(\"Cluster indices are equal, which indicates an error.\")\n",
    "    #     elif idx_c_a > idx_c_b:\n",
    "    #         idx_c_a, idx_c_b = idx_c_b, idx_c_a  # Swap values\n",
    "        \n",
    "    #     return idx_c_a, idx_c_b\n",
    "\n",
    "\n",
    "    def run_step(self, W, A_s_t, A_us_t, Y_t):\n",
    "        nclusters = len(Y_t)\n",
    "        # print(\"Cluster Num: \", nclusters)\n",
    "        # print(\"numc\",A_s_t.size(0))\n",
    "        idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "        A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "        for i in range(len(Y_t)):\n",
    "            if len(Y_t[i]) > 0 and i != idx_c_a:\n",
    "                W_i = W[Y_t[i], :]\n",
    "                W_i_idx_c_a = W_i[:, Y_t[idx_c_a]]\n",
    "                W_idx_c_a = W[Y_t[idx_c_a], :]\n",
    "                W_idx_c_a_i = W_idx_c_a[:, Y_t[i]]\n",
    "                A_us_t[idx_c_a, i] = torch.sum(torch.mm(W_idx_c_a_i, W_i_idx_c_a))\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "        A_s_t[:, idx_c_b] = 0\n",
    "        A_s_t[idx_c_b, :] = 0\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    # def run_step_fast(self,W, A_s_t, A_us_t, Y_t):\n",
    "    #     # timer = torch.Timer()\n",
    "    #     # get the number of clusters\n",
    "    #     nclusters = len(Y_t)\n",
    "    #     print(\"Cluster Num: \", nclusters)\n",
    "    #     print(\"numc\",A_s_t.size(0))\n",
    "    #     # print(\"Cluster Num: \", nclusters)\n",
    "    #     # find maximal value in A_t\n",
    "    #     idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "    #     # update affinity matrix A_t\n",
    "    #     # update A_t(idx_c_a->i) = A_t(idx_c_a->i) + A_t(idx_c_b->i)\n",
    "    #     A_us_t.index_add_(1, torch.LongTensor([idx_c_a]), A_us_t.index_select(1, torch.LongTensor([idx_c_b])))\n",
    "\n",
    "    #     # update A_t(i->idx_c_a) = r_a * A_t(i->idx_c_a) + r_b * A_t(i->idx_c_b) (fast algorithm)\n",
    "    #     # nsamples in cluster idx_c_a\n",
    "    #     A_us_t.index_add_(0, torch.LongTensor([idx_c_a]), A_us_t.index_select(0, torch.LongTensor([idx_c_b])))\n",
    "        \n",
    "    #     # update cluster labels Y_t   \n",
    "    #     print(\"y_t: \", Y_t)\n",
    "    #     print(\"idx_c_a: \", idx_c_a)\n",
    "    #     print(\"idx_c_b: \", idx_c_b)\n",
    "    #     Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "    #     Y_t[idx_c_b] = []\n",
    "        \n",
    "    #     # update A_s_t   \n",
    "    #     for i in range(nclusters):\n",
    "    #         if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = 0\n",
    "    #             A_s_t[idx_c_a, i] = 0\n",
    "    #         elif i < idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] =  A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "    #         elif i > idx_c_a:\n",
    "    #             A_s_t[idx_c_a, i] =  A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "    #     # print(A_us_t.size())\n",
    "    #     # print(nclusters)\n",
    "    #     if idx_c_b != nclusters:\n",
    "    #         # print(idx_c_b)\n",
    "    #         # print(A_us_t.index_select(0, torch.LongTensor([1])))\n",
    "    #         A_us_t.index_copy_(0, torch.LongTensor([idx_c_b]), A_us_t.index_select(0, torch.LongTensor([nclusters-1])))\n",
    "    #         A_us_t.index_copy_(1, torch.LongTensor([idx_c_b]), A_us_t.index_select(1, torch.LongTensor([nclusters-1])))\n",
    "    #         A_us_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "    #         # print(\"Pre: \", A_s_t[:idx_c_b+1, idx_c_b])\n",
    "    #         # print(\"Pre: \", A_s_t[idx_c_b, idx_c_b:nclusters])\n",
    "    #         A_s_t[:idx_c_b+1, idx_c_b] = A_s_t[:idx_c_b+1, nclusters-1]      \n",
    "    #         A_s_t[idx_c_b, idx_c_b:nclusters] = A_s_t[idx_c_b:nclusters, nclusters-1].t()\n",
    "    #         A_s_t[idx_c_b, idx_c_b] = 0\n",
    "    #         # print(\"Cur: \", A_s_t[:idx_c_b+1, idx_c_b])\n",
    "    #         # print(\"Cur: \", A_s_t[idx_c_b, idx_c_b:nclusters])\n",
    "\n",
    "    #         Y_t[idx_c_b].extend(Y_t[nclusters-1])\n",
    "\n",
    "    #     A_us_t = A_us_t[:nclusters-1, :nclusters-1]\n",
    "    #     A_s_t = A_s_t[:nclusters-1, :nclusters-1]\n",
    "    #     del Y_t[nclusters-1]   \n",
    "    #     # print(Y_t)\n",
    "    #     # timer = torch.Timer()\n",
    "    #     # print('Time-2 elapsed: ' .. timer:time().real .. ' seconds')\n",
    "    #     # return updated A_s_t, A_us_t and Y_t\n",
    "    #     return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    def run_step_fast(self, W, A_s_t, A_us_t, Y_t):\n",
    "        nclusters = len(Y_t)\n",
    "        idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "\n",
    "        A_us_t.index_add_(1, torch.LongTensor([idx_c_a]), A_us_t.index_select(1, torch.LongTensor([idx_c_b])))\n",
    "        A_us_t.index_add_(0, torch.LongTensor([idx_c_a]), A_us_t.index_select(0, torch.LongTensor([idx_c_b])))\n",
    "\n",
    "        # print(\"y_t: \", Y_t)\n",
    "        # print(\"idx_c_a: \", idx_c_a)\n",
    "        # print(\"idx_c_b: \", idx_c_b)\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        if idx_c_b != nclusters - 1:\n",
    "            A_us_t.index_copy_(0, torch.LongTensor([idx_c_b]), A_us_t.index_select(0, torch.LongTensor([nclusters-1])))\n",
    "            A_us_t.index_copy_(1, torch.LongTensor([idx_c_b]), A_us_t.index_select(1, torch.LongTensor([nclusters-1])))\n",
    "            A_us_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "            A_s_t[:idx_c_b+1, idx_c_b] = A_s_t[:idx_c_b+1, nclusters-1].clone()\n",
    "            A_s_t[idx_c_b, idx_c_b:nclusters] = A_s_t[idx_c_b:nclusters, nclusters-1].clone().t()\n",
    "            A_s_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "            Y_t[idx_c_b].extend(Y_t[nclusters-1])\n",
    "\n",
    "        # Update the size of A_s_t and A_us_t to match the new number of clusters\n",
    "        A_us_t = A_us_t[:nclusters-1, :nclusters-1]\n",
    "        A_s_t = A_s_t[:nclusters-1, :nclusters-1]\n",
    "        del Y_t[nclusters-1]\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, W, A_unsym_0, A_sym_0, Y_0, T, K_c_in, use_fast):\n",
    "        nclusters = len(Y_0)\n",
    "        A_sym_0_sum = torch.sum(A_sym_0, dim=1)\n",
    "        self.K_c = K_c_in\n",
    "        t = 0\n",
    "        while t < T:\n",
    "            if use_fast:\n",
    "                A_sym_0, A_unsym_0, Y_0 = self.run_step_fast(W, A_sym_0, A_unsym_0, Y_0)\n",
    "            else:\n",
    "                A_sym_0, A_unsym_0, Y_0 = self.run_step(W, A_sym_0, A_unsym_0, Y_0)\n",
    "            t += 1\n",
    "        Y_T = [cluster for cluster in Y_0 if len(cluster) > 0]\n",
    "        return Y_T\n",
    "\n",
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "\n",
    "class Affinity:\n",
    "    def compute(self, X, k):\n",
    "        #print(\"Xtype\", X.dtype)\n",
    "        #print(\"k\", k)\n",
    "        if X.size(0) > 50000:\n",
    "            ind = torch.arange(1, X.size(0) + 1).long().split(10000)\n",
    "            dists = torch.zeros(X.size(0), k + 1, dtype=X.dtype)\n",
    "            indices = torch.zeros(X.size(0), k + 1, dtype=torch.int)\n",
    "\n",
    "            for v in ind:\n",
    "                nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X.detach().numpy())\n",
    "                dists_batch, indices_batch = nbrs.kneighbors(X[v - 1].detach().numpy())\n",
    "                dists[v - 1] = torch.tensor(dists_batch)\n",
    "                indices[v - 1] = torch.tensor(indices_batch)\n",
    "        else:\n",
    "            #print('now')\n",
    "            nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X.detach().numpy())\n",
    "            dists, indices = nbrs.kneighbors(X.detach().numpy())\n",
    "            dists = torch.tensor(dists)\n",
    "            indices = torch.tensor(indices)\n",
    "\n",
    "        sigma_square = torch.mean(dists[:, 1:k+1])\n",
    "        print(\"sigma:\", torch.sqrt(sigma_square))\n",
    "\n",
    "        nsamples = X.size(0)\n",
    "        W = torch.zeros(nsamples, nsamples)\n",
    "\n",
    "        for i in range(nsamples):\n",
    "            for j in range(1, k + 1):\n",
    "                nn_ind = indices[i][j]\n",
    "                W[i][nn_ind] = torch.exp(-dists[i][j] / sigma_square)\n",
    "        \n",
    "        return dists, indices, W\n",
    "\n",
    "    def compute4cluster(self, X, W, Y_0, k, k_target, min_clusters):\n",
    "        nclusters = len(Y_0)\n",
    "        dim = X.size(1)\n",
    "        X_clusters = torch.zeros(nclusters, dim)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            X_clusters[i] = torch.mean(X[torch.LongTensor(Y_0[i])], dim=0)\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_clusters.detach().numpy())\n",
    "        dists, indices = nbrs.kneighbors(X_clusters.detach().numpy())\n",
    "        dists = torch.tensor(dists)\n",
    "        indices = torch.tensor(indices)\n",
    "\n",
    "        NNs = torch.zeros(nclusters, nclusters)\n",
    "        # print(indices.size())\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(1, indices.size(1)):\n",
    "                nn_ind = indices[i][j]\n",
    "                NNs[i][nn_ind] = 1\n",
    "\n",
    "        max_number = max(len(y) for y in Y_0)\n",
    "        Y_0_tensor = torch.zeros(nclusters, max_number)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(len(Y_0[i])):\n",
    "                Y_0_tensor[i][j] = Y_0[i][j]\n",
    "\n",
    "        timer = time.time()\n",
    "        A_unsym_0_c, A_sym_0_c = self.compute_CAff(W, NNs, Y_0_tensor)\n",
    "\n",
    "        if k > 20 * k_target:\n",
    "            A_unsym_0_c = A_unsym_0_c.double()\n",
    "            A_sym_0_c = A_sym_0_c.double()\n",
    "\n",
    "            A_unsym_0_c_sum_r = torch.sum(A_unsym_0_c, dim=1)\n",
    "            A_unsym_0_c_sum_c = torch.sum(A_unsym_0_c, dim=0)\n",
    "\n",
    "            for i in range(nclusters):\n",
    "                if A_unsym_0_c_sum_r[i] == 0 and A_unsym_0_c_sum_c[i] == 0:\n",
    "                    idx_a = i\n",
    "                    idx_b = 0\n",
    "                    for k in range(indices.size(1)):\n",
    "                        if indices[i][k] != i:\n",
    "                            idx_b = indices[i][k]\n",
    "                            break\n",
    "\n",
    "\n",
    "                    # クラスタ数が最小クラスタ数を下回らないようにチェック\n",
    "                    if nclusters <= min_clusters:\n",
    "                        print(f\"クラスタ数が最小クラスタ数 {min_clusters} を下回るため、クラスタ結合を停止します。\")\n",
    "                        return A_unsym_0_c, A_sym_0_c, Y_0\n",
    "\n",
    "                    if idx_b > 0:\n",
    "                        if idx_a > idx_b:\n",
    "                            #print(\"merge\", idx_b, idx_a)\n",
    "                            A_sym_0_c, A_unsym_0_c, Y_0 = self.merge_two_clusters(W, A_sym_0_c, A_unsym_0_c, Y_0, idx_b, idx_a)\n",
    "                        else:\n",
    "                            #print(\"merge\", idx_a, idx_b)\n",
    "                            A_sym_0_c, A_unsym_0_c, Y_0 = self.merge_two_clusters(W, A_sym_0_c, A_unsym_0_c, Y_0, idx_a, idx_b)\n",
    "\n",
    "                        A_unsym_0_c_sum_r = torch.sum(A_unsym_0_c, dim=1)\n",
    "                        A_unsym_0_c_sum_c = torch.sum(A_unsym_0_c, dim=0)\n",
    "\n",
    "        print('Time elapsed for computing cluster affinity:', time.time() - timer, 'seconds')\n",
    "        # print(\"Y_0: \", Y_0)\n",
    "        # print(\"A_unsym_0_c: \", A_unsym_0_c.size())\n",
    "        # print(\"A_sym_0_c: \", A_sym_0_c.size())\n",
    "        return A_unsym_0_c, A_sym_0_c, Y_0\n",
    "\n",
    "    # def compute_CAff(self, W, NNs, Y_0_tensor):\n",
    "    #     # Placeholder function to simulate compute_CAff. Actual implementation required.\n",
    "    #     A_unsym_0_c = torch.rand(W.size())\n",
    "    #     A_sym_0_c = torch.rand(W.size())\n",
    "    #     return A_unsym_0_c, A_sym_0_c\n",
    "\n",
    "    def compute_CAff(self,W, NNs, Y):\n",
    "        nclusters = NNs.size(0)\n",
    "        \n",
    "        A_us = torch.zeros_like(NNs)\n",
    "        A_s = torch.zeros_like(NNs)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(i, nclusters):\n",
    "                if NNs[i, j] == 0 and NNs[j, i] == 0:\n",
    "                    A_us[j, i] = 0\n",
    "                    A_us[i, j] = 0\n",
    "                    A_s[j, i] = 0\n",
    "                    A_s[i, j] = 0\n",
    "                    continue\n",
    "\n",
    "                if i == j:\n",
    "                    A_us[j, i] = 0\n",
    "                    A_s[j, i] = 0\n",
    "                    continue\n",
    "\n",
    "                # get the size of Y[i] and Y[j]\n",
    "                Y_i_size = (Y[i] != 0).sum().item()\n",
    "                Y_j_size = (Y[j] != 0).sum().item()\n",
    "\n",
    "                if Y_i_size == 0 or Y_j_size == 0:\n",
    "                    continue\n",
    "                # compute affinity from cluster i to cluster j\n",
    "                A_c_i_j = 0\n",
    "                for m in range(Y_i_size):\n",
    "                    s_W_c_j_i = 0\n",
    "                    s_W_c_i_j = 0\n",
    "                    for n in range(Y_j_size):\n",
    "                        s_W_c_j_i += W[Y[j, n].long() - 1, Y[i, m].long() - 1]\n",
    "                        s_W_c_i_j += W[Y[i, m].long() - 1, Y[j, n].long() - 1]\n",
    "                    A_c_i_j += s_W_c_j_i * s_W_c_i_j\n",
    "\n",
    "                # compute affinity from cluster j to cluster i\n",
    "                A_c_j_i = 0\n",
    "                for m in range(Y_j_size):\n",
    "                    s_W_c_j_i = 0\n",
    "                    s_W_c_i_j = 0\n",
    "                    for n in range(Y_i_size):\n",
    "                        s_W_c_j_i += W[Y[j, m].long() - 1, Y[i, n].long() - 1]\n",
    "                        s_W_c_i_j += W[Y[i, n].long() - 1, Y[j, m].long() - 1]\n",
    "                    A_c_j_i += s_W_c_i_j * s_W_c_j_i\n",
    "\n",
    "                A_us[j, i] = A_c_i_j\n",
    "                A_us[i, j] = A_c_j_i\n",
    "                # print('Y_i_size:', Y_i_size)\n",
    "                # print('Y_j_size:', Y_j_size)\n",
    "                A_s[i, j] = A_c_i_j / (Y_j_size ** 2) + A_c_j_i / (Y_i_size ** 2)\n",
    "                A_s[j, i] = 0\n",
    "\n",
    "        return A_us, A_s\n",
    "\n",
    "\n",
    "    def merge_two_clusters(self, W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "        nclusters = len(Y_t)\n",
    "\n",
    "        nsamples_c_a = len(Y_t[idx_c_a])\n",
    "        nsamples_c_b = len(Y_t[idx_c_b])\n",
    "        if nsamples_c_a == 0 or nsamples_c_b == 0:\n",
    "            print(f\"Skipping merge as one of the clusters is empty: idx_c_a={idx_c_a}, nsamples_c_a={nsamples_c_a}, idx_c_b={idx_c_b}, nsamples_c_b={nsamples_c_b}\")\n",
    "            return A_s_t, A_us_t, Y_t\n",
    "\n",
    "        A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "        ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "\n",
    "        A_us_t[idx_c_a] *= ratio\n",
    "        A_us_t[idx_c_b] *= 1 - ratio\n",
    "        A_us_t[idx_c_a] += A_us_t[idx_c_b]\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "    \n",
    "class Evaluate:\n",
    "    def NMI(self, labels_gt, labels_pre):\n",
    "        N = sum(len(l) for l in labels_gt)\n",
    "        # Compute entropy for labels_gt\n",
    "        pr_gt = torch.zeros(len(labels_gt), 1)\n",
    "        for i, label in enumerate(labels_gt):\n",
    "            pr_gt[i] = len(label) / N\n",
    "        pr_gt_log = torch.log(pr_gt)\n",
    "        H_gt = -torch.sum(pr_gt * pr_gt_log)\n",
    "\n",
    "        # Compute entropy for labels_pre\n",
    "        pr_pre = torch.zeros(len(labels_pre), 1)\n",
    "        for i, label in enumerate(labels_pre):\n",
    "            pr_pre[i] = len(label) / N\n",
    "        pr_pre_log = torch.log(pr_pre)\n",
    "        H_pre = -torch.sum(pr_pre * pr_pre_log)\n",
    "\n",
    "        # Compute mutual information\n",
    "        # Build M_gt\n",
    "        M_gt = torch.zeros(N, len(labels_gt))\n",
    "        for i, label in enumerate(labels_gt):\n",
    "            for j in label:\n",
    "                if j < N:  # Ensure the index is within bounds\n",
    "                    M_gt[j, i] = 1  # Keep it zero-based\n",
    "\n",
    "        # Build M_pre\n",
    "        M_pre = torch.zeros(N, len(labels_pre))\n",
    "        for i, label in enumerate(labels_pre):\n",
    "            for j in label:\n",
    "                if j < N:  # Ensure the index is within bounds\n",
    "                    M_pre[j, i] = 1  # Keep it zero-based\n",
    "\n",
    "        pr_gp = torch.mm(M_gt.t(), M_pre) / N\n",
    "        pr_gp_log = torch.log(pr_gp + 1e-10)\n",
    "        H_gp = -torch.sum(pr_gp * pr_gp_log)\n",
    "\n",
    "        # Compute mutual information\n",
    "        MI = H_gt + H_pre - H_gp\n",
    "        NMI = MI / torch.sqrt(H_gt * H_pre)\n",
    "\n",
    "        return NMI.item()\n",
    "    \n",
    "# 直接スクリプト内で設定するオプション\n",
    "class Options:\n",
    "    dataset = 'custom'\n",
    "    eta = 0.2\n",
    "    epoch_rnn = 1\n",
    "    batchSize = 1000\n",
    "    learningRate = 0.01\n",
    "    weightDecay = 5e-5\n",
    "    momentum = 0.9\n",
    "    gamma_lr = 0.0001\n",
    "    power_lr = 0.75\n",
    "    num_nets = 1\n",
    "    epoch_pp = 20\n",
    "    epoch_max = 1000\n",
    "    K_s = 5\n",
    "    K_c = 5\n",
    "    gamma_tr = 1\n",
    "    margin_tr = 0.2\n",
    "    num_nsampling = 5\n",
    "    use_fast = 1\n",
    "    updateCNN = 1\n",
    "    centralize_input = 0\n",
    "    centralize_feature = 0\n",
    "    normalize = 1\n",
    "\n",
    "opt = Options()\n",
    "\n",
    "# Initialize networks\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "affinity = Affinity()\n",
    "evaluate = Evaluate()\n",
    "agg_clustering = AggClustering()\n",
    "\n",
    "class CustomVectorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def load_model(input_size):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# データのロード\n",
    "def load_data(data_loader):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for vectors, lbls in data_loader:\n",
    "        data.append(vectors)\n",
    "        labels.append(lbls)\n",
    "    data = torch.cat(data)\n",
    "    labels = torch.cat(labels)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Initialize CNN models and variables\n",
    "print('==> configuring model')\n",
    "num_networks = opt.num_nets\n",
    "network_table = []\n",
    "optimizer_table = []\n",
    "criterion_triplet = TripletEmbeddingCriterion(opt.margin_tr, opt.gamma_tr)\n",
    "\n",
    "for _ in range(num_networks):\n",
    "    model = load_model(320)\n",
    "    model.apply(init_weights)\n",
    "    network_table.append(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.learningRate, weight_decay=opt.weightDecay, momentum=opt.momentum)\n",
    "    optimizer_table.append(optimizer)\n",
    "\n",
    "label_gt_table_table = []\n",
    "label_pre_table_table = []\n",
    "label_pre_tensor_table = []\n",
    "target_nclusters_table = []\n",
    "labels_log=[]\n",
    "\n",
    "def cvt2TableLabels(labels):\n",
    "    unique_labels = torch.unique(labels)\n",
    "    label_table = {label.item(): [] for label in unique_labels}\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_table[label.item()].append(idx)\n",
    "    return list(label_table.values())\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "saiteki_number_list=[]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def most_common_elements(lst):\n",
    "    if not lst:\n",
    "        return None  # リストが空の場合は None を返す\n",
    "    counter = Counter(lst)\n",
    "    max_count = counter.most_common(1)[0][1]  # 最も多い出現回数を取得\n",
    "    most_common_elements = [elem for elem, count in counter.items() if count == max_count]\n",
    "    return most_common_elements[-1]\n",
    "\n",
    "def estimate_optimal_clusters(features):\n",
    "    max_clusters = min(20, len(features))  # 最大クラスタ数をデータサイズに応じて制限\n",
    "    distortions = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in range(2, max_clusters):\n",
    "        labels = KMeans(n_clusters=k,n_init=20).fit_predict(features.detach().numpy() )\n",
    "        distortions.append(KMeans(n_clusters=k,n_init=20).fit(features.detach().numpy()).inertia_)\n",
    "        silhouette_scores.append(silhouette_score(features.detach().numpy(), labels))\n",
    "    \n",
    "    kl = KneeLocator(range(2, max_clusters), distortions, curve=\"convex\", direction=\"decreasing\")\n",
    "    optimal_clusters_elbow = kl.elbow\n",
    "    optimal_clusters_silhouette = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    saiteki_number= max(optimal_clusters_elbow, optimal_clusters_silhouette)\n",
    "    saiteki_number_list.append(saiteki_number)\n",
    "    if (len(saiteki_number_list)>5):\n",
    "        saiteki_number_list.pop(0)\n",
    "    #saiteki_number_list.append(saiteki_number)\n",
    "    print('saiteki_number_list',saiteki_number_list)\n",
    "    print(\"saiteki_class:\",most_common_elements(saiteki_number_list))\n",
    "    return most_common_elements(saiteki_number_list)\n",
    "\n",
    "epoch_reset_labels = [0] * num_networks\n",
    "\n",
    "def getnClusters(label_pre):\n",
    "    nClusters = 0\n",
    "    for cluster in label_pre:\n",
    "        if len(cluster) > 0:\n",
    "            nClusters += 1\n",
    "    return nClusters\n",
    "\n",
    "def update_labels(features, label_pre, target_clusters, iter,estimate_optimal_clusters):\n",
    "    # print(\"compute affinity, \", features.size())\n",
    "    d, ind, W = affinity.compute(features, opt.K_s)  # sigma_l not used here\n",
    "    # sigma = sigma_l\n",
    "    if iter == 0:\n",
    "        print(\"initialize clusters...\")\n",
    "        # print(\"ind\", ind.size())\n",
    "        label_pre = agg_clustering.init(ind)\n",
    "        # print(\"nclusters: \", getnClusters(label_pre))\n",
    "        return label_pre\n",
    "\n",
    "    print(\"nclusters: \", getnClusters(label_pre))\n",
    "    A_us, A_s, label_pre = affinity.compute4cluster(features, W, label_pre, getnClusters(label_pre), target_clusters,estimate_optimal_clusters)\n",
    "    # print(\"nclusters affinity_compute: \", getnClusters(label_pre))\n",
    "    n_clusters = getnClusters(label_pre)\n",
    "    # print(\"A_s\", A_s.size())\n",
    "    # print(\"new_n_clusters\", n_clusters)\n",
    "    #12\n",
    "    print(\"run agglomerative clustering...\")\n",
    "\n",
    "    # Convert n_clusters to tensor\n",
    "    n_clusters_tensor = torch.tensor(n_clusters, dtype=torch.float32)\n",
    "    unfold_iter = torch.ceil(n_clusters_tensor * opt.eta).item()\n",
    "    unfold_valid_iter = n_clusters - target_clusters\n",
    "    iterations = min(unfold_iter, unfold_valid_iter)\n",
    "\n",
    "    if iterations <= 0:\n",
    "        print(\"nclusters:1Time: \", getnClusters(label_pre))\n",
    "        return label_pre\n",
    "\n",
    "    label_pre = agg_clustering.run(W, A_us, A_s, label_pre, iterations, opt.K_c, opt.use_fast)\n",
    "    return label_pre\n",
    "\n",
    "def extract_features(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = []\n",
    "        for batch in DataLoader(CustomVectorDataset(data, torch.zeros(len(data))), batch_size=opt.batchSize, shuffle=False):\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu())\n",
    "    return torch.cat(features)\n",
    "\n",
    "def cvt2TensorLabels(labels):\n",
    "    tensor_labels = torch.zeros(sum(len(l) for l in labels), dtype=torch.long)\n",
    "    for cluster_id, cluster in enumerate(labels):\n",
    "        tensor_labels[cluster] = cluster_id + 1\n",
    "    return tensor_labels.unsqueeze(1)\n",
    "\n",
    "def merge_labels(network_table, epoch_reset_labels, train_data):\n",
    "    for i, model in enumerate(network_table):\n",
    "        if epoch_reset_labels[i] == 0 or opt.updateCNN == 0:\n",
    "            features = train_data\n",
    "        else:\n",
    "            features =  train_data\n",
    "            #features = extract_features(model, train_data)\n",
    "        \n",
    "        if opt.centralize_feature == 1:\n",
    "            features -= features.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # if opt.normalize == 1:\n",
    "        #     features = nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        print(\"feature dims:\", features.size())\n",
    "        print('epoch_reset_labels:', epoch_reset_labels[i])\n",
    "        print('target_nclusters:', target_nclusters_table[i])\n",
    "        print('label_pre_table:', label_pre_table_table[i])\n",
    "        # 終了条件のチェック\n",
    "       \n",
    "        fin_flag,estimated_clusters=is_all_finished(features)\n",
    "        if label_pre_table_table[i]!=[]:\n",
    "            print(fin_flag)\n",
    "            if fin_flag:\n",
    "                print(\"Optimal number of clusters reached based on Elbow method and Silhouette analysis.\")\n",
    "                return True\n",
    "        label_pre_table_table[i] = update_labels(features, label_pre_table_table[i], target_nclusters_table[i], epoch_reset_labels[i],estimated_clusters)\n",
    "        labels_log.append([f\"epoch:{0} {1}\".format(epoch_reset_labels[i],label_pre_table_table[i])])\n",
    "        epoch_reset_labels[i] += 1\n",
    "        nclusters = len(label_pre_table_table[i])\n",
    "\n",
    "        print(\"nclusters:\", nclusters)\n",
    "        label_pre_tensor_table[i] = cvt2TensorLabels(label_pre_table_table[i])\n",
    "\n",
    "        # 終了条件のチェック\n",
    "        if nclusters!=0 and nclusters<=estimated_clusters:\n",
    "\n",
    "            print(\"Optimal number of clusters reached based on Elbow method and Silhouette analysis.\")\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def merge_labels_final():\n",
    "    for i, model in enumerate(network_table):\n",
    "        features = extract_features(model, train_data)\n",
    "        if opt.centralize_feature == 1:\n",
    "            features -= features.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if opt.normalize == 1:\n",
    "            features = nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        label_pre_table_table[i] = update_labels(features, label_pre_table_table[i], target_nclusters_table[i], epoch_reset_labels[i])\n",
    "        epoch_reset_labels[i] += 1\n",
    "        nclusters = len(label_pre_table_table[i])\n",
    "        print(\"nclusters:\", nclusters)\n",
    "        label_pre_tensor_table[i] = cvt2TensorLabels(label_pre_table_table[i])\n",
    "\n",
    "def organize_samples(X, y):\n",
    "    num_s = X.size(0)\n",
    "    y_table = cvt2TableLabels(y)\n",
    "    nclusters = len(y_table)\n",
    "    if nclusters == 1:\n",
    "        return None, None\n",
    "    num_neg_sampling = min(opt.num_nsampling, nclusters - 1)\n",
    "    num_triplet = sum(len(cluster) * (len(cluster) - 1) * num_neg_sampling // 2 for cluster in y_table if len(cluster) > 1)\n",
    "    if num_triplet == 0:\n",
    "        return None, None\n",
    "\n",
    "    A = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    B = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    C = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    A_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    B_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    C_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    id_triplet = 0\n",
    "\n",
    "    for i, cluster in enumerate(y_table):\n",
    "        if len(cluster) > 1:\n",
    "            for m in range(len(cluster)):\n",
    "                for n in range(m + 1, len(cluster)):\n",
    "                    is_chosen = torch.zeros(num_s, dtype=torch.bool)\n",
    "                    chosen_count = 0\n",
    "                    while chosen_count < num_neg_sampling:\n",
    "                        id_s = random.randint(0, num_s - 1)\n",
    "                        if not is_chosen[id_s] and y[id_s] != y[cluster[m]]:\n",
    "                            A_ind[id_triplet] = cluster[m]\n",
    "                            B_ind[id_triplet] = cluster[n]\n",
    "                            C_ind[id_triplet] = id_s\n",
    "                            is_chosen[id_s] = True\n",
    "                            chosen_count += 1\n",
    "                            id_triplet += 1\n",
    "\n",
    "    A.copy_(X[A_ind])\n",
    "    B.copy_(X[B_ind])\n",
    "    C.copy_(X[C_ind])\n",
    "    return [A, B, C], [A_ind, B_ind, C_ind]\n",
    "\n",
    "def cvt2df_do(df_do, df_dtriplets, triplets_ind):\n",
    "    df_do.index_add_(0, triplets_ind[0], df_dtriplets[0])\n",
    "    df_do.index_add_(0, triplets_ind[1], df_dtriplets[1])\n",
    "    df_do.index_add_(0, triplets_ind[2], df_dtriplets[2])\n",
    "    return df_do\n",
    "\n",
    "# def update_CNN():\n",
    "#     for model, optimizer in zip(network_table, optimizer_table):\n",
    "#         model.train()\n",
    "#     epoch = 1\n",
    "#     print(f'==> online epoch # {epoch} [batchSize = {opt.batchSize}] [learningRate = {opt.learningRate}]')\n",
    "#     indices = torch.randperm(len(train_data)).split(opt.batchSize)\n",
    "\n",
    "#     for t, v in enumerate(indices, 1):\n",
    "#         iter = epoch * len(indices) + t - 1\n",
    "#         learning_rate = opt.learningRate * (1 + opt.gamma_lr * iter) ** -opt.power_lr\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = learning_rate\n",
    "        \n",
    "#         inputs = train_data[v]\n",
    "#         targets = label_pre_tensor_table[0][v].squeeze()  # Change this if you have multiple networks\n",
    "        \n",
    "#         for model, optimizer in zip(network_table, optimizer_table):\n",
    "#             def closure():\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(inputs)\n",
    "#                 triplets, triplets_ind = organize_samples(outputs, targets)\n",
    "#                 loss = torch.tensor(0)\n",
    "#                 if triplets:\n",
    "#                     anchor, positive, negative = triplets\n",
    "#                     loss = criterion_triplet(anchor, positive, negative)\n",
    "#                     loss.backward()\n",
    "#                 if t % 10 == 0:\n",
    "#                     print(\"loss:\", loss.item())\n",
    "#                 return loss\n",
    "            \n",
    "#             optimizer.step(closure)\n",
    "#     epoch += 1\n",
    "\n",
    "def update_CNN(train_data):\n",
    "    for model in network_table:\n",
    "        model.train()\n",
    "    \n",
    "    global epoch\n",
    "    epoch = epoch if 'epoch' in globals() else 1\n",
    "    print(f'==> online epoch # {epoch} [batchSize = {opt.batchSize}] [learningRate = {opt.learningRate}]')\n",
    "    \n",
    "    indices = torch.randperm(len(train_data)).split(opt.batchSize)\n",
    "    total_loss=[]\n",
    "\n",
    "    for t, v in enumerate(indices):\n",
    "        # vの値と型を確認\n",
    "        #print(f\"v: {v}, type: {type(v)}\")\n",
    "        iter = epoch * len(indices) + t\n",
    "        learning_rate = opt.learningRate * (1 + opt.gamma_lr * iter) ** (-opt.power_lr)\n",
    "        \n",
    "        inputs = train_data[v]\n",
    "        \n",
    "        for i, (model, optimizer) in enumerate(zip(network_table, optimizer_table)):\n",
    "            #print(label_pre_tensor_table)\n",
    "            #print(f\"i: {i}, type: {type(i)}\")\n",
    "            targets = label_pre_tensor_table[i][v]\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                triplets, triplets_ind = organize_samples(outputs, targets.float())\n",
    "                loss = torch.tensor(0.0)\n",
    "                if triplets is not None:\n",
    "                    anchor, positive, negative = triplets\n",
    "                    loss = criterion_triplet(anchor, positive, negative)\n",
    "                    #loss.backward()\n",
    "                \n",
    "                    if t % 10 == 0:\n",
    "                        print(\"loss:\", loss.item())\n",
    "                    #print(\"losstypesA:\", type(loss))\n",
    "                    return loss\n",
    "                else:\n",
    "                    #print(\"losstypesB:\", type(loss))\n",
    "                    return loss\n",
    "            \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            \n",
    "            loss = optimizer.step(closure)  # ここでclosureの戻り値を取得\n",
    "            total_loss.append(loss.item())\n",
    "    \n",
    "    epoch += 1\n",
    "    average_loss = sum(total_loss) / len(total_loss) if total_loss else 0.0\n",
    "    return average_loss\n",
    "\n",
    "def eval_perf():\n",
    "    for model in network_table:\n",
    "        model.eval()\n",
    "    print('==> testing')\n",
    "    for i, model in enumerate(network_table):\n",
    "        nmi = Evaluate().NMI(label_gt_table_table[i], label_pre_table_table[i])\n",
    "        print('NMI:', nmi)\n",
    "        print(\" \")\n",
    "\n",
    "def is_all_finished(features):\n",
    "    estimated_clusters = estimate_optimal_clusters(features)\n",
    "    for label_pre, target_nclusters in zip(label_pre_table_table, target_nclusters_table):\n",
    "        if len(label_pre) > estimated_clusters:\n",
    "            return False,estimated_clusters\n",
    "    return True,estimated_clusters\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from models import TSEncoder\n",
    "from models.losses import hierarchical_contrastive_loss\n",
    "from utils import take_per_row, split_with_nan, centerize_vary_length_series, torch_pad_nan\n",
    "import math\n",
    "\n",
    "class TS2Vec:\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims=320,\n",
    "        hidden_dims=64,\n",
    "        length_dim=10,\n",
    "        depth=10,\n",
    "        # device='cuda',\n",
    "        device='cpu',\n",
    "        lr=0.001,\n",
    "        batch_size=16,\n",
    "        max_train_length=None,\n",
    "        temporal_unit=0,\n",
    "        after_iter_callback=None,\n",
    "        after_epoch_callback=None,\n",
    "        input_total=1\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            input_dims (int): The input dimension. For a univariate time series, this should be set to 1.\n",
    "            output_dims (int): The representation dimension.\n",
    "            hidden_dims (int): The hidden dimension of the encoder.\n",
    "            depth (int): The number of hidden residual blocks in the encoder.\n",
    "            device (int): The gpu used for training and inference.\n",
    "            lr (int): The learning rate.\n",
    "            batch_size (int): The batch size.\n",
    "            max_train_length (Union[int, NoneType]): The maximum allowed sequence length for training. For sequence with a length greater than <max_train_length>, it would be cropped into some sequences, each of which has a length less than <max_train_length>.\n",
    "            temporal_unit (int): The minimum unit to perform temporal contrast. When training on a very long sequence, this param helps to reduce the cost of time and memory.\n",
    "            after_iter_callback (Union[Callable, NoneType]): A callback function that would be called after each iteration.\n",
    "            after_epoch_callback (Union[Callable, NoneType]): A callback function that would be called after each epoch.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_train_length = max_train_length\n",
    "        self.temporal_unit = temporal_unit\n",
    "        \n",
    "        self._net = TSEncoder(input_dims=input_dims, output_dims=output_dims, hidden_dims=hidden_dims, length_dim=length_dim,depth=depth,input_total=input_total).to(self.device)\n",
    "        self.net = torch.optim.swa_utils.AveragedModel(self._net)\n",
    "        self.net.update_parameters(self._net)\n",
    "        \n",
    "        self.after_iter_callback = after_iter_callback\n",
    "        self.after_epoch_callback = after_epoch_callback\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "    \n",
    "    def fit(self, train_data, n_epochs=None, n_iters=None, verbose=False,save_model=\"test.pth\"):\n",
    "        ''' Training the TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray): The training data. It should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            n_epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
    "            n_iters (Union[int, NoneType]): The number of iterations. When this reaches, the training stops. If both n_epochs and n_iters are not specified, a default setting would be used that sets n_iters to 200 for a dataset with size <= 100000, 600 otherwise.\n",
    "            verbose (bool): Whether to print the training loss after each epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss_log: a list containing the training losses on each epoch.\n",
    "        '''\n",
    "        assert train_data.ndim == 3\n",
    "        \n",
    "        if n_iters is None and n_epochs is None:\n",
    "            #n_iters 200to400\n",
    "            n_iters = 400 if train_data.size <= 100000 else 600  # default param for n_iters\n",
    "        \n",
    "        if self.max_train_length is not None:\n",
    "            sections = train_data.shape[1] // self.max_train_length\n",
    "            if sections >= 2:\n",
    "                train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "\n",
    "        temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "        if temporal_missing[0] or temporal_missing[-1]:\n",
    "            train_data = centerize_vary_length_series(train_data)\n",
    "                \n",
    "        train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=min(self.batch_size, len(train_dataset)), shuffle=True, drop_last=True)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self._net.parameters(), lr=self.lr)\n",
    "        \n",
    "        loss_log = []\n",
    "        best_loss = float('inf')\n",
    "        #スライディングウィンドウなしの時はslide_num=1にしてください\n",
    "        slide_num=1\n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs:\n",
    "                break\n",
    "            \n",
    "            cum_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "            \n",
    "            interrupted = False\n",
    "            merge_flag = False\n",
    "            for batch in train_loader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "                \n",
    "                x = batch[0]\n",
    "                if self.max_train_length is not None and x.size(1) > self.max_train_length:\n",
    "                    window_offset = np.random.randint(x.size(1) - self.max_train_length + 1)\n",
    "                    x = x[:, window_offset : window_offset + self.max_train_length]\n",
    "                x = x.to(self.device)\n",
    "                # print(\"x.shape\")\n",
    "                # print(x.shape)\n",
    "                \n",
    "                ts_l = x.size(1)\n",
    "                \n",
    "                if slide_num==1:\n",
    "                    crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "                else:\n",
    "                    crop_l = np.random.randint(low=slide_num, high=ts_l+1)\n",
    "                #print(crop_l)\n",
    "                crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "                crop_right = crop_left + crop_l\n",
    "                crop_eleft = np.random.randint(crop_left + 1)\n",
    "                crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "\n",
    "                crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                #forwardの処理\n",
    "                #take_per_rowは行列の一部を取り出す関数 各行目でcrop_offset+crop_eleftからcrop_right - crop_eleft個のデータを取り出す\n",
    "            \n",
    "                out1,loss_k1 = self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                #out11,out12,out13,loss_k_11,loss_k_12,loss_k_13=self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                # print(\"crop_l\")\n",
    "                # print(crop_l)\n",
    "                # print(out1.shape)\n",
    "                out1 = out1[:, -crop_l+(slide_num-1):]\n",
    "\n",
    "                out2,loss_k2= self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                #out21,out22,out23,loss_k_21,loss_k_22,loss_k_23= self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                #print(crop_l)\n",
    "                # print(out2.shape)\n",
    "                out2 = out2[:, :crop_l-(slide_num-1)]\n",
    "                \n",
    "                loss= hierarchical_contrastive_loss(\n",
    "                    out1,\n",
    "                    out2,\n",
    "                    temporal_unit=self.temporal_unit\n",
    "                )\n",
    "                \n",
    "                print(\"loss_k1,loss_k2\")\n",
    "                print(loss_k1,loss_k2)\n",
    "                loss += loss_k1*1.2+loss_k2*1.2\n",
    "                #loss=loss1+loss2+loss3\n",
    "                #loss += loss_k1*0.1+loss_k2*0.1\n",
    "                \n",
    "                out_jule,loss_jule= self._net(x)\n",
    "                #print(out_jule)\n",
    "                jule_X=out_jule.reshape(out_jule.size(1),out_jule.size(2))\n",
    "                #jule_X=torch.from_numpy(out_jule).float()\n",
    "\n",
    "                if self.n_epochs%20==0 and self.n_epochs!=0:\n",
    "                    merge_flag=merge_labels(network_table, epoch_reset_labels, jule_X)\n",
    "                    eval_perf()\n",
    "                \n",
    "                if merge_flag:\n",
    "                    break\n",
    "                \n",
    "                #indices = torch.randperm(len(train_data)).split(opt.batchSize)\n",
    "                indices = torch.arange(len(jule_X)).split(opt.batchSize)\n",
    "                #print(\"indices\",indices)\n",
    "                total_loss=[]\n",
    "                if self.n_epochs>=20:\n",
    "                    for t, v in enumerate(indices):\n",
    "                        iter = self.n_epochs * len(indices) + t\n",
    "                        learning_rate = opt.learningRate * (1 + opt.gamma_lr * iter) ** (-opt.power_lr)\n",
    "                        \n",
    "                        inputs = jule_X[v]\n",
    "                        \n",
    "                        for i, (_model, _optimizer) in enumerate(zip(network_table, optimizer_table)):\n",
    "                            targets = label_pre_tensor_table[i][v]\n",
    "\n",
    "                            #print('t,i',t,i)\n",
    "                            \n",
    "                            # 損失の計算\n",
    "                            triplets, triplets_ind = organize_samples(inputs, targets.float())\n",
    "                            j_loss = torch.tensor(0.0, device=inputs.device)\n",
    "                            if triplets is not None:\n",
    "                                anchor, positive, negative = triplets\n",
    "                                j_loss = criterion_triplet(anchor, positive, negative)\n",
    "                                \n",
    "                                if t % 10 == 0:\n",
    "                                    print(\"loss:\", j_loss.item())\n",
    "                            \n",
    "                            #loss = optimizer.step(closure)  # ここでclosureの戻り値を取得\n",
    "                                total_loss.append(j_loss.item())\n",
    "                \n",
    "                #epoch += 1\n",
    "                #print(\"total_loss\",total_loss)\n",
    "                average_loss = sum(total_loss) / len(total_loss) if total_loss else 0.0\n",
    "\n",
    "                print(\"juleloss\",average_loss)\n",
    "                print(\"\")\n",
    "                loss += average_loss*10\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.net.update_parameters(self._net)\n",
    "                    \n",
    "                cum_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                \n",
    "                self.n_iters += 1\n",
    "                \n",
    "                if self.after_iter_callback is not None:\n",
    "                    self.after_iter_callback(self, loss.item())\n",
    "            \n",
    "            if interrupted:\n",
    "                break\n",
    "            if merge_flag:\n",
    "                break\n",
    "            \n",
    "            cum_loss /= n_epoch_iters\n",
    "            if cum_loss < best_loss:\n",
    "                best_loss = cum_loss\n",
    "                print(f\"Best model updated: loss={cum_loss}\")\n",
    "                loss_log.append(cum_loss)\n",
    "                torch.save(self.net.state_dict(), save_model)\n",
    "            \n",
    "                #torch.save(model.state_dict(), 'best_model.pth')\n",
    "            if verbose:\n",
    "                print(f\"Epoch #{self.n_epochs}: loss={cum_loss}\")\n",
    "            self.n_epochs += 1\n",
    "            \n",
    "            if self.after_epoch_callback is not None:\n",
    "                self.after_epoch_callback(self, cum_loss)\n",
    "            \n",
    "        return loss_log\n",
    "    \n",
    "    def _eval_with_pooling(self, x, mask=None, slicing=None, encoding_window=None):\n",
    "        out ,_loss= self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        if encoding_window == 'full_series':\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = out.size(1),\n",
    "            ).transpose(1, 2)\n",
    "            \n",
    "        elif isinstance(encoding_window, int):\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = encoding_window,\n",
    "                stride = 1,\n",
    "                padding = encoding_window // 2\n",
    "            ).transpose(1, 2)\n",
    "            if encoding_window % 2 == 0:\n",
    "                out = out[:, :-1]\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        elif encoding_window == 'multiscale':\n",
    "            p = 0\n",
    "            reprs = []\n",
    "            while (1 << p) + 1 < out.size(1):\n",
    "                t_out = F.max_pool1d(\n",
    "                    out.transpose(1, 2),\n",
    "                    kernel_size = (1 << (p + 1)) + 1,\n",
    "                    stride = 1,\n",
    "                    padding = 1 << p\n",
    "                ).transpose(1, 2)\n",
    "                if slicing is not None:\n",
    "                    t_out = t_out[:, slicing]\n",
    "                reprs.append(t_out)\n",
    "                p += 1\n",
    "            out = torch.cat(reprs, dim=-1)\n",
    "            \n",
    "        else:\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        return out.cpu()\n",
    "    \n",
    "    def encode(self, data, mask=None, encoding_window=None, causal=False, sliding_length=None, sliding_padding=0, batch_size=None):\n",
    "        ''' Compute representations using the model.\n",
    "        \n",
    "        Args:\n",
    "            data (numpy.ndarray): This should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            mask (str): The mask used by encoder can be specified with this parameter. This can be set to 'binomial', 'continuous', 'all_true', 'all_false' or 'mask_last'.\n",
    "            encoding_window (Union[str, int]): When this param is specified, the computed representation would the max pooling over this window. This can be set to 'full_series', 'multiscale' or an integer specifying the pooling kernel size.\n",
    "            causal (bool): When this param is set to True, the future informations would not be encoded into representation of each timestamp.\n",
    "            sliding_length (Union[int, NoneType]): The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "            sliding_padding (int): This param specifies the contextual data length used for inference every sliding windows.\n",
    "            batch_size (Union[int, NoneType]): The batch size used for inference. If not specified, this would be the same batch size as training.\n",
    "            \n",
    "        Returns:\n",
    "            repr: The representations for data.\n",
    "        '''\n",
    "        assert self.net is not None, 'please train or load a net first'\n",
    "        assert data.ndim == 3\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        n_samples, ts_l, _ = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "        \n",
    "        dataset = TensorDataset(torch.from_numpy(data).to(torch.float))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        print(\"a\")\n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for batch in loader:\n",
    "                x = batch[0]\n",
    "                if sliding_length is not None:\n",
    "                    reprs = []\n",
    "                    if n_samples < batch_size:\n",
    "                        calc_buffer = []\n",
    "                        calc_buffer_l = 0\n",
    "                    for i in range(0, ts_l, sliding_length):\n",
    "                        l = i - sliding_padding\n",
    "                        r = i + sliding_length + (sliding_padding if not causal else 0)\n",
    "                        x_sliding = torch_pad_nan(\n",
    "                            x[:, max(l, 0) : min(r, ts_l)],\n",
    "                            left=-l if l<0 else 0,\n",
    "                            right=r-ts_l if r>ts_l else 0,\n",
    "                            dim=1\n",
    "                        )\n",
    "                        if n_samples < batch_size:\n",
    "                            if calc_buffer_l + n_samples > batch_size:\n",
    "                                out = self._eval_with_pooling(\n",
    "                                    torch.cat(calc_buffer, dim=0),\n",
    "                                    mask,\n",
    "                                    slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                    encoding_window=encoding_window\n",
    "                                )\n",
    "                                reprs += torch.split(out, n_samples)\n",
    "                                calc_buffer = []\n",
    "                                calc_buffer_l = 0\n",
    "                            calc_buffer.append(x_sliding)\n",
    "                            calc_buffer_l += n_samples\n",
    "                        else:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                x_sliding,\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs.append(out)\n",
    "\n",
    "                    if n_samples < batch_size:\n",
    "                        if calc_buffer_l > 0:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                torch.cat(calc_buffer, dim=0),\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs += torch.split(out, n_samples)\n",
    "                            calc_buffer = []\n",
    "                            calc_buffer_l = 0\n",
    "                    \n",
    "                    out = torch.cat(reprs, dim=1)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = F.max_pool1d(\n",
    "                            out.transpose(1, 2).contiguous(),\n",
    "                            kernel_size = out.size(1),\n",
    "                        ).squeeze(1)\n",
    "                else:\n",
    "                    out = self._eval_with_pooling(x, mask, encoding_window=encoding_window)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = out.squeeze(1)\n",
    "                        \n",
    "                output.append(out)\n",
    "                \n",
    "            output = torch.cat(output, dim=0)\n",
    "            \n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        ''' Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        torch.save(self.net.state_dict(), fn)\n",
    "    \n",
    "    def load(self, fn):\n",
    "        ''' Load the model from a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        state_dict = torch.load(fn, map_location=self.device)\n",
    "        self.net.load_state_dict(state_dict)\n",
    "\n",
    "epoch_reset_labels = [0] * num_networks\n",
    "optimState = {'learningRate': opt.learningRate}\n",
    "\n",
    "def create_answer(data_dict):\n",
    "    # 出力を初期化（必要なサイズを計算する）\n",
    "    max_index = max(max(indices) for indices in data_dict.values())\n",
    "    output = [-1] * (max_index + 1)\n",
    "\n",
    "    # 各リストの値に対応するインデックスを設定\n",
    "    for key, indices in data_dict.items():\n",
    "        for index in indices:\n",
    "            output[index] = key\n",
    "\n",
    "    # 出力を表示\n",
    "    return output\n",
    "    \n",
    "from sklearn import metrics\n",
    "def ARI(prediction, groundtruth):\n",
    "    return metrics.adjusted_rand_score(groundtruth, prediction)\n",
    "\n",
    "\n",
    "def ANMI(prediction, groundtruth):\n",
    "    return metrics.adjusted_mutual_info_score(groundtruth, prediction)\n",
    "\n",
    "\n",
    "def NMI(groundtruth, prediction):\n",
    "    return metrics.normalized_mutual_info_score(groundtruth, prediction)\n",
    "\n",
    "\n",
    "def evaluate_clustering(groundtruth, prediction):\n",
    "    ari = ARI(groundtruth, prediction)\n",
    "    anmi = ANMI(groundtruth, prediction)\n",
    "    nmi = NMI(groundtruth, prediction)\n",
    "    return ari, anmi, nmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katoutsubasa/ts2vec/datautils.py:140: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data = pd.read_csv(f'datasets/{name}.csv', index_col='date', parse_dates=True)\n",
      "/var/folders/m9/vc40_2nj7wgb3tgln7x3gdzc0000gn/T/ipykernel_61454/3952738982.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_y=torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "(7352, 10)\n",
      "Index([   0.0,    1.0,    2.0,    3.0,    4.0,    5.0,    6.0,    7.0,    8.0,\n",
      "          9.0,\n",
      "       ...\n",
      "       7342.0, 7343.0, 7344.0, 7345.0, 7346.0, 7347.0, 7348.0, 7349.0, 7350.0,\n",
      "       7351.0],\n",
      "      dtype='float64', name='date', length=7352)\n",
      "(7352, 10)\n",
      "data\n",
      "(7352, 10)\n",
      "slice(None, 4411, None)\n",
      "slice(4411, 5881, None)\n",
      "slice(5881, None, None)\n",
      "(1, 7352, 10)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x10cabe910>\n",
      "torch.Size([1000])\n",
      "[6]\n",
      "1\n",
      "10\n",
      "loss_k1,loss_k2\n",
      "tensor(108.9578) tensor(210.9112)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=389.62042236328125\n",
      "Epoch #0: loss=389.62042236328125\n",
      "loss_k1,loss_k2\n",
      "tensor(469.6056) tensor(482.6997)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #1: loss=1164.6959228515625\n",
      "loss_k1,loss_k2\n",
      "tensor(299.5709) tensor(287.1086)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #2: loss=714.9304809570312\n",
      "loss_k1,loss_k2\n",
      "tensor(174.7201) tensor(168.2712)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #3: loss=418.6698913574219\n",
      "loss_k1,loss_k2\n",
      "tensor(164.3660) tensor(136.6102)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=368.0142822265625\n",
      "Epoch #4: loss=368.0142822265625\n",
      "loss_k1,loss_k2\n",
      "tensor(121.0091) tensor(115.8522)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=289.7265625\n",
      "Epoch #5: loss=289.7265625\n",
      "loss_k1,loss_k2\n",
      "tensor(89.0244) tensor(95.8170)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=226.09957885742188\n",
      "Epoch #6: loss=226.09957885742188\n",
      "loss_k1,loss_k2\n",
      "tensor(90.5030) tensor(104.8418)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #7: loss=238.725830078125\n",
      "loss_k1,loss_k2\n",
      "tensor(17.4818) tensor(46.4023)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=78.29082489013672\n",
      "Epoch #8: loss=78.29082489013672\n",
      "loss_k1,loss_k2\n",
      "tensor(77.2182) tensor(22.3492)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #9: loss=121.56102752685547\n",
      "loss_k1,loss_k2\n",
      "tensor(15.9061) tensor(19.7630)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=43.99891662597656\n",
      "Epoch #10: loss=43.99891662597656\n",
      "loss_k1,loss_k2\n",
      "tensor(32.6326) tensor(43.0965)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #11: loss=93.46099853515625\n",
      "loss_k1,loss_k2\n",
      "tensor(59.7086) tensor(56.5887)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #12: loss=142.9794921875\n",
      "loss_k1,loss_k2\n",
      "tensor(73.6328) tensor(73.5357)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #13: loss=181.1900634765625\n",
      "loss_k1,loss_k2\n",
      "tensor(38.2885) tensor(41.4774)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #14: loss=98.03263854980469\n",
      "loss_k1,loss_k2\n",
      "tensor(17.7751) tensor(34.0592)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #15: loss=63.939674377441406\n",
      "loss_k1,loss_k2\n",
      "tensor(27.7737) tensor(39.2498)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #16: loss=82.95425415039062\n",
      "loss_k1,loss_k2\n",
      "tensor(32.3028) tensor(29.0153)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #17: loss=76.02272033691406\n",
      "loss_k1,loss_k2\n",
      "tensor(31.7889) tensor(38.1812)\n",
      "juleloss 0.0\n",
      "\n",
      "Epoch #18: loss=86.68733215332031\n",
      "loss_k1,loss_k2\n",
      "tensor(12.4699) tensor(13.4802)\n",
      "juleloss 0.0\n",
      "\n",
      "Best model updated: loss=33.20536422729492\n",
      "Epoch #19: loss=33.20536422729492\n",
      "loss_k1,loss_k2\n",
      "tensor(19.7191) tensor(19.4367)\n",
      "feature dims: torch.Size([1000, 320])\n",
      "epoch_reset_labels: 0\n",
      "target_nclusters: 6\n",
      "label_pre_table: []\n",
      "saiteki_number_list [6]\n",
      "saiteki_class: 6\n",
      "sigma: tensor(0.6925, dtype=torch.float64)\n",
      "initialize clusters...\n",
      "[[34, 37], [8, 11], [2, 6], [0], [1], [3], [4], [5], [7], [9], [10], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [35], [36], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526], [527], [528], [529], [530], [531], [532], [533], [534], [535], [536], [537], [538], [539], [540], [541], [542], [543], [544], [545], [546], [547], [548], [549], [550], [551], [552], [553], [554], [555], [556], [557], [558], [559], [560], [561], [562], [563], [564], [565], [566], [567], [568], [569], [570], [571], [572], [573], [574], [575], [576], [577], [578], [579], [580], [581], [582], [583], [584], [585], [586], [587], [588], [589], [590], [591], [592], [593], [594], [595], [596], [597], [598], [599], [600], [601], [602], [603], [604], [605], [606], [607], [608], [609], [610], [611], [612], [613], [614], [615], [616], [617], [618], [619], [620], [621], [622], [623], [624], [625], [626], [627], [628], [629], [630], [631], [632], [633], [634], [635], [636], [637], [638], [639], [640], [641], [642], [643], [644], [645], [646], [647], [648], [649], [650], [651], [652], [653], [654], [655], [656], [657], [658], [659], [660], [661], [662], [663], [664], [665], [666], [667], [668], [669], [670], [671], [672], [673], [674], [675], [676], [677], [678], [679], [680], [681], [682], [683], [684], [685], [686], [687], [688], [689], [690], [691], [692], [693], [694], [695], [696], [697], [698], [699], [700], [701], [702], [703], [704], [705], [706], [707], [708], [709], [710], [711], [712], [713], [714], [715], [716], [717], [718], [719], [720], [721], [722], [723], [724], [725], [726], [727], [728], [729], [730], [731], [732], [733], [734], [735], [736], [737], [738], [739], [740], [741], [742], [743], [744], [745], [746], [747], [748], [749], [750], [751], [752], [753], [754], [755], [756], [757], [758], [759], [760], [761], [762], [763], [764], [765], [766], [767], [768], [769], [770], [771], [772], [773], [774], [775], [776], [777], [778], [779], [780], [781], [782], [783], [784], [785], [786], [787], [788], [789], [790], [791], [792], [793], [794], [795], [796], [797], [798], [799], [800], [801], [802], [803], [804], [805], [806], [807], [808], [809], [810], [811], [812], [813], [814], [815], [816], [817], [818], [819], [820], [821], [822], [823], [824], [825], [826], [827], [828], [829], [830], [831], [832], [833], [834], [835], [836], [837], [838], [839], [840], [841], [842], [843], [844], [845], [846], [847], [848], [849], [850], [851], [852], [853], [854], [855], [856], [857], [858], [859], [860], [861], [862], [863], [864], [865], [866], [867], [868], [869], [870], [871], [872], [873], [874], [875], [876], [877], [878], [879], [880], [881], [882], [883], [884], [885], [886], [887], [888], [889], [890], [891], [892], [893], [894], [895], [896], [897], [898], [899], [900], [901], [902], [903], [904], [905], [906], [907], [908], [909], [910], [911], [912], [913], [914], [915], [916], [917], [918], [919], [920], [921], [922], [923], [924], [925], [926], [927], [928], [929], [930], [931], [932], [933], [934], [935], [936], [937], [938], [939], [940], [941], [942], [943], [944], [945], [946], [947], [948], [949], [950], [951], [952], [953], [954], [955], [956], [957], [958], [959], [960], [961], [962], [963], [964], [965], [966], [967], [968], [969], [970], [971], [972], [973], [974], [975], [976], [977], [978], [979], [980], [981], [982], [983], [984], [985], [986], [987], [988], [989], [990], [991], [992], [993], [994], [995], [996], [997], [998], [999]]\n",
      "nclusters: 997\n",
      "==> testing\n",
      "NMI: 0.5081794857978821\n",
      " \n",
      "loss: 0.12485700100660324\n",
      "juleloss 0.12485700100660324\n",
      "\n",
      "Epoch #20: loss=50.59990692138672\n",
      "loss_k1,loss_k2\n",
      "tensor(33.4397) tensor(28.5316)\n",
      "loss: 0.10325539857149124\n",
      "juleloss 0.10325539857149124\n",
      "\n",
      "Epoch #21: loss=78.26216125488281\n",
      "loss_k1,loss_k2\n",
      "tensor(20.0541) tensor(9.6683)\n",
      "loss: 0.11347130686044693\n",
      "juleloss 0.11347130686044693\n",
      "\n",
      "Epoch #22: loss=38.78401565551758\n",
      "loss_k1,loss_k2\n",
      "tensor(13.1500) tensor(15.2651)\n",
      "loss: 0.007072332315146923\n",
      "juleloss 0.007072332315146923\n",
      "\n",
      "Epoch #23: loss=36.01826477050781\n",
      "loss_k1,loss_k2\n",
      "tensor(18.6364) tensor(13.8010)\n",
      "loss: 0.02942824363708496\n",
      "juleloss 0.02942824363708496\n",
      "\n",
      "Epoch #24: loss=41.521095275878906\n",
      "loss_k1,loss_k2\n",
      "tensor(5.3222) tensor(7.7125)\n",
      "loss: 0.09463842958211899\n",
      "juleloss 0.09463842958211899\n",
      "\n",
      "Best model updated: loss=18.379932403564453\n",
      "Epoch #25: loss=18.379932403564453\n",
      "loss_k1,loss_k2\n",
      "tensor(12.4267) tensor(11.2683)\n",
      "loss: 0.08397256582975388\n",
      "juleloss 0.08397256582975388\n",
      "\n",
      "Epoch #26: loss=31.511878967285156\n",
      "loss_k1,loss_k2\n",
      "tensor(5.0202) tensor(6.9882)\n",
      "loss: 0.07171540707349777\n",
      "juleloss 0.07171540707349777\n",
      "\n",
      "Best model updated: loss=16.399520874023438\n",
      "Epoch #27: loss=16.399520874023438\n",
      "loss_k1,loss_k2\n",
      "tensor(10.9267) tensor(11.7169)\n",
      "loss: 0.09243261069059372\n",
      "juleloss 0.09243261069059372\n",
      "\n",
      "Epoch #28: loss=30.26814842224121\n",
      "loss_k1,loss_k2\n",
      "tensor(4.2660) tensor(0.9338)\n",
      "loss: 0.0913827121257782\n",
      "juleloss 0.0913827121257782\n",
      "\n",
      "Best model updated: loss=8.489309310913086\n",
      "Epoch #29: loss=8.489309310913086\n",
      "loss_k1,loss_k2\n",
      "tensor(1.7679) tensor(2.9065)\n",
      "loss: 0.08397158980369568\n",
      "juleloss 0.08397158980369568\n",
      "\n",
      "Best model updated: loss=7.895912170410156\n",
      "Epoch #30: loss=7.895912170410156\n",
      "loss_k1,loss_k2\n",
      "tensor(15.3929) tensor(14.4414)\n",
      "loss: 0.11201204359531403\n",
      "juleloss 0.11201204359531403\n",
      "\n",
      "Epoch #31: loss=39.54767608642578\n",
      "loss_k1,loss_k2\n",
      "tensor(16.7293) tensor(15.3462)\n",
      "loss: 0.09171519428491592\n",
      "juleloss 0.09171519428491592\n",
      "\n",
      "Epoch #32: loss=42.114437103271484\n",
      "loss_k1,loss_k2\n",
      "tensor(7.5608) tensor(5.7278)\n",
      "loss: 0.08539322763681412\n",
      "juleloss 0.08539322763681412\n",
      "\n",
      "Epoch #33: loss=18.92745590209961\n",
      "loss_k1,loss_k2\n",
      "tensor(12.1173) tensor(11.2037)\n",
      "loss: 0.0765383392572403\n",
      "juleloss 0.0765383392572403\n",
      "\n",
      "Epoch #34: loss=31.25784683227539\n",
      "loss_k1,loss_k2\n",
      "tensor(5.3038) tensor(4.6714)\n",
      "loss: 0.04873453080654144\n",
      "juleloss 0.04873453080654144\n",
      "\n",
      "Epoch #35: loss=14.467267990112305\n",
      "loss_k1,loss_k2\n",
      "tensor(8.7746) tensor(8.9634)\n",
      "loss: 0.07258972525596619\n",
      "juleloss 0.07258972525596619\n",
      "\n",
      "Epoch #36: loss=24.416534423828125\n",
      "loss_k1,loss_k2\n",
      "tensor(3.8433) tensor(1.5247)\n",
      "loss: 0.10867516696453094\n",
      "juleloss 0.10867516696453094\n",
      "\n",
      "Epoch #37: loss=9.156021118164062\n",
      "loss_k1,loss_k2\n",
      "tensor(9.7440) tensor(7.9889)\n",
      "loss: 0.0639873668551445\n",
      "juleloss 0.0639873668551445\n",
      "\n",
      "Epoch #38: loss=24.345991134643555\n",
      "loss_k1,loss_k2\n",
      "tensor(6.6273) tensor(6.9908)\n",
      "loss: 0.08549759536981583\n",
      "juleloss 0.08549759536981583\n",
      "\n",
      "Epoch #39: loss=19.2945556640625\n",
      "loss_k1,loss_k2\n",
      "tensor(3.8143) tensor(5.0699)\n",
      "feature dims: torch.Size([1000, 320])\n",
      "epoch_reset_labels: 1\n",
      "target_nclusters: 6\n",
      "label_pre_table: [[34, 37], [8, 11], [2, 6], [0], [1], [3], [4], [5], [7], [9], [10], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [35], [36], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86], [87], [88], [89], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104], [105], [106], [107], [108], [109], [110], [111], [112], [113], [114], [115], [116], [117], [118], [119], [120], [121], [122], [123], [124], [125], [126], [127], [128], [129], [130], [131], [132], [133], [134], [135], [136], [137], [138], [139], [140], [141], [142], [143], [144], [145], [146], [147], [148], [149], [150], [151], [152], [153], [154], [155], [156], [157], [158], [159], [160], [161], [162], [163], [164], [165], [166], [167], [168], [169], [170], [171], [172], [173], [174], [175], [176], [177], [178], [179], [180], [181], [182], [183], [184], [185], [186], [187], [188], [189], [190], [191], [192], [193], [194], [195], [196], [197], [198], [199], [200], [201], [202], [203], [204], [205], [206], [207], [208], [209], [210], [211], [212], [213], [214], [215], [216], [217], [218], [219], [220], [221], [222], [223], [224], [225], [226], [227], [228], [229], [230], [231], [232], [233], [234], [235], [236], [237], [238], [239], [240], [241], [242], [243], [244], [245], [246], [247], [248], [249], [250], [251], [252], [253], [254], [255], [256], [257], [258], [259], [260], [261], [262], [263], [264], [265], [266], [267], [268], [269], [270], [271], [272], [273], [274], [275], [276], [277], [278], [279], [280], [281], [282], [283], [284], [285], [286], [287], [288], [289], [290], [291], [292], [293], [294], [295], [296], [297], [298], [299], [300], [301], [302], [303], [304], [305], [306], [307], [308], [309], [310], [311], [312], [313], [314], [315], [316], [317], [318], [319], [320], [321], [322], [323], [324], [325], [326], [327], [328], [329], [330], [331], [332], [333], [334], [335], [336], [337], [338], [339], [340], [341], [342], [343], [344], [345], [346], [347], [348], [349], [350], [351], [352], [353], [354], [355], [356], [357], [358], [359], [360], [361], [362], [363], [364], [365], [366], [367], [368], [369], [370], [371], [372], [373], [374], [375], [376], [377], [378], [379], [380], [381], [382], [383], [384], [385], [386], [387], [388], [389], [390], [391], [392], [393], [394], [395], [396], [397], [398], [399], [400], [401], [402], [403], [404], [405], [406], [407], [408], [409], [410], [411], [412], [413], [414], [415], [416], [417], [418], [419], [420], [421], [422], [423], [424], [425], [426], [427], [428], [429], [430], [431], [432], [433], [434], [435], [436], [437], [438], [439], [440], [441], [442], [443], [444], [445], [446], [447], [448], [449], [450], [451], [452], [453], [454], [455], [456], [457], [458], [459], [460], [461], [462], [463], [464], [465], [466], [467], [468], [469], [470], [471], [472], [473], [474], [475], [476], [477], [478], [479], [480], [481], [482], [483], [484], [485], [486], [487], [488], [489], [490], [491], [492], [493], [494], [495], [496], [497], [498], [499], [500], [501], [502], [503], [504], [505], [506], [507], [508], [509], [510], [511], [512], [513], [514], [515], [516], [517], [518], [519], [520], [521], [522], [523], [524], [525], [526], [527], [528], [529], [530], [531], [532], [533], [534], [535], [536], [537], [538], [539], [540], [541], [542], [543], [544], [545], [546], [547], [548], [549], [550], [551], [552], [553], [554], [555], [556], [557], [558], [559], [560], [561], [562], [563], [564], [565], [566], [567], [568], [569], [570], [571], [572], [573], [574], [575], [576], [577], [578], [579], [580], [581], [582], [583], [584], [585], [586], [587], [588], [589], [590], [591], [592], [593], [594], [595], [596], [597], [598], [599], [600], [601], [602], [603], [604], [605], [606], [607], [608], [609], [610], [611], [612], [613], [614], [615], [616], [617], [618], [619], [620], [621], [622], [623], [624], [625], [626], [627], [628], [629], [630], [631], [632], [633], [634], [635], [636], [637], [638], [639], [640], [641], [642], [643], [644], [645], [646], [647], [648], [649], [650], [651], [652], [653], [654], [655], [656], [657], [658], [659], [660], [661], [662], [663], [664], [665], [666], [667], [668], [669], [670], [671], [672], [673], [674], [675], [676], [677], [678], [679], [680], [681], [682], [683], [684], [685], [686], [687], [688], [689], [690], [691], [692], [693], [694], [695], [696], [697], [698], [699], [700], [701], [702], [703], [704], [705], [706], [707], [708], [709], [710], [711], [712], [713], [714], [715], [716], [717], [718], [719], [720], [721], [722], [723], [724], [725], [726], [727], [728], [729], [730], [731], [732], [733], [734], [735], [736], [737], [738], [739], [740], [741], [742], [743], [744], [745], [746], [747], [748], [749], [750], [751], [752], [753], [754], [755], [756], [757], [758], [759], [760], [761], [762], [763], [764], [765], [766], [767], [768], [769], [770], [771], [772], [773], [774], [775], [776], [777], [778], [779], [780], [781], [782], [783], [784], [785], [786], [787], [788], [789], [790], [791], [792], [793], [794], [795], [796], [797], [798], [799], [800], [801], [802], [803], [804], [805], [806], [807], [808], [809], [810], [811], [812], [813], [814], [815], [816], [817], [818], [819], [820], [821], [822], [823], [824], [825], [826], [827], [828], [829], [830], [831], [832], [833], [834], [835], [836], [837], [838], [839], [840], [841], [842], [843], [844], [845], [846], [847], [848], [849], [850], [851], [852], [853], [854], [855], [856], [857], [858], [859], [860], [861], [862], [863], [864], [865], [866], [867], [868], [869], [870], [871], [872], [873], [874], [875], [876], [877], [878], [879], [880], [881], [882], [883], [884], [885], [886], [887], [888], [889], [890], [891], [892], [893], [894], [895], [896], [897], [898], [899], [900], [901], [902], [903], [904], [905], [906], [907], [908], [909], [910], [911], [912], [913], [914], [915], [916], [917], [918], [919], [920], [921], [922], [923], [924], [925], [926], [927], [928], [929], [930], [931], [932], [933], [934], [935], [936], [937], [938], [939], [940], [941], [942], [943], [944], [945], [946], [947], [948], [949], [950], [951], [952], [953], [954], [955], [956], [957], [958], [959], [960], [961], [962], [963], [964], [965], [966], [967], [968], [969], [970], [971], [972], [973], [974], [975], [976], [977], [978], [979], [980], [981], [982], [983], [984], [985], [986], [987], [988], [989], [990], [991], [992], [993], [994], [995], [996], [997], [998], [999]]\n",
      "saiteki_number_list [6, 9]\n",
      "saiteki_class: 9\n",
      "False\n",
      "sigma: tensor(0.5459, dtype=torch.float64)\n",
      "nclusters:  997\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=4, nsamples_c_a=0, idx_c_b=9, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=16, nsamples_c_a=1, idx_c_b=17, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=21, nsamples_c_a=0, idx_c_b=54, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=17, nsamples_c_a=0, idx_c_b=29, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=31, nsamples_c_a=1, idx_c_b=381, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=34, nsamples_c_a=0, idx_c_b=59, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=37, nsamples_c_a=0, idx_c_b=55, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=39, nsamples_c_a=0, idx_c_b=43, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=51, nsamples_c_a=0, idx_c_b=54, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=51, nsamples_c_a=0, idx_c_b=53, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=54, nsamples_c_a=0, idx_c_b=55, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=57, nsamples_c_a=0, idx_c_b=58, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=46, nsamples_c_a=2, idx_c_b=59, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=69, nsamples_c_a=0, idx_c_b=387, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=71, nsamples_c_a=0, idx_c_b=194, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=16, nsamples_c_a=1, idx_c_b=73, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=17, nsamples_c_a=0, idx_c_b=74, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=86, nsamples_c_a=2, idx_c_b=87, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=75, nsamples_c_a=2, idx_c_b=89, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=95, nsamples_c_a=0, idx_c_b=113, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=98, nsamples_c_a=0, idx_c_b=249, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=87, nsamples_c_a=0, idx_c_b=101, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=103, nsamples_c_a=0, idx_c_b=113, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=55, nsamples_c_a=1, idx_c_b=104, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=111, nsamples_c_a=0, idx_c_b=284, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=95, nsamples_c_a=0, idx_c_b=113, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=108, nsamples_c_a=2, idx_c_b=116, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=86, nsamples_c_a=2, idx_c_b=117, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=116, nsamples_c_a=0, idx_c_b=121, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=145, nsamples_c_a=0, idx_c_b=249, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=153, nsamples_c_a=2, idx_c_b=159, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=156, nsamples_c_a=1, idx_c_b=162, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=168, nsamples_c_a=0, idx_c_b=307, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=159, nsamples_c_a=0, idx_c_b=172, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=174, nsamples_c_a=0, idx_c_b=208, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=199, nsamples_c_a=1, idx_c_b=200, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=210, nsamples_c_a=1, idx_c_b=396, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=206, nsamples_c_a=2, idx_c_b=212, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=183, nsamples_c_a=1, idx_c_b=220, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=220, nsamples_c_a=0, idx_c_b=228, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=229, nsamples_c_a=0, idx_c_b=428, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=235, nsamples_c_a=0, idx_c_b=243, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=237, nsamples_c_a=0, idx_c_b=258, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=215, nsamples_c_a=1, idx_c_b=239, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=242, nsamples_c_a=2, idx_c_b=243, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=245, nsamples_c_a=2, idx_c_b=246, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=159, nsamples_c_a=0, idx_c_b=247, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=252, nsamples_c_a=0, idx_c_b=274, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=248, nsamples_c_a=2, idx_c_b=259, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=260, nsamples_c_a=0, idx_c_b=276, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=264, nsamples_c_a=0, idx_c_b=276, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=276, nsamples_c_a=0, idx_c_b=286, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=268, nsamples_c_a=0, idx_c_b=278, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=130, nsamples_c_a=2, idx_c_b=299, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=301, nsamples_c_a=2, idx_c_b=305, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=313, nsamples_c_a=0, idx_c_b=493, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=168, nsamples_c_a=0, idx_c_b=320, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=268, nsamples_c_a=0, idx_c_b=332, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=339, nsamples_c_a=0, idx_c_b=453, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=136, nsamples_c_a=3, idx_c_b=341, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=346, nsamples_c_a=1, idx_c_b=394, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=349, nsamples_c_a=1, idx_c_b=441, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=350, nsamples_c_a=0, idx_c_b=539, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=356, nsamples_c_a=1, idx_c_b=357, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=354, nsamples_c_a=2, idx_c_b=357, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=364, nsamples_c_a=1, idx_c_b=396, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=191, nsamples_c_a=1, idx_c_b=370, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=372, nsamples_c_a=0, idx_c_b=409, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=248, nsamples_c_a=2, idx_c_b=374, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=368, nsamples_c_a=2, idx_c_b=378, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=31, nsamples_c_a=2, idx_c_b=381, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=381, nsamples_c_a=0, idx_c_b=383, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=386, nsamples_c_a=1, idx_c_b=394, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=370, nsamples_c_a=0, idx_c_b=388, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=394, nsamples_c_a=0, idx_c_b=396, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=394, nsamples_c_a=0, idx_c_b=396, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=398, nsamples_c_a=0, idx_c_b=410, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=400, nsamples_c_a=0, idx_c_b=428, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=390, nsamples_c_a=3, idx_c_b=402, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=398, nsamples_c_a=0, idx_c_b=404, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=396, nsamples_c_a=0, idx_c_b=405, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=409, nsamples_c_a=1, idx_c_b=428, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=398, nsamples_c_a=0, idx_c_b=410, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=412, nsamples_c_a=0, idx_c_b=428, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=414, nsamples_c_a=0, idx_c_b=423, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=409, nsamples_c_a=1, idx_c_b=428, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=432, nsamples_c_a=0, idx_c_b=435, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=435, nsamples_c_a=1, idx_c_b=441, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=442, nsamples_c_a=1, idx_c_b=443, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=432, nsamples_c_a=0, idx_c_b=448, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=451, nsamples_c_a=0, idx_c_b=452, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=453, nsamples_c_a=0, idx_c_b=461, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=461, nsamples_c_a=0, idx_c_b=469, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=472, nsamples_c_a=2, idx_c_b=473, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=432, nsamples_c_a=0, idx_c_b=477, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=481, nsamples_c_a=0, idx_c_b=484, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=493, nsamples_c_a=0, idx_c_b=494, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=268, nsamples_c_a=0, idx_c_b=499, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=453, nsamples_c_a=0, idx_c_b=503, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=505, nsamples_c_a=2, idx_c_b=506, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=521, nsamples_c_a=0, idx_c_b=545, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=522, nsamples_c_a=1, idx_c_b=523, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=523, nsamples_c_a=0, idx_c_b=861, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=524, nsamples_c_a=0, idx_c_b=855, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=525, nsamples_c_a=1, idx_c_b=530, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=529, nsamples_c_a=1, idx_c_b=532, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=535, nsamples_c_a=0, idx_c_b=715, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=540, nsamples_c_a=1, idx_c_b=715, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=544, nsamples_c_a=1, idx_c_b=715, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=558, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=550, nsamples_c_a=1, idx_c_b=714, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=558, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=540, nsamples_c_a=2, idx_c_b=566, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=567, nsamples_c_a=2, idx_c_b=579, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=584, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=586, nsamples_c_a=0, idx_c_b=694, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=590, nsamples_c_a=0, idx_c_b=593, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=592, nsamples_c_a=1, idx_c_b=725, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=590, nsamples_c_a=0, idx_c_b=593, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=601, nsamples_c_a=0, idx_c_b=715, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=612, nsamples_c_a=0, idx_c_b=617, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=270, nsamples_c_a=1, idx_c_b=614, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=615, nsamples_c_a=2, idx_c_b=616, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=639, nsamples_c_a=1, idx_c_b=647, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=646, nsamples_c_a=1, idx_c_b=648, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=638, nsamples_c_a=2, idx_c_b=652, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=662, nsamples_c_a=0, idx_c_b=663, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=486, nsamples_c_a=0, idx_c_b=673, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=523, nsamples_c_a=0, idx_c_b=674, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=678, nsamples_c_a=1, idx_c_b=694, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=609, nsamples_c_a=1, idx_c_b=679, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=111, nsamples_c_a=0, idx_c_b=683, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=344, nsamples_c_a=2, idx_c_b=685, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=535, nsamples_c_a=0, idx_c_b=693, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=586, nsamples_c_a=0, idx_c_b=694, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=697, nsamples_c_a=1, idx_c_b=709, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=562, nsamples_c_a=2, idx_c_b=703, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=707, nsamples_c_a=0, idx_c_b=726, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=709, nsamples_c_a=0, idx_c_b=754, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=714, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=537, nsamples_c_a=2, idx_c_b=715, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=523, nsamples_c_a=0, idx_c_b=717, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=718, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=545, nsamples_c_a=0, idx_c_b=723, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=554, nsamples_c_a=1, idx_c_b=725, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=728, nsamples_c_a=0, idx_c_b=729, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=730, nsamples_c_a=0, idx_c_b=744, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=570, nsamples_c_a=1, idx_c_b=731, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=537, nsamples_c_a=2, idx_c_b=734, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=725, nsamples_c_a=0, idx_c_b=738, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=579, nsamples_c_a=0, idx_c_b=747, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=752, nsamples_c_a=1, idx_c_b=861, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=750, nsamples_c_a=1, idx_c_b=753, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=742, nsamples_c_a=1, idx_c_b=754, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=765, nsamples_c_a=2, idx_c_b=766, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=769, nsamples_c_a=1, idx_c_b=770, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=776, nsamples_c_a=0, idx_c_b=789, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=779, nsamples_c_a=0, idx_c_b=790, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=782, nsamples_c_a=0, idx_c_b=790, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=625, nsamples_c_a=3, idx_c_b=790, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=779, nsamples_c_a=0, idx_c_b=792, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=800, nsamples_c_a=1, idx_c_b=805, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=651, nsamples_c_a=0, idx_c_b=803, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=795, nsamples_c_a=2, idx_c_b=805, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=648, nsamples_c_a=0, idx_c_b=813, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=808, nsamples_c_a=2, idx_c_b=815, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=658, nsamples_c_a=1, idx_c_b=818, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=803, nsamples_c_a=0, idx_c_b=819, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=489, nsamples_c_a=1, idx_c_b=822, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=660, nsamples_c_a=1, idx_c_b=828, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=835, nsamples_c_a=1, idx_c_b=839, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=532, nsamples_c_a=0, idx_c_b=854, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=524, nsamples_c_a=0, idx_c_b=855, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=699, nsamples_c_a=2, idx_c_b=857, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=709, nsamples_c_a=0, idx_c_b=860, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=523, nsamples_c_a=0, idx_c_b=861, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=863, nsamples_c_a=1, idx_c_b=864, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=863, nsamples_c_a=1, idx_c_b=864, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=754, nsamples_c_a=0, idx_c_b=867, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=714, nsamples_c_a=0, idx_c_b=884, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=885, nsamples_c_a=0, idx_c_b=886, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=859, nsamples_c_a=1, idx_c_b=886, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=888, nsamples_c_a=1, idx_c_b=896, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=888, nsamples_c_a=1, idx_c_b=896, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=898, nsamples_c_a=1, idx_c_b=906, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=899, nsamples_c_a=1, idx_c_b=904, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=902, nsamples_c_a=0, idx_c_b=906, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=901, nsamples_c_a=2, idx_c_b=903, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=904, nsamples_c_a=0, idx_c_b=906, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=902, nsamples_c_a=0, idx_c_b=906, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=903, nsamples_c_a=0, idx_c_b=909, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=910, nsamples_c_a=1, idx_c_b=912, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=906, nsamples_c_a=0, idx_c_b=916, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=612, nsamples_c_a=0, idx_c_b=921, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=936, nsamples_c_a=0, idx_c_b=974, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=923, nsamples_c_a=2, idx_c_b=940, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=461, nsamples_c_a=0, idx_c_b=945, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=803, nsamples_c_a=0, idx_c_b=950, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=951, nsamples_c_a=2, idx_c_b=954, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=954, nsamples_c_a=0, idx_c_b=961, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=969, nsamples_c_a=0, idx_c_b=970, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=936, nsamples_c_a=0, idx_c_b=974, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=934, nsamples_c_a=2, idx_c_b=981, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=981, nsamples_c_a=0, idx_c_b=987, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=989, nsamples_c_a=2, idx_c_b=991, nsamples_c_b=0\n",
      "Time elapsed for computing cluster affinity: 35.10136890411377 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 653\n",
      "==> testing\n",
      "NMI: 0.4753344655036926\n",
      " \n",
      "loss: 0.10777921229600906\n",
      "juleloss 0.10777921229600906\n",
      "\n",
      "Epoch #40: loss=13.54714584350586\n",
      "loss_k1,loss_k2\n",
      "tensor(5.0793) tensor(6.4228)\n",
      "loss: 0.15795418620109558\n",
      "juleloss 0.15795418620109558\n",
      "\n",
      "Epoch #41: loss=17.45524787902832\n",
      "loss_k1,loss_k2\n",
      "tensor(7.5736) tensor(1.9177)\n",
      "loss: 0.16303913295269012\n",
      "juleloss 0.16303913295269012\n",
      "\n",
      "Epoch #42: loss=14.649576187133789\n",
      "loss_k1,loss_k2\n",
      "tensor(6.6178) tensor(5.7270)\n",
      "loss: 0.1730699986219406\n",
      "juleloss 0.1730699986219406\n",
      "\n",
      "Epoch #43: loss=18.798463821411133\n",
      "loss_k1,loss_k2\n",
      "tensor(5.7331) tensor(3.9178)\n",
      "loss: 0.1595313996076584\n",
      "juleloss 0.1595313996076584\n",
      "\n",
      "Epoch #44: loss=15.19912052154541\n",
      "loss_k1,loss_k2\n",
      "tensor(3.7762) tensor(2.5494)\n",
      "loss: 0.1588772088289261\n",
      "juleloss 0.1588772088289261\n",
      "\n",
      "Epoch #45: loss=11.14139175415039\n",
      "loss_k1,loss_k2\n",
      "tensor(2.8016) tensor(6.0307)\n",
      "loss: 0.15974876284599304\n",
      "juleloss 0.15974876284599304\n",
      "\n",
      "Epoch #46: loss=14.062810897827148\n",
      "loss_k1,loss_k2\n",
      "tensor(2.7619) tensor(2.5478)\n",
      "loss: 0.16429297626018524\n",
      "juleloss 0.16429297626018524\n",
      "\n",
      "Epoch #47: loss=9.72756576538086\n",
      "loss_k1,loss_k2\n",
      "tensor(3.7637) tensor(5.0660)\n",
      "loss: 0.1621353179216385\n",
      "juleloss 0.1621353179216385\n",
      "\n",
      "Epoch #48: loss=14.217183113098145\n",
      "loss_k1,loss_k2\n",
      "tensor(2.3432) tensor(1.9944)\n",
      "loss: 0.16963407397270203\n",
      "juleloss 0.16963407397270203\n",
      "\n",
      "Epoch #49: loss=8.763936996459961\n",
      "loss_k1,loss_k2\n",
      "tensor(6.6492) tensor(6.0481)\n",
      "loss: 0.14232465624809265\n",
      "juleloss 0.14232465624809265\n",
      "\n",
      "Epoch #50: loss=18.871688842773438\n",
      "loss_k1,loss_k2\n",
      "tensor(4.8392) tensor(6.5645)\n",
      "loss: 0.15603213012218475\n",
      "juleloss 0.15603213012218475\n",
      "\n",
      "Epoch #51: loss=17.339984893798828\n",
      "loss_k1,loss_k2\n",
      "tensor(1.8071) tensor(1.9322)\n",
      "loss: 0.15520299971103668\n",
      "juleloss 0.15520299971103668\n",
      "\n",
      "Best model updated: loss=6.762763500213623\n",
      "Epoch #52: loss=6.762763500213623\n",
      "loss_k1,loss_k2\n",
      "tensor(5.9144) tensor(5.1476)\n",
      "loss: 0.1579681634902954\n",
      "juleloss 0.1579681634902954\n",
      "\n",
      "Epoch #53: loss=17.11372184753418\n",
      "loss_k1,loss_k2\n",
      "tensor(3.6541) tensor(4.1091)\n",
      "loss: 0.1552075743675232\n",
      "juleloss 0.1552075743675232\n",
      "\n",
      "Epoch #54: loss=12.93802261352539\n",
      "loss_k1,loss_k2\n",
      "tensor(2.7608) tensor(3.6285)\n",
      "loss: 0.15397833287715912\n",
      "juleloss 0.15397833287715912\n",
      "\n",
      "Epoch #55: loss=11.143488883972168\n",
      "loss_k1,loss_k2\n",
      "tensor(0.8086) tensor(3.6633)\n",
      "loss: 0.16503353416919708\n",
      "juleloss 0.16503353416919708\n",
      "\n",
      "Epoch #56: loss=7.955499649047852\n",
      "loss_k1,loss_k2\n",
      "tensor(5.3160) tensor(5.1311)\n",
      "loss: 0.16404078900814056\n",
      "juleloss 0.16404078900814056\n",
      "\n",
      "Epoch #57: loss=16.372032165527344\n",
      "loss_k1,loss_k2\n",
      "tensor(4.2007) tensor(4.7605)\n",
      "loss: 0.16365797817707062\n",
      "juleloss 0.16365797817707062\n",
      "\n",
      "Epoch #58: loss=14.655557632446289\n",
      "loss_k1,loss_k2\n",
      "tensor(4.6698) tensor(3.5934)\n",
      "loss: 0.16452758014202118\n",
      "juleloss 0.16452758014202118\n",
      "\n",
      "Epoch #59: loss=13.620335578918457\n",
      "loss_k1,loss_k2\n",
      "tensor(2.9641) tensor(2.1640)\n",
      "feature dims: torch.Size([1000, 320])\n",
      "epoch_reset_labels: 2\n",
      "target_nclusters: 6\n",
      "label_pre_table: [[34, 37, 44], [8, 11], [2, 6], [0, 1], [3], [4, 10], [5, 7], [982], [9, 18], [12], [13], [14], [15, 26], [16], [17], [19], [20, 22], [21], [23], [24, 60], [25], [27, 384], [28], [29, 54], [30], [31, 36], [32, 67], [33], [35, 42], [38, 45], [39, 40], [41, 57], [43, 47, 48, 51, 53], [861], [46], [911, 893], [49, 62], [50, 52], [930, 931], [887], [55], [56], [58], [59, 108], [61], [63], [64, 397], [65, 148], [66, 527, 70], [68, 107, 69], [71, 72], [73, 74], [75, 76], [77], [78, 92], [79, 93], [80, 101], [81, 114], [82], [83], [84, 120], [85], [86], [87], [88, 98], [89, 90], [91], [996], [94, 96], [95], [881], [97], [902], [99, 117], [100, 124], [102], [103], [104], [105, 106], [897, 909], [901], [946, 941], [109, 267], [110, 232], [111, 116], [112, 137], [113, 119], [115], [886, 889], [118, 263], [125], [121], [122], [123], [862], [126, 132, 331], [127], [128], [129, 334], [130], [131, 467], [133, 149], [134], [135], [136, 377], [138], [139, 344, 330], [140], [141, 142, 143], [921], [920], [144, 154], [145, 158], [146, 319], [147, 157, 163, 164], [150, 165], [151], [152, 313], [153], [155], [156, 162, 320], [961], [891], [159, 328], [160], [161], [166], [167, 171, 174, 321], [168], [169], [170], [172], [173], [175], [176, 177], [178, 201], [179, 202], [180, 189, 195, 374], [181, 212, 233, 194], [182, 183], [978], [184, 419], [185, 187, 224], [186, 243], [938], [188, 193], [875, 883, 885], [190, 353, 363], [191], [192, 403], [986, 969, 970], [942], [877], [196, 401, 206], [197, 210, 216, 218, 402, 411, 414], [198, 203], [199, 214], [200, 697], [890], [204], [975], [205, 250], [207, 399], [208], [209, 215], [917, 746, 747, 760, 765], [211, 225], [213], [919], [217], [219, 409], [220], [221, 227, 228], [222, 223], [937, 984], [226, 410], [229], [230, 373], [231], [944], [234, 431], [235, 413], [236, 446], [237, 238], [239, 240], [241, 242], [244], [245, 246], [247], [248, 249], [251, 262], [252, 378], [253], [254, 255], [256], [257], [258], [259], [260], [261], [980], [981], [264, 435, 271, 654], [265, 436, 439], [266], [268], [269, 500], [270, 480], [272, 503], [273], [274, 490], [275, 300], [276, 286], [277, 279], [278, 664, 685], [457, 465], [280], [281], [282], [283, 782], [284], [285, 444], [910, 908], [287], [288], [289, 489], [290], [291], [292], [293], [294, 295], [296, 481], [297, 482], [298], [299, 454], [301, 302], [303, 823], [304, 308], [305], [306], [307], [309], [310], [311], [312], [314], [315, 316], [317, 318], [870], [995], [322], [323], [324], [325], [326], [327], [329], [332], [333], [879], [335], [336, 650], [337], [338], [339, 456], [340], [341, 342], [343], [345], [346, 846], [347, 688, 847], [348], [349], [350, 548], [351, 533], [352], [354, 408], [355, 538, 855], [356, 524, 390], [357, 360, 849], [358, 361], [359], [871], [362], [364, 417], [365, 535], [366, 526], [367], [368, 375], [369, 376], [370, 422], [371, 381], [372, 382], [990], [876, 884], [874], [949], [379, 717], [380, 728], [880, 907], [383, 415], [385], [386], [387], [388, 733], [389], [391], [392, 569], [393, 405, 395, 740], [394, 406], [396], [959], [398, 400], [963], [923], [404, 432], [898, 915], [407], [992, 994], [412], [425], [922, 928], [434], [416, 421], [418, 427, 433], [420], [951], [895], [423], [424], [974], [426, 428], [429], [430], [985], [437, 448], [438], [989], [440, 443], [441, 450], [442], [953], [445], [967], [447], [449], [451], [452], [453], [455], [458], [459], [460, 933], [461, 464], [462], [463], [904, 906], [466], [866], [468, 479], [469], [470, 484], [471], [472], [473, 646], [474], [475, 476], [477], [478], [912], [483, 651], [485], [486, 939], [487], [488, 806], [491, 669], [492], [493], [494, 604], [495, 496], [497, 498], [968], [499], [947], [501], [502], [896], [504, 513], [505], [506], [507], [508, 509], [510], [511], [512], [514], [515, 617], [516], [517], [518, 680], [519], [520, 741, 521], [522, 589, 693], [523], [525], [998], [999], [528, 859, 868], [529], [530, 539], [531, 737], [532], [987], [534, 692], [536, 856], [537, 544], [952], [540, 718], [541, 567], [542, 551], [543, 552], [966], [545, 710], [546, 712], [547], [964], [549, 562], [550, 555, 865], [553], [554, 864], [976], [556, 708], [557, 709], [558, 743], [559, 711, 730], [560], [561], [958], [563], [564], [565, 706], [566, 585], [852, 858], [568], [570, 582], [571, 736], [572, 584], [573, 590, 729], [574, 579], [575], [576], [577], [578, 734], [580, 731], [581, 598], [927], [583], [586], [587], [588, 850], [926, 943], [997], [591], [592, 593], [594, 597], [857], [595], [596], [878, 899], [599, 600], [601, 715], [602, 605], [603, 608, 716], [954, 957, 960], [606], [607], [873, 882], [609, 761], [610, 612], [611], [991], [613, 640], [614, 615], [616], [618, 619], [620], [621], [622], [623, 665], [624], [625, 784], [626, 668], [627, 633], [628, 793, 684, 988], [629, 783, 794], [630], [631, 632], [894, 905], [892, 918], [634], [635], [636], [637], [638], [639], [641, 655, 659], [642], [643], [644], [645], [900], [647], [648], [649], [652], [929], [653, 795], [656], [657], [658], [660], [661], [662], [663], [666, 828], [667], [670], [671], [672, 831], [673], [674, 682], [675], [676], [677], [678], [679], [681], [683], [686], [687, 773], [689], [690, 888], [691], [694, 853, 854], [695], [696], [698, 742], [699], [700], [701], [702, 860], [703], [704], [705, 863], [707], [993], [965], [713, 758], [714], [956], [872], [719], [720], [721], [722, 727], [723, 724], [977], [725], [726], [932], [983], [971], [903], [732], [979], [735], [738], [739], [948], [744, 867], [745], [913], [973], [748, 869], [749, 914, 916], [750], [751, 757], [752, 756], [753], [754], [755], [759], [940, 950], [762], [763], [764, 766], [767, 779], [768, 769], [770], [771], [772], [774], [775, 791, 839], [776, 785], [777, 786], [778], [780], [781, 936], [925], [945], [787], [788, 844], [789], [790, 801], [935], [792, 797], [924], [796], [798, 808], [799], [800], [802], [803], [804], [805], [807], [809], [810], [811, 818], [812, 817], [813], [814, 842], [815, 821], [816], [819], [820], [822], [824, 825], [826], [827], [829], [830], [843], [832, 837], [833, 838], [834], [835, 851], [836], [955, 962], [934, 972], [840], [841, 845], [848]]\n",
      "saiteki_number_list [6, 9, 10]\n",
      "saiteki_class: 10\n",
      "False\n",
      "sigma: tensor(0.4796, dtype=torch.float64)\n",
      "nclusters:  653\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=16, nsamples_c_a=0, idx_c_b=17, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=48, nsamples_c_a=0, idx_c_b=154, nsamples_c_b=7\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=48, nsamples_c_a=0, idx_c_b=50, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=54, nsamples_c_a=2, idx_c_b=114, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=60, nsamples_c_a=0, idx_c_b=78, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=89, nsamples_c_a=2, idx_c_b=93, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=94, nsamples_c_a=0, idx_c_b=407, nsamples_c_b=3\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=114, nsamples_c_a=0, idx_c_b=127, nsamples_c_b=4\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=158, nsamples_c_a=1, idx_c_b=294, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=160, nsamples_c_a=0, idx_c_b=330, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=197, nsamples_c_a=1, idx_c_b=221, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=220, nsamples_c_a=1, idx_c_b=353, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=221, nsamples_c_a=0, idx_c_b=335, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=202, nsamples_c_a=4, idx_c_b=228, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=95, nsamples_c_a=3, idx_c_b=232, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=251, nsamples_c_a=2, idx_c_b=252, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=232, nsamples_c_a=0, idx_c_b=257, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=279, nsamples_c_a=0, idx_c_b=288, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=157, nsamples_c_a=0, idx_c_b=286, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=288, nsamples_c_a=0, idx_c_b=291, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=294, nsamples_c_a=0, idx_c_b=476, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=295, nsamples_c_a=1, idx_c_b=498, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=307, nsamples_c_a=2, idx_c_b=546, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=319, nsamples_c_a=0, idx_c_b=333, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=324, nsamples_c_a=0, idx_c_b=331, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=324, nsamples_c_a=0, idx_c_b=328, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=343, nsamples_c_a=0, idx_c_b=632, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=353, nsamples_c_a=0, idx_c_b=355, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=357, nsamples_c_a=1, idx_c_b=361, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=353, nsamples_c_a=0, idx_c_b=360, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=347, nsamples_c_a=1, idx_c_b=361, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=366, nsamples_c_a=0, idx_c_b=370, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=327, nsamples_c_a=1, idx_c_b=386, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=225, nsamples_c_a=3, idx_c_b=399, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=402, nsamples_c_a=0, idx_c_b=415, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=404, nsamples_c_a=1, idx_c_b=546, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=402, nsamples_c_a=0, idx_c_b=414, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=415, nsamples_c_a=0, idx_c_b=543, nsamples_c_b=3\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=418, nsamples_c_a=0, idx_c_b=451, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=419, nsamples_c_a=0, idx_c_b=445, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=423, nsamples_c_a=2, idx_c_b=424, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=424, nsamples_c_a=0, idx_c_b=434, nsamples_c_b=3\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=432, nsamples_c_a=2, idx_c_b=445, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=437, nsamples_c_a=0, idx_c_b=620, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=424, nsamples_c_a=0, idx_c_b=440, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=432, nsamples_c_a=2, idx_c_b=445, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=445, nsamples_c_a=0, idx_c_b=447, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=448, nsamples_c_a=0, idx_c_b=452, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=448, nsamples_c_a=0, idx_c_b=456, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=35, nsamples_c_a=2, idx_c_b=468, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=454, nsamples_c_a=4, idx_c_b=470, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=454, nsamples_c_a=4, idx_c_b=471, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=479, nsamples_c_a=1, idx_c_b=484, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=484, nsamples_c_a=0, idx_c_b=496, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=484, nsamples_c_a=0, idx_c_b=488, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=299, nsamples_c_a=2, idx_c_b=498, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=484, nsamples_c_a=0, idx_c_b=500, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=359, nsamples_c_a=5, idx_c_b=505, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=496, nsamples_c_a=2, idx_c_b=507, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=508, nsamples_c_a=0, idx_c_b=509, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=512, nsamples_c_a=1, idx_c_b=526, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=370, nsamples_c_a=2, idx_c_b=513, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=335, nsamples_c_a=3, idx_c_b=526, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=531, nsamples_c_a=0, idx_c_b=650, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=526, nsamples_c_a=0, idx_c_b=538, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=484, nsamples_c_a=0, idx_c_b=539, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=402, nsamples_c_a=0, idx_c_b=541, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=403, nsamples_c_a=1, idx_c_b=542, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=434, nsamples_c_a=4, idx_c_b=545, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=434, nsamples_c_a=4, idx_c_b=546, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=407, nsamples_c_a=4, idx_c_b=550, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=484, nsamples_c_a=0, idx_c_b=562, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=157, nsamples_c_a=0, idx_c_b=563, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=546, nsamples_c_a=0, idx_c_b=576, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=420, nsamples_c_a=2, idx_c_b=579, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=165, nsamples_c_a=6, idx_c_b=586, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=639, nsamples_c_a=1, idx_c_b=643, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=643, nsamples_c_a=0, idx_c_b=644, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=644, nsamples_c_a=0, idx_c_b=645, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=644, nsamples_c_a=0, idx_c_b=650, nsamples_c_b=1\n",
      "Time elapsed for computing cluster affinity: 25.344430685043335 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 446\n",
      "==> testing\n",
      "NMI: 0.4536587595939636\n",
      " \n",
      "loss: 0.1502789705991745\n",
      "juleloss 0.1502789705991745\n",
      "\n",
      "Epoch #60: loss=9.4833345413208\n",
      "loss_k1,loss_k2\n",
      "tensor(2.7367) tensor(1.3415)\n",
      "loss: 0.16861535608768463\n",
      "juleloss 0.16861535608768463\n",
      "\n",
      "Epoch #61: loss=8.37709903717041\n",
      "loss_k1,loss_k2\n",
      "tensor(3.5682) tensor(4.9581)\n",
      "loss: 0.1664627194404602\n",
      "juleloss 0.1664627194404602\n",
      "\n",
      "Epoch #62: loss=14.071232795715332\n",
      "loss_k1,loss_k2\n",
      "tensor(2.3232) tensor(2.4038)\n",
      "loss: 0.15825632214546204\n",
      "juleloss 0.15825632214546204\n",
      "\n",
      "Epoch #63: loss=9.2486572265625\n",
      "loss_k1,loss_k2\n",
      "tensor(1.5162) tensor(2.3470)\n",
      "loss: 0.16507765650749207\n",
      "juleloss 0.16507765650749207\n",
      "\n",
      "Epoch #64: loss=7.8470659255981445\n",
      "loss_k1,loss_k2\n",
      "tensor(1.5011) tensor(1.9077)\n",
      "loss: 0.1579635590314865\n",
      "juleloss 0.1579635590314865\n",
      "\n",
      "Epoch #65: loss=7.364446640014648\n",
      "loss_k1,loss_k2\n",
      "tensor(2.5339) tensor(2.8994)\n",
      "loss: 0.15671297907829285\n",
      "juleloss 0.15671297907829285\n",
      "\n",
      "Epoch #66: loss=9.978907585144043\n",
      "loss_k1,loss_k2\n",
      "tensor(4.1342) tensor(3.6667)\n",
      "loss: 0.1634768694639206\n",
      "juleloss 0.1634768694639206\n",
      "\n",
      "Epoch #67: loss=13.1550931930542\n",
      "loss_k1,loss_k2\n",
      "tensor(2.3699) tensor(2.6610)\n",
      "loss: 0.16988368332386017\n",
      "juleloss 0.16988368332386017\n",
      "\n",
      "Epoch #68: loss=9.628402709960938\n",
      "loss_k1,loss_k2\n",
      "tensor(4.5955) tensor(2.0647)\n",
      "loss: 0.1589789092540741\n",
      "juleloss 0.1589789092540741\n",
      "\n",
      "Epoch #69: loss=11.523016929626465\n",
      "loss_k1,loss_k2\n",
      "tensor(4.0261) tensor(1.6445)\n",
      "loss: 0.15979298949241638\n",
      "juleloss 0.15979298949241638\n",
      "\n",
      "Epoch #70: loss=10.323348045349121\n",
      "loss_k1,loss_k2\n",
      "tensor(0.7994) tensor(0.9014)\n",
      "loss: 0.1624050736427307\n",
      "juleloss 0.1624050736427307\n",
      "\n",
      "Best model updated: loss=5.250110626220703\n",
      "Epoch #71: loss=5.250110626220703\n",
      "loss_k1,loss_k2\n",
      "tensor(2.3429) tensor(2.6513)\n",
      "loss: 0.1610586792230606\n",
      "juleloss 0.1610586792230606\n",
      "\n",
      "Epoch #72: loss=9.518109321594238\n",
      "loss_k1,loss_k2\n",
      "tensor(2.2580) tensor(0.7826)\n",
      "loss: 0.16546976566314697\n",
      "juleloss 0.16546976566314697\n",
      "\n",
      "Epoch #73: loss=6.915566444396973\n",
      "loss_k1,loss_k2\n",
      "tensor(2.9526) tensor(2.4282)\n",
      "loss: 0.15438665449619293\n",
      "juleloss 0.15438665449619293\n",
      "\n",
      "Epoch #74: loss=9.932389259338379\n",
      "loss_k1,loss_k2\n",
      "tensor(4.2300) tensor(3.8232)\n",
      "loss: 0.16008970141410828\n",
      "juleloss 0.16008970141410828\n",
      "\n",
      "Epoch #75: loss=13.440071105957031\n",
      "loss_k1,loss_k2\n",
      "tensor(4.1997) tensor(4.1281)\n",
      "loss: 0.15701903402805328\n",
      "juleloss 0.15701903402805328\n",
      "\n",
      "Epoch #76: loss=13.803102493286133\n",
      "loss_k1,loss_k2\n",
      "tensor(2.8446) tensor(2.5087)\n",
      "loss: 0.1615685075521469\n",
      "juleloss 0.1615685075521469\n",
      "\n",
      "Epoch #77: loss=10.060933113098145\n",
      "loss_k1,loss_k2\n",
      "tensor(1.3203) tensor(1.5011)\n",
      "loss: 0.15767809748649597\n",
      "juleloss 0.15767809748649597\n",
      "\n",
      "Epoch #78: loss=6.646200180053711\n",
      "loss_k1,loss_k2\n",
      "tensor(3.6612) tensor(3.2221)\n",
      "loss: 0.16047945618629456\n",
      "juleloss 0.16047945618629456\n",
      "\n",
      "Epoch #79: loss=11.89345932006836\n",
      "loss_k1,loss_k2\n",
      "tensor(2.1486) tensor(1.4005)\n",
      "feature dims: torch.Size([1000, 320])\n",
      "epoch_reset_labels: 3\n",
      "target_nclusters: 6\n",
      "label_pre_table: [[34, 37, 44], [8, 11], [2, 6], [0, 1, 999, 998, 997, 993, 995], [3], [4, 10, 12], [5, 7], [982], [9, 18, 25], [13, 14], [814, 842], [15, 26, 20, 22, 23], [16], [17, 33], [19, 21], [721], [924], [748, 869], [24, 60, 43, 47, 48, 51, 53, 56], [720], [27, 384, 386], [28], [29, 54], [30], [31, 36], [32, 67, 66, 527, 70], [805], [35, 42], [38, 45], [39, 40], [41, 57], [861, 744, 867], [46, 61], [911, 893, 895], [49, 62], [50, 52, 55], [930, 931], [887, 537, 544], [58, 63], [59, 108], [925, 926, 943], [64, 397], [65, 148, 168], [68, 107, 69], [71, 72], [73, 74, 147, 157, 163, 164], [75, 76, 412], [77, 89, 90], [78, 92], [79, 93], [80, 101, 97, 103], [81, 114, 86, 102, 104], [82, 109, 267], [83, 84, 120], [85], [87, 696], [88, 98], [848], [91], [996], [94, 96, 493], [95], [881, 902, 901], [819], [835, 851, 834], [99, 117], [100, 124], [105, 106], [897, 909], [810], [946, 941], [110, 232], [111, 116], [112, 137], [113, 119], [115], [886, 889, 862], [118, 263], [125, 345], [121], [122, 123], [754, 768, 769, 770, 764, 766], [126, 132, 331], [127, 128], [129, 334], [130], [131, 467], [133, 149], [134, 644], [135, 332], [136, 377, 138], [139, 344, 330, 340], [140, 470, 484], [141, 142, 143, 297, 482], [921, 920], [790, 801, 798, 808], [144, 154], [145, 158], [146, 319], [150, 165], [151], [152, 313], [153], [155, 169, 172], [156, 162, 320, 247, 310], [961, 967, 952], [891, 896], [159, 328], [160], [161], [166], [167, 171, 174, 321], [777, 786], [788, 844], [170], [787, 822], [173], [175], [176, 177, 239, 240], [178, 201], [179, 202, 204], [180, 189, 195, 374, 396], [181, 212, 233, 194], [182, 183], [978, 975], [184, 419, 248, 249], [185, 187, 224, 200, 697, 389], [186, 243], [938], [188, 193, 208], [875, 883, 885], [190, 353, 363], [191, 198, 203], [192, 403], [986, 969, 970], [942, 958], [877, 876, 884], [196, 401, 206], [197, 210, 216, 218, 402, 411, 414], [752, 756, 762], [199, 214], [890], [703, 543, 552], [977], [205, 250, 229], [207, 399], [723, 724], [209, 215], [917, 746, 747, 760, 765, 871, 755], [211, 225, 241, 242], [213, 226, 410], [919, 892, 918], [217, 244], [219, 409], [220], [221, 227, 228, 222, 223], [937, 984, 510], [948], [230, 373], [231, 423], [944, 945], [234, 431, 416, 421, 235, 413, 425], [809], [236, 446], [237, 238], [245, 246, 434], [251, 262], [252, 378, 253], [803], [254, 255, 285, 444], [256], [257, 292], [258, 293], [259], [260, 461, 464], [261, 287], [980], [981], [264, 435, 271, 654], [265, 436, 439, 451], [266], [268, 290, 505], [269, 500], [270, 480], [272, 503, 499], [273], [274, 490], [275, 300, 485], [276, 286], [277, 279, 288, 291], [278, 664, 685, 495, 496], [457, 465], [280, 339, 456], [281], [282], [283, 782], [284], [910, 908], [843, 836, 841, 845], [289, 489, 517, 447], [722, 727], [661, 336, 650], [294, 295], [296, 481], [298], [299, 454], [301, 302], [303, 823, 649], [304, 308], [305], [306], [307, 312], [309], [311], [775, 791, 839], [314], [315, 316], [317, 318], [870, 857], [776, 785], [322], [323, 324], [325], [326], [327], [329], [333, 475, 476], [879, 878, 899], [335], [774, 713, 758], [337], [338], [341, 342], [343, 487], [346, 846], [347, 688, 847], [348], [349, 357, 360, 849], [350, 548, 707, 739], [351, 533], [352, 542, 551], [354, 408], [355, 538, 855], [356, 524, 390], [358, 361], [359, 522, 589, 693], [362, 698, 742], [364, 417], [365, 535, 547], [366, 526], [367, 368, 375], [369, 376], [370, 422], [371, 381], [372, 382], [990, 991], [874], [949], [379, 717], [380, 728, 573, 590, 729], [880, 907], [383, 415], [385, 601, 715], [387], [388, 733, 574, 579], [391, 418, 427, 433], [392, 569], [393, 405, 395, 740], [394, 406], [983], [959], [398, 400], [963], [923], [404, 432, 407], [898, 915], [992, 994], [763, 767, 779], [922, 928, 935], [771, 772], [812, 817], [420, 424, 429], [951], [813], [759], [974, 973], [426, 428], [826], [430], [985], [437, 448, 442], [438, 455], [989], [440, 443], [441, 450], [953], [445, 453], [827], [449], [452], [830, 658], [458, 670], [459], [460, 933], [462, 463], [804], [904, 906, 903], [466], [866, 550, 555, 865, 719], [468, 479, 641, 655, 659], [469], [471], [472, 501], [473, 646], [474], [477, 483, 651], [478], [912, 753], [971], [486, 939], [488, 806], [491, 669], [492], [494, 604], [497, 498], [968], [705, 863], [947], [502], [913, 749, 914, 916], [504, 513], [506], [507], [508, 509], [511, 623, 665], [512], [514, 518, 680], [515, 617, 618, 619], [516, 675, 676], [519], [520, 741, 521, 643], [523], [525], [780], [781, 936], [528, 859, 868, 701], [529, 571, 736], [530, 539, 702, 860], [531, 737, 725], [532, 732], [987], [534, 692], [536, 856], [540, 718, 541, 567], [966, 820], [545, 710, 606], [546, 712], [964, 956], [549, 562], [940, 950], [553, 565, 706], [554, 864], [976], [556, 708], [557, 709], [558, 743, 563, 735], [559, 711, 730, 695], [560, 704], [561, 602, 605, 591], [564, 691], [566, 585], [852, 858, 694, 853, 854], [568, 576, 577, 587], [570, 582, 745], [572, 584, 595, 596], [979], [575, 714], [815, 821], [934, 972], [578, 734], [580, 731], [581, 598, 594, 597], [927], [583], [586], [955, 962], [588, 850], [816], [778, 789], [726, 738], [592, 593], [802], [599, 600, 607], [603, 608, 716], [954, 957, 960], [840], [750, 751, 757], [873, 882, 872], [609, 761], [610, 612, 636], [611], [613, 640, 624], [614, 615], [616, 635, 634], [620, 663], [621], [622, 672, 831], [625, 784], [626, 668], [627, 633], [628, 793, 684, 988], [629, 783, 794], [630], [631, 632], [894, 905, 900], [699, 700], [807], [824, 825, 833, 838], [637, 638], [639, 652], [642], [811, 818], [645], [647], [648], [929, 932], [653, 795, 800], [656], [657], [796, 792, 797], [660, 671, 829], [799], [662], [666, 828], [667], [673, 832, 837], [674, 682], [965], [677], [678, 687, 773], [679], [681], [683], [686], [689], [690, 888]]\n",
      "saiteki_number_list [6, 9, 10, 9]\n",
      "saiteki_class: 9\n",
      "False\n",
      "sigma: tensor(0.4446, dtype=torch.float64)\n",
      "nclusters:  446\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=50, nsamples_c_a=0, idx_c_b=51, nsamples_c_b=5\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=78, nsamples_c_a=2, idx_c_b=90, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=108, nsamples_c_a=1, idx_c_b=109, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=119, nsamples_c_a=4, idx_c_b=133, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=156, nsamples_c_a=0, idx_c_b=410, nsamples_c_b=4\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=138, nsamples_c_a=7, idx_c_b=159, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=152, nsamples_c_a=2, idx_c_b=165, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=237, nsamples_c_a=0, idx_c_b=246, nsamples_c_b=3\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=243, nsamples_c_a=3, idx_c_b=258, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=62, nsamples_c_a=3, idx_c_b=260, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=70, nsamples_c_a=2, idx_c_b=285, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=285, nsamples_c_a=0, idx_c_b=289, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=290, nsamples_c_a=3, idx_c_b=293, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=64, nsamples_c_a=3, idx_c_b=300, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=174, nsamples_c_a=3, idx_c_b=304, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=307, nsamples_c_a=4, idx_c_b=315, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=245, nsamples_c_a=4, idx_c_b=347, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=360, nsamples_c_a=0, idx_c_b=445, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=372, nsamples_c_a=4, idx_c_b=374, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=156, nsamples_c_a=0, idx_c_b=378, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=374, nsamples_c_a=0, idx_c_b=383, nsamples_c_b=1\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=105, nsamples_c_a=3, idx_c_b=385, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=257, nsamples_c_a=2, idx_c_b=387, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=283, nsamples_c_a=2, idx_c_b=416, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=86, nsamples_c_a=3, idx_c_b=422, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=115, nsamples_c_a=4, idx_c_b=433, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=411, nsamples_c_a=3, idx_c_b=438, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=245, nsamples_c_a=4, idx_c_b=444, nsamples_c_b=0\n",
      "Time elapsed for computing cluster affinity: 20.65614891052246 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 315\n",
      "==> testing\n",
      "NMI: 0.4412540793418884\n",
      " \n",
      "loss: 0.1580759733915329\n",
      "juleloss 0.1580759733915329\n",
      "\n",
      "Epoch #80: loss=7.765715599060059\n",
      "loss_k1,loss_k2\n",
      "tensor(3.0619) tensor(3.6676)\n",
      "loss: 0.16102111339569092\n",
      "juleloss 0.16102111339569092\n",
      "\n",
      "Epoch #81: loss=11.680848121643066\n",
      "loss_k1,loss_k2\n",
      "tensor(3.8007) tensor(3.5880)\n",
      "loss: 0.15753677487373352\n",
      "juleloss 0.15753677487373352\n",
      "\n",
      "Epoch #82: loss=12.531804084777832\n",
      "loss_k1,loss_k2\n",
      "tensor(3.1031) tensor(1.5532)\n",
      "loss: 0.1568494588136673\n",
      "juleloss 0.1568494588136673\n",
      "\n",
      "Epoch #83: loss=8.985435485839844\n",
      "loss_k1,loss_k2\n",
      "tensor(0.3988) tensor(0.4919)\n",
      "loss: 0.16043996810913086\n",
      "juleloss 0.16043996810913086\n",
      "\n",
      "Best model updated: loss=3.821523427963257\n",
      "Epoch #84: loss=3.821523427963257\n",
      "loss_k1,loss_k2\n",
      "tensor(2.1850) tensor(1.6642)\n",
      "loss: 0.16147495806217194\n",
      "juleloss 0.16147495806217194\n",
      "\n",
      "Epoch #85: loss=8.101608276367188\n",
      "loss_k1,loss_k2\n",
      "tensor(2.4138) tensor(4.3832)\n",
      "loss: 0.15649427473545074\n",
      "juleloss 0.15649427473545074\n",
      "\n",
      "Epoch #86: loss=11.672111511230469\n",
      "loss_k1,loss_k2\n",
      "tensor(3.1614) tensor(2.8245)\n",
      "loss: 0.15278784930706024\n",
      "juleloss 0.15278784930706024\n",
      "\n",
      "Epoch #87: loss=10.730949401855469\n",
      "loss_k1,loss_k2\n",
      "tensor(3.2267) tensor(3.1864)\n",
      "loss: 0.15899136662483215\n",
      "juleloss 0.15899136662483215\n",
      "\n",
      "Epoch #88: loss=11.27695369720459\n",
      "loss_k1,loss_k2\n",
      "tensor(2.3666) tensor(1.8917)\n",
      "loss: 0.15527117252349854\n",
      "juleloss 0.15527117252349854\n",
      "\n",
      "Epoch #89: loss=8.631611824035645\n",
      "loss_k1,loss_k2\n",
      "tensor(3.5714) tensor(3.5489)\n",
      "loss: 0.15442997217178345\n",
      "juleloss 0.15442997217178345\n",
      "\n",
      "Epoch #90: loss=12.307578086853027\n",
      "loss_k1,loss_k2\n",
      "tensor(1.1817) tensor(0.9708)\n",
      "loss: 0.15888936817646027\n",
      "juleloss 0.15888936817646027\n",
      "\n",
      "Epoch #91: loss=5.67697811126709\n",
      "loss_k1,loss_k2\n",
      "tensor(0.7724) tensor(0.7039)\n",
      "loss: 0.15644246339797974\n",
      "juleloss 0.15644246339797974\n",
      "\n",
      "Epoch #92: loss=4.842527389526367\n",
      "loss_k1,loss_k2\n",
      "tensor(3.5575) tensor(2.1679)\n",
      "loss: 0.15438596904277802\n",
      "juleloss 0.15438596904277802\n",
      "\n",
      "Epoch #93: loss=10.312713623046875\n",
      "loss_k1,loss_k2\n",
      "tensor(3.7634) tensor(3.9023)\n",
      "loss: 0.15599234402179718\n",
      "juleloss 0.15599234402179718\n",
      "\n",
      "Epoch #94: loss=12.899641036987305\n",
      "loss_k1,loss_k2\n",
      "tensor(2.6229) tensor(3.0987)\n",
      "loss: 0.1573934108018875\n",
      "juleloss 0.1573934108018875\n",
      "\n",
      "Epoch #95: loss=10.420368194580078\n",
      "loss_k1,loss_k2\n",
      "tensor(3.7522) tensor(4.8767)\n",
      "loss: 0.15453419089317322\n",
      "juleloss 0.15453419089317322\n",
      "\n",
      "Epoch #96: loss=14.124109268188477\n",
      "loss_k1,loss_k2\n",
      "tensor(3.0000) tensor(3.5095)\n",
      "loss: 0.15457187592983246\n",
      "juleloss 0.15457187592983246\n",
      "\n",
      "Epoch #97: loss=11.478809356689453\n",
      "loss_k1,loss_k2\n",
      "tensor(0.7667) tensor(0.7456)\n",
      "loss: 0.15666937828063965\n",
      "juleloss 0.15666937828063965\n",
      "\n",
      "Epoch #98: loss=4.278969764709473\n",
      "loss_k1,loss_k2\n",
      "tensor(1.4013) tensor(1.2838)\n",
      "loss: 0.15498845279216766\n",
      "juleloss 0.15498845279216766\n",
      "\n",
      "Epoch #99: loss=5.941136360168457\n",
      "loss_k1,loss_k2\n",
      "tensor(2.6280) tensor(1.0446)\n",
      "feature dims: torch.Size([1000, 320])\n",
      "epoch_reset_labels: 4\n",
      "target_nclusters: 6\n",
      "label_pre_table: [[34, 37, 44, 24, 60, 43, 47, 48, 51, 53, 56, 49, 62, 58, 63, 31, 36], [8, 11], [2, 6], [0, 1, 999, 998, 997, 993, 995, 996, 992, 994], [3], [4, 10, 12], [5, 7], [982, 978, 975, 976, 979], [9, 18, 25, 28], [13, 14, 16], [814, 842], [15, 26, 20, 22, 23, 30, 19, 21, 17, 33], [966, 820], [721, 580, 731], [924, 929, 932], [748, 869, 917, 746, 747, 760, 765, 871, 755, 870, 857], [566, 585, 572, 584, 595, 596], [720, 705, 863, 530, 539, 702, 860, 556, 708], [27, 384, 386, 387], [686, 458, 670, 512], [29, 54], [690, 888, 525], [561, 602, 605, 591, 586], [32, 67, 66, 527, 70], [805, 653, 795, 800, 809], [35, 42], [38, 45, 39, 40], [41, 57], [861, 744, 867], [46, 61], [911, 893, 895, 898, 915, 881, 902, 901], [588, 850], [50, 52, 55], [930, 931, 923, 927], [887, 537, 544], [59, 108], [925, 926, 943], [64, 397], [65, 148, 168], [68, 107, 69, 379, 717], [71, 72], [73, 74, 147, 157, 163, 164, 166], [75, 76, 412, 440, 443], [77, 89, 90], [78, 92, 80, 101, 97, 103], [79, 93], [81, 114, 86, 102, 104], [82, 109, 267], [83, 84, 120], [85], [87, 696], [88, 98], [848, 553, 565, 706], [91, 94, 96, 493, 115], [678, 687, 773], [673, 832, 837, 622, 672, 831], [95, 99, 117, 111, 116], [627, 633, 630], [819], [835, 851, 834], [100, 124, 118, 263], [105, 106], [897, 909], [810, 804], [946, 941, 948, 940, 950], [110, 232], [112, 137], [113, 119], [616, 635, 634], [886, 889, 862], [125, 345], [121, 145, 158], [122, 123], [754, 768, 769, 770, 764, 766, 778, 789], [126, 132, 331], [127, 128], [129, 334], [130], [131, 467, 338], [133, 149], [134, 644], [135, 332, 136, 377, 138], [139, 344, 330, 340], [140, 470, 484], [141, 142, 143, 297, 482], [921, 920], [790, 801, 798, 808], [144, 154], [146, 319, 315, 316], [150, 165], [151, 161], [152, 313, 314], [153, 155, 169, 172], [679, 689], [156, 162, 320, 247, 310], [961, 967, 952], [891, 896], [159, 328, 160], [894, 905, 900], [325], [603, 608, 716], [167, 171, 174, 321], [777, 786], [788, 844], [170, 173], [787, 822, 830, 658], [175], [176, 177, 239, 240], [178, 201, 192, 403], [179, 202, 204], [180, 189, 195, 374, 396], [181, 212, 233, 194], [182, 183], [583], [184, 419, 248, 249], [185, 187, 224, 200, 697, 389], [186, 243], [938, 947, 486, 939], [188, 193, 208], [875, 883, 885, 874], [190, 353, 363], [191, 198, 203, 199, 214], [986, 969, 970, 983], [942, 958], [877, 876, 884], [196, 401, 206], [197, 210, 216, 218, 402, 411, 414], [752, 756, 762, 759, 750, 751, 757, 609, 761], [890, 880, 907], [703, 543, 552], [977, 937, 984, 510], [205, 250, 229], [207, 399], [723, 724, 531, 737, 725], [209, 215], [610, 612, 636], [211, 225, 241, 242], [213, 226, 410, 219, 409], [919, 892, 918], [217, 244], [220, 231, 423], [221, 227, 228, 222, 223], [230, 373], [944, 945], [234, 431, 416, 421, 235, 413, 425, 430, 420, 424, 429], [236, 446], [237, 238, 245, 246, 434], [251, 262, 266], [252, 378, 253], [803, 660, 671, 829, 965], [254, 255, 285, 444, 449], [256], [257, 292, 282], [258, 293], [259], [260, 461, 464], [261, 287], [980], [981], [264, 435, 271, 654, 306, 322, 335], [265, 436, 439, 451], [796, 792, 797], [268, 290, 505], [269, 500], [270, 480], [272, 503, 499], [273], [274, 490, 341, 342], [275, 300, 485], [276, 286], [277, 279, 288, 291], [278, 664, 685, 495, 496, 667, 674, 682], [457, 465], [280, 339, 456], [281, 294, 295], [283, 782, 781, 936], [284, 462, 463], [910, 908], [843, 836, 841, 845], [289, 489, 517, 447], [722, 727], [661, 336, 650], [570, 582, 745], [296, 481], [298], [299, 454], [301, 302], [303, 823, 649], [304, 308], [305], [307, 312], [309, 311, 329], [639, 652, 637, 638], [775, 791, 839, 516, 675, 676], [621], [317, 318], [776, 785], [323, 324, 327], [811, 818], [326], [726, 738], [333, 475, 476], [879, 878, 899], [774, 713, 758], [337], [343, 487], [346, 846, 666, 828], [347, 688, 847], [348, 349, 357, 360, 849], [350, 548, 707, 739], [351, 533], [352, 542, 551], [354, 408], [355, 538, 855], [356, 524, 390], [358, 361], [359, 522, 589, 693], [362, 698, 742], [364, 417], [365, 535, 547], [366, 526], [367, 368, 375, 369, 376], [370, 422], [371, 381], [372, 382], [990, 991, 974, 973, 989], [626, 668, 647], [949, 816], [380, 728, 573, 590, 729], [383, 415], [385, 601, 715], [388, 733, 574, 579], [391, 418, 427, 433], [392, 569, 578, 734], [393, 405, 395, 740], [394, 406], [568, 576, 577, 587], [959], [398, 400], [963], [599, 600, 607, 611], [404, 432, 407], [575, 714], [763, 767, 779], [922, 928, 935], [771, 772], [812, 817], [581, 598, 594, 597], [951], [813, 807], [620, 663], [802], [426, 428], [826], [683, 656, 625, 784], [985, 987], [437, 448, 442], [438, 455, 459, 452], [441, 450], [564, 691], [953, 955, 962], [445, 453], [827], [657], [613, 640, 624, 648], [460, 933], [662], [904, 906, 903], [466, 477, 483, 651], [866, 550, 555, 865, 719, 554, 864], [468, 479, 641, 655, 659], [469, 471], [824, 825, 833, 838, 840], [472, 501], [473, 646, 474], [478], [912, 753], [971, 934, 972], [488, 806, 492], [491, 669], [494, 604], [497, 498, 523], [968], [799, 815, 821], [502], [913, 749, 914, 916], [504, 513], [506, 677], [507], [508, 509], [511, 623, 665], [514, 518, 680], [515, 617, 618, 619, 519, 681], [520, 741, 521, 643], [642, 645], [780, 629, 783, 794], [852, 858, 694, 853, 854], [528, 859, 868, 701], [529, 571, 736], [954, 957, 960], [532, 732], [534, 692], [536, 856], [540, 718, 541, 567, 699, 700], [873, 882, 872], [545, 710, 606], [546, 712], [964, 956], [549, 562], [631, 632, 628, 793, 684, 988], [614, 615], [557, 709], [558, 743, 563, 735, 592, 593], [559, 711, 730, 695], [560, 704]]\n",
      "saiteki_number_list [6, 9, 10, 9, 6]\n",
      "saiteki_class: 9\n",
      "False\n",
      "sigma: tensor(0.4281, dtype=torch.float64)\n",
      "nclusters:  315\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=43, nsamples_c_a=3, idx_c_b=51, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=126, nsamples_c_a=0, idx_c_b=144, nsamples_c_b=11\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=152, nsamples_c_a=0, idx_c_b=153, nsamples_c_b=2\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=16, nsamples_c_a=6, idx_c_b=182, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=144, nsamples_c_a=11, idx_c_b=224, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=17, nsamples_c_a=9, idx_c_b=297, nsamples_c_b=0\n",
      "Skipping merge as one of the clusters is empty: idx_c_a=64, nsamples_c_a=6, idx_c_b=299, nsamples_c_b=0\n",
      "Time elapsed for computing cluster affinity: 17.84754467010498 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 236\n",
      "==> testing\n",
      "NMI: 0.4364284574985504\n",
      " \n",
      "loss: 0.15406249463558197\n",
      "juleloss 0.15406249463558197\n",
      "\n",
      "Epoch #100: loss=7.558139801025391\n",
      "loss_k1,loss_k2\n",
      "tensor(3.2910) tensor(2.3636)\n",
      "loss: 0.15171879529953003\n",
      "juleloss 0.15171879529953003\n",
      "\n",
      "Epoch #101: loss=10.312506675720215\n",
      "loss_k1,loss_k2\n",
      "tensor(3.9203) tensor(1.8736)\n",
      "loss: 0.14677311480045319\n",
      "juleloss 0.14677311480045319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datautils import _get_time_features,load_forecast_csv\n",
    "data, train_slice, valid_slice, test_slice, scaler, pred_lens, data1=load_forecast_csv(\"phone/phone_data_10\",False)\n",
    "\n",
    "# ファイルのパス\n",
    "file_path = 'datasets/phone/y_train.txt'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# データをリストに変換\n",
    "data_y = []\n",
    "for line in lines:\n",
    "    # 行の空白を削除し、カンマで分割して浮動小数点数に変換\n",
    "    numbers = [float(num) for num in line.strip().split(',')]\n",
    "    data_y.append(numbers)\n",
    "\n",
    "# リストをPyTorchのテンソルに変換\n",
    "data_t=torch.tensor(data)\n",
    "tensor_data_y = torch.tensor(data_y)[:1000]\n",
    "tensor_data_y=tensor_data_y.reshape(1000)\n",
    "y=tensor_data_y\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "dataset = CustomVectorDataset(data_t[:,:1000,], y)\n",
    "data_loader = DataLoader(dataset, batch_size=opt.batchSize, shuffle=False)\n",
    "print(data_loader)\n",
    "\n",
    "labels_y=torch.tensor(y)\n",
    "print(labels_y.size())\n",
    "train_data, train_labels = load_data(data_loader)\n",
    "test_data = train_data.clone()\n",
    "test_labels = labels_y.clone()\n",
    "\n",
    "for _ in range(num_networks):\n",
    "    label_gt_table_table.append(cvt2TableLabels(test_labels))\n",
    "    label_pre_table_table.append([])\n",
    "    label_pre_tensor_table.append([])\n",
    "    target_nclusters_table.append(len(label_gt_table_table[-1]))\n",
    "print(target_nclusters_table)\n",
    "\n",
    "ts2_model = TS2Vec(\n",
    "    input_dims=data.shape[-1],\n",
    "    #length_dim=275,\n",
    "    device=\"cpu\",\n",
    "    output_dims=320,\n",
    "    input_total=1,\n",
    "    #max_train_length=300,\n",
    "    #output_dims=10\n",
    ")\n",
    "\n",
    "loss_log = ts2_model.fit(\n",
    "    data[:,:1000,:],\n",
    "    verbose=True,\n",
    "    n_iters=600,\n",
    "    save_model=\"ts2vec_jule_auto_t.pth\"\n",
    ")\n",
    "\n",
    "# merge_labels_final()\n",
    "# eval_perf()\n",
    "# 最終的なクラスタリング結果を取得\n",
    "final_clusters = label_pre_table_table\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "for cluster_id, cluster in enumerate(final_clusters):\n",
    "    print(f\"Cluster {cluster_id}: {cluster}\")\n",
    "\n",
    "sorted_data = []\n",
    "#print(final_clusters)\n",
    "for sublist in final_clusters[0]:\n",
    "    #print(sublist)\n",
    "    if isinstance(sublist, list):\n",
    "        sorted_data.append(sorted(sublist))\n",
    "#print(sorted_data)\n",
    "data_dict={}\n",
    "#予測結果の表示\n",
    "for i, sublist in enumerate(sorted_data):\n",
    "    data_dict[i]=sublist\n",
    "    \n",
    "answer_list=create_answer(data_dict)\n",
    "ari, anmi, nmi=evaluate_clustering(answer_list,y.numpy().tolist())\n",
    "print('ARI: %f, ANMI: %f, NMI: %f' % ( ari, anmi, nmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
