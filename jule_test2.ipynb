{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TripletEmbeddingCriterion(nn.Module):\n",
    "    def __init__(self, margin=0.5, gamma=2):\n",
    "        super(TripletEmbeddingCriterion, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        N = anchor.size(0)\n",
    "        \n",
    "        delta_pos = anchor - positive\n",
    "        delta_neg = anchor - negative\n",
    "\n",
    "        norm_delta_pos = torch.norm(delta_pos, p=2, dim=1)\n",
    "        norm_delta_neg = torch.norm(delta_neg, p=2, dim=1)\n",
    "\n",
    "        norm_delta_pos = norm_delta_pos * norm_delta_pos * self.gamma\n",
    "        norm_delta_neg = norm_delta_neg * norm_delta_neg\n",
    "\n",
    "        delta_pos_neg = norm_delta_pos - norm_delta_neg + self.margin\n",
    "\n",
    "        loss = F.relu(delta_pos_neg)\n",
    "        return loss.mean()\n",
    "\n",
    "    def backward(self, anchor, positive, negative):\n",
    "        N = anchor.size(0)\n",
    "        \n",
    "        delta_pos = anchor - positive\n",
    "        delta_neg = anchor - negative\n",
    "\n",
    "        norm_delta_pos = torch.norm(delta_pos, p=2, dim=1)\n",
    "        norm_delta_neg = torch.norm(delta_neg, p=2, dim=1)\n",
    "\n",
    "        norm_delta_pos = norm_delta_pos * norm_delta_pos * self.gamma\n",
    "        norm_delta_neg = norm_delta_neg * norm_delta_neg\n",
    "\n",
    "        delta_pos_neg = norm_delta_pos - norm_delta_neg + self.margin\n",
    "\n",
    "        mask = (delta_pos_neg > 0).float().view(-1, 1)\n",
    "\n",
    "        grad_anchor = mask * (delta_neg - delta_pos * self.gamma) * (2 / N)\n",
    "        grad_positive = mask * (delta_pos * self.gamma) * (-2 / N)\n",
    "        grad_negative = mask * delta_neg * (2 / N)\n",
    "\n",
    "        return grad_anchor, grad_positive, grad_negative\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "torch_fix_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AggClustering:\n",
    "    def __init__(self):\n",
    "        self.K_c = 5\n",
    "\n",
    "    def init(self, indices):\n",
    "        # nsamples = indices.size(0)\n",
    "        # visited = torch.full((nsamples, 1), -1, dtype=torch.int32)\n",
    "        # count = 0\n",
    "        # for i in range(nsamples):\n",
    "        #     cur_idx = i\n",
    "        #     pos = []\n",
    "        #     while visited[cur_idx, 0] == -1:\n",
    "        #         pos.append(cur_idx)\n",
    "        #         neighbor = 0\n",
    "        #         for k in range(indices.size(0)):\n",
    "        #             neighbor = indices[cur_idx, k].item()\n",
    "        #             if cur_idx != neighbor:\n",
    "        #                 break\n",
    "        #         visited[cur_idx, 0] = -2\n",
    "        #         cur_idx = neighbor\n",
    "        #         if len(pos) > 50:\n",
    "        #             break\n",
    "        #     if visited[cur_idx, 0] < 0:\n",
    "        #         visited[cur_idx, 0] = count\n",
    "        #         count += 1\n",
    "        #     for p in pos:\n",
    "        #         visited[p, 0] = visited[cur_idx, 0]\n",
    "        # label_indice = [[] for _ in range(count)]\n",
    "        # for i in range(nsamples):\n",
    "        #     label_indice[visited[i, 0]].append(i)\n",
    "        # return label_indice\n",
    "\n",
    "        # Initialize labels for input data given KNN indices\n",
    "        nsamples = indices.size(0)\n",
    "        k = indices.size(1)\n",
    "        visited = torch.full((nsamples, 1), -1, dtype=torch.int)\n",
    "        count = 0\n",
    "        \n",
    "        for i in range(nsamples):\n",
    "            cur_idx = i\n",
    "            pos = []\n",
    "            while visited[cur_idx][0] == -1:\n",
    "                pos.append(cur_idx)\n",
    "                neighbor = 0\n",
    "                for k_idx in range(indices[cur_idx].size(0)):\n",
    "                    neighbor = indices[cur_idx][k_idx].item()\n",
    "                    if cur_idx != neighbor:\n",
    "                        break\n",
    "                visited[cur_idx][0] = -2\n",
    "                cur_idx = neighbor\n",
    "                if len(pos) > 50:\n",
    "                    break\n",
    "            \n",
    "            if visited[cur_idx][0] < 0:\n",
    "                visited[cur_idx][0] = count\n",
    "                count += 1\n",
    "            \n",
    "            for j in pos:\n",
    "                visited[j][0] = visited[cur_idx][0]\n",
    "        \n",
    "        label_indices = [[] for _ in range(count)]\n",
    "        \n",
    "        for i in range(nsamples):\n",
    "            label_indices[visited[i][0]].append(i)\n",
    "        \n",
    "        for i in range(count):\n",
    "            if len(label_indices[i]) == 0:\n",
    "                print(\"error\")\n",
    "        \n",
    "        return label_indices\n",
    "\n",
    "    # def merge_two_clusters(self, W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "    #     A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "    #     nsamples_c_a = len(Y_t[idx_c_a])\n",
    "    #     nsamples_c_b = len(Y_t[idx_c_b])\n",
    "    #     ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "    #     A_us_t[idx_c_a, :] *= ratio\n",
    "    #     A_us_t[idx_c_b, :] *= (1 - ratio)\n",
    "    #     A_us_t[idx_c_a, :] += A_us_t[idx_c_b, :]\n",
    "    #     A_us_t[idx_c_a, idx_c_a] = 0\n",
    "    #     A_us_t[:, idx_c_b] = 0\n",
    "    #     A_us_t[idx_c_b, :] = 0\n",
    "    #     Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "    #     Y_t[idx_c_b] = []\n",
    "    #     for i in range(len(Y_t)):\n",
    "    #         if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = 0\n",
    "    #             A_s_t[idx_c_a, i] = 0\n",
    "    #         elif i < idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "    #         elif i > idx_c_a:\n",
    "    #             A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "    #     return A_s_t, A_us_t, Y_t\n",
    "\n",
    "\n",
    "    def merge_two_clusters(W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "        nclusters = len(Y_t)\n",
    "\n",
    "        idx_c_a_tensor = torch.tensor([idx_c_a], dtype=torch.long)\n",
    "        idx_c_b_tensor = torch.tensor([idx_c_b], dtype=torch.long)\n",
    "\n",
    "        A_us_t.index_add_(1, idx_c_a_tensor, A_us_t.index_select(1, idx_c_b_tensor))\n",
    "\n",
    "        nsamples_c_a = len(Y_t[idx_c_a])\n",
    "        nsamples_c_b = len(Y_t[idx_c_b])\n",
    "        ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "\n",
    "        A_us_t[idx_c_a, :] *= ratio\n",
    "        A_us_t[idx_c_b, :] *= (1 - ratio)\n",
    "        A_us_t.index_add_(0, idx_c_a_tensor, A_us_t.index_select(0, idx_c_b_tensor))\n",
    "\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    # def search_clusters(self, A_s_t):\n",
    "    #     A_sorted, idx_sort = torch.sort(A_s_t, dim=1, descending=True)\n",
    "    #     aff = torch.zeros(1, A_sorted.size(1))\n",
    "    #     for i in range(A_sorted.size(1)):\n",
    "    #         aff[0, i] = A_sorted[0, i]\n",
    "    #         if A_sorted.size(1) > 100:\n",
    "    #             for k in range(1, self.K_c):\n",
    "    #                 aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "    #     v_c, idx_c = torch.max(aff, 1)\n",
    "    #     idx_c_b = idx_c.item()\n",
    "    #     idx_c_a = idx_sort[0, idx_c_b].item()\n",
    "    #     if idx_c_a == idx_c_b:\n",
    "    #         raise ValueError(\"Error: idx_c_a == idx_c_b\")\n",
    "    #     if idx_c_a > idx_c_b:\n",
    "    #         idx_c_a, idx_c_b = idx_c_b, idx_c_a\n",
    "    #     return idx_c_a, idx_c_b\n",
    "\n",
    "    def search_clusters(self, A_s_t):\n",
    "        # print(\"cluster numbers:\", nclusters)\n",
    "        nclusters = A_s_t.size(0)  # クラスタの数を取得\n",
    "        A_sorted, idx_sort = torch.sort(A_s_t, dim=0, descending=True)\n",
    "        # print(\"A_s_t: \", A_s_t.size())\n",
    "        # print(\"nclusters: \", nclusters)\n",
    "        # print(\"A_sorted: \", A_sorted)\n",
    "        # print(\"idx_sort: \", idx_sort)\n",
    "        aff = torch.zeros(1, A_sorted.size(1), dtype=torch.float32)\n",
    "\n",
    "        for i in range(A_sorted.size(1)):\n",
    "            aff[0, i] = A_sorted[0, i]\n",
    "            if A_sorted.size(1) > 100:\n",
    "                for k in range(1, self.K_c):\n",
    "                    aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "\n",
    "        v_c, idx_c = torch.max(aff, dim=1)  # each row\n",
    "        # print(\"idx_c: \", idx_c)\n",
    "        # find corresponding cluster labels for two clusters\n",
    "        idx_c_b = idx_c[0].item()         # col\n",
    "        idx_c_a = idx_sort[0, idx_c_b].item()        # row\n",
    "\n",
    "        # インデックスの検証と調整\n",
    "        if idx_c_a >= nclusters or idx_c_b >= nclusters:\n",
    "            raise ValueError(f\"Error: idx_c_a ({idx_c_a}) or idx_c_b ({idx_c_b}) is out of range (nclusters: {nclusters})\")\n",
    "        if idx_c_a == idx_c_b:\n",
    "            print(\"error\")\n",
    "            raise ValueError(\"idx_c_a and idx_c_b are the same\")\n",
    "        elif idx_c_a > idx_c_b:\n",
    "            idx_c_a, idx_c_b = idx_c_b, idx_c_a\n",
    "\n",
    "        return idx_c_a, idx_c_b\n",
    "\n",
    "    # def search_clusters(self,A_s_t):\n",
    "    #     # Sort the tensor along the first dimension in descending order\n",
    "    #     A_sorted, Idx_sort = torch.sort(A_s_t, dim=0, descending=True)\n",
    "        \n",
    "    #     # Initialize affinity tensor\n",
    "    #     aff = torch.zeros(1, A_sorted.size(1))\n",
    "        \n",
    "    #     for i in range(A_sorted.size(1)):\n",
    "    #         aff[0, i] = A_sorted[0, i]\n",
    "    #         if A_sorted.size(1) > 100:\n",
    "    #             for k in range(1, self.K_c ):  # Adjusting index for Python's 0-based indexing\n",
    "    #                 aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]) / (self.K_c - 1)\n",
    "        \n",
    "    #     # Find the maximum value in the affinity tensor along the second dimension\n",
    "    #     v_c, idx_c = torch.max(aff, dim=1)\n",
    "        \n",
    "    #     # Find corresponding cluster labels for two clusters\n",
    "    #     idx_c_b = idx_c.item()  # Converting tensor to integer\n",
    "    #     idx_c_a = Idx_sort[0, idx_c_b].item()  # Converting tensor to integer\n",
    "        \n",
    "    #     if idx_c_a == idx_c_b:\n",
    "    #         print(\"error\")\n",
    "    #         raise ValueError(\"Cluster indices are equal, which indicates an error.\")\n",
    "    #     elif idx_c_a > idx_c_b:\n",
    "    #         idx_c_a, idx_c_b = idx_c_b, idx_c_a  # Swap values\n",
    "        \n",
    "    #     return idx_c_a, idx_c_b\n",
    "\n",
    "\n",
    "    def run_step(self, W, A_s_t, A_us_t, Y_t):\n",
    "        nclusters = len(Y_t)\n",
    "        # print(\"Cluster Num: \", nclusters)\n",
    "        # print(\"numc\",A_s_t.size(0))\n",
    "        idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "        A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "        for i in range(len(Y_t)):\n",
    "            if len(Y_t[i]) > 0 and i != idx_c_a:\n",
    "                W_i = W[Y_t[i], :]\n",
    "                W_i_idx_c_a = W_i[:, Y_t[idx_c_a]]\n",
    "                W_idx_c_a = W[Y_t[idx_c_a], :]\n",
    "                W_idx_c_a_i = W_idx_c_a[:, Y_t[i]]\n",
    "                A_us_t[idx_c_a, i] = torch.sum(torch.mm(W_idx_c_a_i, W_i_idx_c_a))\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / len(Y_t[idx_c_a])**2 + A_us_t[i, idx_c_a] / len(Y_t[i])**2\n",
    "        A_s_t[:, idx_c_b] = 0\n",
    "        A_s_t[idx_c_b, :] = 0\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    # def run_step_fast(self,W, A_s_t, A_us_t, Y_t):\n",
    "    #     # timer = torch.Timer()\n",
    "    #     # get the number of clusters\n",
    "    #     nclusters = len(Y_t)\n",
    "    #     print(\"Cluster Num: \", nclusters)\n",
    "    #     print(\"numc\",A_s_t.size(0))\n",
    "    #     # print(\"Cluster Num: \", nclusters)\n",
    "    #     # find maximal value in A_t\n",
    "    #     idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "    #     # update affinity matrix A_t\n",
    "    #     # update A_t(idx_c_a->i) = A_t(idx_c_a->i) + A_t(idx_c_b->i)\n",
    "    #     A_us_t.index_add_(1, torch.LongTensor([idx_c_a]), A_us_t.index_select(1, torch.LongTensor([idx_c_b])))\n",
    "\n",
    "    #     # update A_t(i->idx_c_a) = r_a * A_t(i->idx_c_a) + r_b * A_t(i->idx_c_b) (fast algorithm)\n",
    "    #     # nsamples in cluster idx_c_a\n",
    "    #     A_us_t.index_add_(0, torch.LongTensor([idx_c_a]), A_us_t.index_select(0, torch.LongTensor([idx_c_b])))\n",
    "        \n",
    "    #     # update cluster labels Y_t   \n",
    "    #     print(\"y_t: \", Y_t)\n",
    "    #     print(\"idx_c_a: \", idx_c_a)\n",
    "    #     print(\"idx_c_b: \", idx_c_b)\n",
    "    #     Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "    #     Y_t[idx_c_b] = []\n",
    "        \n",
    "    #     # update A_s_t   \n",
    "    #     for i in range(nclusters):\n",
    "    #         if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] = 0\n",
    "    #             A_s_t[idx_c_a, i] = 0\n",
    "    #         elif i < idx_c_a:\n",
    "    #             A_s_t[i, idx_c_a] =  A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "    #         elif i > idx_c_a:\n",
    "    #             A_s_t[idx_c_a, i] =  A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "    #     # print(A_us_t.size())\n",
    "    #     # print(nclusters)\n",
    "    #     if idx_c_b != nclusters:\n",
    "    #         # print(idx_c_b)\n",
    "    #         # print(A_us_t.index_select(0, torch.LongTensor([1])))\n",
    "    #         A_us_t.index_copy_(0, torch.LongTensor([idx_c_b]), A_us_t.index_select(0, torch.LongTensor([nclusters-1])))\n",
    "    #         A_us_t.index_copy_(1, torch.LongTensor([idx_c_b]), A_us_t.index_select(1, torch.LongTensor([nclusters-1])))\n",
    "    #         A_us_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "    #         # print(\"Pre: \", A_s_t[:idx_c_b+1, idx_c_b])\n",
    "    #         # print(\"Pre: \", A_s_t[idx_c_b, idx_c_b:nclusters])\n",
    "    #         A_s_t[:idx_c_b+1, idx_c_b] = A_s_t[:idx_c_b+1, nclusters-1]      \n",
    "    #         A_s_t[idx_c_b, idx_c_b:nclusters] = A_s_t[idx_c_b:nclusters, nclusters-1].t()\n",
    "    #         A_s_t[idx_c_b, idx_c_b] = 0\n",
    "    #         # print(\"Cur: \", A_s_t[:idx_c_b+1, idx_c_b])\n",
    "    #         # print(\"Cur: \", A_s_t[idx_c_b, idx_c_b:nclusters])\n",
    "\n",
    "    #         Y_t[idx_c_b].extend(Y_t[nclusters-1])\n",
    "\n",
    "    #     A_us_t = A_us_t[:nclusters-1, :nclusters-1]\n",
    "    #     A_s_t = A_s_t[:nclusters-1, :nclusters-1]\n",
    "    #     del Y_t[nclusters-1]   \n",
    "    #     # print(Y_t)\n",
    "    #     # timer = torch.Timer()\n",
    "    #     # print('Time-2 elapsed: ' .. timer:time().real .. ' seconds')\n",
    "    #     # return updated A_s_t, A_us_t and Y_t\n",
    "    #     return A_s_t, A_us_t, Y_t\n",
    "\n",
    "    def run_step_fast(self, W, A_s_t, A_us_t, Y_t):\n",
    "        nclusters = len(Y_t)\n",
    "        idx_c_a, idx_c_b = self.search_clusters(A_s_t)\n",
    "\n",
    "        A_us_t.index_add_(1, torch.LongTensor([idx_c_a]), A_us_t.index_select(1, torch.LongTensor([idx_c_b])))\n",
    "        A_us_t.index_add_(0, torch.LongTensor([idx_c_a]), A_us_t.index_select(0, torch.LongTensor([idx_c_b])))\n",
    "\n",
    "        # print(\"y_t: \", Y_t)\n",
    "        # print(\"idx_c_a: \", idx_c_a)\n",
    "        # print(\"idx_c_b: \", idx_c_b)\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        if idx_c_b != nclusters - 1:\n",
    "            A_us_t.index_copy_(0, torch.LongTensor([idx_c_b]), A_us_t.index_select(0, torch.LongTensor([nclusters-1])))\n",
    "            A_us_t.index_copy_(1, torch.LongTensor([idx_c_b]), A_us_t.index_select(1, torch.LongTensor([nclusters-1])))\n",
    "            A_us_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "            A_s_t[:idx_c_b+1, idx_c_b] = A_s_t[:idx_c_b+1, nclusters-1].clone()\n",
    "            A_s_t[idx_c_b, idx_c_b:nclusters] = A_s_t[idx_c_b:nclusters, nclusters-1].clone().t()\n",
    "            A_s_t[idx_c_b, idx_c_b] = 0\n",
    "\n",
    "            Y_t[idx_c_b].extend(Y_t[nclusters-1])\n",
    "\n",
    "        # Update the size of A_s_t and A_us_t to match the new number of clusters\n",
    "        A_us_t = A_us_t[:nclusters-1, :nclusters-1]\n",
    "        A_s_t = A_s_t[:nclusters-1, :nclusters-1]\n",
    "        del Y_t[nclusters-1]\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, W, A_unsym_0, A_sym_0, Y_0, T, K_c_in, use_fast):\n",
    "        nclusters = len(Y_0)\n",
    "        A_sym_0_sum = torch.sum(A_sym_0, dim=1)\n",
    "        self.K_c = K_c_in\n",
    "        t = 0\n",
    "        while t < T:\n",
    "            if use_fast:\n",
    "                A_sym_0, A_unsym_0, Y_0 = self.run_step_fast(W, A_sym_0, A_unsym_0, Y_0)\n",
    "            else:\n",
    "                A_sym_0, A_unsym_0, Y_0 = self.run_step(W, A_sym_0, A_unsym_0, Y_0)\n",
    "            t += 1\n",
    "        Y_T = [cluster for cluster in Y_0 if len(cluster) > 0]\n",
    "        return Y_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "\n",
    "class Affinity:\n",
    "    def compute(self, X, k):\n",
    "        if X.size(0) > 50000:\n",
    "            ind = torch.arange(1, X.size(0) + 1).long().split(10000)\n",
    "            dists = torch.zeros(X.size(0), k + 1, dtype=X.dtype)\n",
    "            indices = torch.zeros(X.size(0), k + 1, dtype=torch.int)\n",
    "\n",
    "            for v in ind:\n",
    "                nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X)\n",
    "                dists_batch, indices_batch = nbrs.kneighbors(X[v - 1])\n",
    "                dists[v - 1] = torch.tensor(dists_batch)\n",
    "                indices[v - 1] = torch.tensor(indices_batch)\n",
    "        else:\n",
    "            nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(X)\n",
    "            dists, indices = nbrs.kneighbors(X)\n",
    "            dists = torch.tensor(dists)\n",
    "            indices = torch.tensor(indices)\n",
    "\n",
    "        sigma_square = torch.mean(dists[:, 1:k+1])\n",
    "        print(\"sigma:\", torch.sqrt(sigma_square))\n",
    "\n",
    "        nsamples = X.size(0)\n",
    "        W = torch.zeros(nsamples, nsamples)\n",
    "\n",
    "        for i in range(nsamples):\n",
    "            for j in range(1, k + 1):\n",
    "                nn_ind = indices[i][j]\n",
    "                W[i][nn_ind] = torch.exp(-dists[i][j] / sigma_square)\n",
    "        \n",
    "        return dists, indices, W\n",
    "\n",
    "    def compute4cluster(self, X, W, Y_0, k, k_target):\n",
    "        nclusters = len(Y_0)\n",
    "        dim = X.size(1)\n",
    "        X_clusters = torch.zeros(nclusters, dim)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            X_clusters[i] = torch.mean(X[torch.LongTensor(Y_0[i])], dim=0)\n",
    "\n",
    "        nbrs = NearestNeighbors(n_neighbors=k, algorithm='auto').fit(X_clusters)\n",
    "        dists, indices = nbrs.kneighbors(X_clusters)\n",
    "        dists = torch.tensor(dists)\n",
    "        indices = torch.tensor(indices)\n",
    "\n",
    "        NNs = torch.zeros(nclusters, nclusters)\n",
    "        # print(indices.size())\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(1, indices.size(1)):\n",
    "                nn_ind = indices[i][j]\n",
    "                NNs[i][nn_ind] = 1\n",
    "\n",
    "        max_number = max(len(y) for y in Y_0)\n",
    "        Y_0_tensor = torch.zeros(nclusters, max_number)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(len(Y_0[i])):\n",
    "                Y_0_tensor[i][j] = Y_0[i][j]\n",
    "\n",
    "        timer = time.time()\n",
    "        A_unsym_0_c, A_sym_0_c = self.compute_CAff(W, NNs, Y_0_tensor)\n",
    "\n",
    "        if k > 20 * k_target:\n",
    "            A_unsym_0_c = A_unsym_0_c.double()\n",
    "            A_sym_0_c = A_sym_0_c.double()\n",
    "\n",
    "            A_unsym_0_c_sum_r = torch.sum(A_unsym_0_c, dim=1)\n",
    "            A_unsym_0_c_sum_c = torch.sum(A_unsym_0_c, dim=0)\n",
    "\n",
    "            for i in range(nclusters):\n",
    "                if A_unsym_0_c_sum_r[i] == 0 and A_unsym_0_c_sum_c[i] == 0:\n",
    "                    idx_a = i\n",
    "                    idx_b = 0\n",
    "                    for k in range(indices.size(1)):\n",
    "                        if indices[i][k] != i:\n",
    "                            idx_b = indices[i][k]\n",
    "                            break\n",
    "\n",
    "                    if idx_b > 0:\n",
    "                        if idx_a > idx_b:\n",
    "                            print(\"merge\", idx_b, idx_a)\n",
    "                            A_sym_0_c, A_unsym_0_c, Y_0 = self.merge_two_clusters(W, A_sym_0_c, A_unsym_0_c, Y_0, idx_b, idx_a)\n",
    "                        else:\n",
    "                            print(\"merge\", idx_a, idx_b)\n",
    "                            A_sym_0_c, A_unsym_0_c, Y_0 = self.merge_two_clusters(W, A_sym_0_c, A_unsym_0_c, Y_0, idx_a, idx_b)\n",
    "\n",
    "                        A_unsym_0_c_sum_r = torch.sum(A_unsym_0_c, dim=1)\n",
    "                        A_unsym_0_c_sum_c = torch.sum(A_unsym_0_c, dim=0)\n",
    "\n",
    "        print('Time elapsed for computing cluster affinity:', time.time() - timer, 'seconds')\n",
    "        # print(\"Y_0: \", Y_0)\n",
    "        # print(\"A_unsym_0_c: \", A_unsym_0_c.size())\n",
    "        # print(\"A_sym_0_c: \", A_sym_0_c.size())\n",
    "        return A_unsym_0_c, A_sym_0_c, Y_0\n",
    "\n",
    "    # def compute_CAff(self, W, NNs, Y_0_tensor):\n",
    "    #     # Placeholder function to simulate compute_CAff. Actual implementation required.\n",
    "    #     A_unsym_0_c = torch.rand(W.size())\n",
    "    #     A_sym_0_c = torch.rand(W.size())\n",
    "    #     return A_unsym_0_c, A_sym_0_c\n",
    "\n",
    "    def compute_CAff(self,W, NNs, Y):\n",
    "        nclusters = NNs.size(0)\n",
    "        \n",
    "        A_us = torch.zeros_like(NNs)\n",
    "        A_s = torch.zeros_like(NNs)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            for j in range(i, nclusters):\n",
    "                if NNs[i, j] == 0 and NNs[j, i] == 0:\n",
    "                    A_us[j, i] = 0\n",
    "                    A_us[i, j] = 0\n",
    "                    A_s[j, i] = 0\n",
    "                    A_s[i, j] = 0\n",
    "                    continue\n",
    "\n",
    "                if i == j:\n",
    "                    A_us[j, i] = 0\n",
    "                    A_s[j, i] = 0\n",
    "                    continue\n",
    "\n",
    "                # get the size of Y[i] and Y[j]\n",
    "                Y_i_size = (Y[i] != 0).sum().item()\n",
    "                Y_j_size = (Y[j] != 0).sum().item()\n",
    "\n",
    "                # compute affinity from cluster i to cluster j\n",
    "                A_c_i_j = 0\n",
    "                for m in range(Y_i_size):\n",
    "                    s_W_c_j_i = 0\n",
    "                    s_W_c_i_j = 0\n",
    "                    for n in range(Y_j_size):\n",
    "                        s_W_c_j_i += W[Y[j, n].long() - 1, Y[i, m].long() - 1]\n",
    "                        s_W_c_i_j += W[Y[i, m].long() - 1, Y[j, n].long() - 1]\n",
    "                    A_c_i_j += s_W_c_j_i * s_W_c_i_j\n",
    "\n",
    "                # compute affinity from cluster j to cluster i\n",
    "                A_c_j_i = 0\n",
    "                for m in range(Y_j_size):\n",
    "                    s_W_c_j_i = 0\n",
    "                    s_W_c_i_j = 0\n",
    "                    for n in range(Y_i_size):\n",
    "                        s_W_c_j_i += W[Y[j, m].long() - 1, Y[i, n].long() - 1]\n",
    "                        s_W_c_i_j += W[Y[i, n].long() - 1, Y[j, m].long() - 1]\n",
    "                    A_c_j_i += s_W_c_i_j * s_W_c_j_i\n",
    "\n",
    "                A_us[j, i] = A_c_i_j\n",
    "                A_us[i, j] = A_c_j_i\n",
    "                A_s[i, j] = A_c_i_j / (Y_j_size ** 2) + A_c_j_i / (Y_i_size ** 2)\n",
    "                A_s[j, i] = 0\n",
    "\n",
    "        return A_us, A_s\n",
    "\n",
    "\n",
    "    def merge_two_clusters(self, W, A_s_t, A_us_t, Y_t, idx_c_a, idx_c_b):\n",
    "        nclusters = len(Y_t)\n",
    "\n",
    "        A_us_t[:, idx_c_a] += A_us_t[:, idx_c_b]\n",
    "\n",
    "        nsamples_c_a = len(Y_t[idx_c_a])\n",
    "        nsamples_c_b = len(Y_t[idx_c_b])\n",
    "        ratio = nsamples_c_a / (nsamples_c_a + nsamples_c_b)\n",
    "\n",
    "        A_us_t[idx_c_a] *= ratio\n",
    "        A_us_t[idx_c_b] *= 1 - ratio\n",
    "        A_us_t[idx_c_a] += A_us_t[idx_c_b]\n",
    "        A_us_t[idx_c_a, idx_c_a] = 0\n",
    "        A_us_t[:, idx_c_b] = 0\n",
    "        A_us_t[idx_c_b, :] = 0\n",
    "\n",
    "        Y_t[idx_c_a].extend(Y_t[idx_c_b])\n",
    "        Y_t[idx_c_b] = []\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if len(Y_t[i]) == 0 or i == idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = 0\n",
    "                A_s_t[idx_c_a, i] = 0\n",
    "            elif i < idx_c_a:\n",
    "                A_s_t[i, idx_c_a] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "            elif i > idx_c_a:\n",
    "                A_s_t[idx_c_a, i] = A_us_t[idx_c_a, i] / (len(Y_t[idx_c_a]) ** 2) + A_us_t[i, idx_c_a] / (len(Y_t[i]) ** 2)\n",
    "\n",
    "        return A_s_t, A_us_t, Y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:\n",
    "    def NMI(self, labels_gt, labels_pre):\n",
    "        N = sum(len(l) for l in labels_gt)\n",
    "        # Compute entropy for labels_gt\n",
    "        pr_gt = torch.zeros(len(labels_gt), 1)\n",
    "        for i, label in enumerate(labels_gt):\n",
    "            pr_gt[i] = len(label) / N\n",
    "        pr_gt_log = torch.log(pr_gt)\n",
    "        H_gt = -torch.sum(pr_gt * pr_gt_log)\n",
    "\n",
    "        # Compute entropy for labels_pre\n",
    "        pr_pre = torch.zeros(len(labels_pre), 1)\n",
    "        for i, label in enumerate(labels_pre):\n",
    "            pr_pre[i] = len(label) / N\n",
    "        pr_pre_log = torch.log(pr_pre)\n",
    "        H_pre = -torch.sum(pr_pre * pr_pre_log)\n",
    "\n",
    "        # Compute mutual information\n",
    "        # Build M_gt\n",
    "        M_gt = torch.zeros(N, len(labels_gt))\n",
    "        for i, label in enumerate(labels_gt):\n",
    "            for j in label:\n",
    "                if j < N:  # Ensure the index is within bounds\n",
    "                    M_gt[j, i] = 1  # Keep it zero-based\n",
    "\n",
    "        # Build M_pre\n",
    "        M_pre = torch.zeros(N, len(labels_pre))\n",
    "        for i, label in enumerate(labels_pre):\n",
    "            for j in label:\n",
    "                if j < N:  # Ensure the index is within bounds\n",
    "                    M_pre[j, i] = 1  # Keep it zero-based\n",
    "\n",
    "        pr_gp = torch.mm(M_gt.t(), M_pre) / N\n",
    "        pr_gp_log = torch.log(pr_gp + 1e-10)\n",
    "        H_gp = -torch.sum(pr_gp * pr_gp_log)\n",
    "\n",
    "        # Compute mutual information\n",
    "        MI = H_gt + H_pre - H_gp\n",
    "        NMI = MI / torch.sqrt(H_gt * H_pre)\n",
    "\n",
    "        return NMI.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接スクリプト内で設定するオプション\n",
    "class Options:\n",
    "    dataset = 'custom'\n",
    "    eta = 0.2\n",
    "    epoch_rnn = 1\n",
    "    batchSize = 10\n",
    "    learningRate = 0.01\n",
    "    weightDecay = 5e-5\n",
    "    momentum = 0.9\n",
    "    gamma_lr = 0.0001\n",
    "    power_lr = 0.75\n",
    "    num_nets = 1\n",
    "    epoch_pp = 20\n",
    "    epoch_max = 1000\n",
    "    K_s = 5\n",
    "    K_c = 5\n",
    "    gamma_tr = 1\n",
    "    margin_tr = 0.2\n",
    "    num_nsampling = 5\n",
    "    use_fast = 1\n",
    "    updateCNN = 1\n",
    "    centralize_input = 0\n",
    "    centralize_feature = 0\n",
    "    normalize = 1\n",
    "\n",
    "opt = Options()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "(7352, 10)\n",
      "Index([   0.0,    1.0,    2.0,    3.0,    4.0,    5.0,    6.0,    7.0,    8.0,\n",
      "          9.0,\n",
      "       ...\n",
      "       7342.0, 7343.0, 7344.0, 7345.0, 7346.0, 7347.0, 7348.0, 7349.0, 7350.0,\n",
      "       7351.0],\n",
      "      dtype='float64', name='date', length=7352)\n",
      "(7352, 10)\n",
      "data\n",
      "(7352, 10)\n",
      "slice(None, 4411, None)\n",
      "slice(4411, 5881, None)\n",
      "slice(5881, None, None)\n",
      "(1, 7352, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katoutsubasa/ts2vec/datautils.py:140: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data = pd.read_csv(f'datasets/{name}.csv', index_col='date', parse_dates=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datautils import _get_time_features,load_forecast_csv\n",
    "data, train_slice, valid_slice, test_slice, scaler, pred_lens, data1=load_forecast_csv(\"phone/phone_data_10\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from ts2vec import TS2Vec\n",
    "ts2_model = TS2Vec(\n",
    "    input_dims=data.shape[-1],\n",
    "    length_dim=275,\n",
    "    device=\"cpu\",\n",
    "    output_dims=320,\n",
    "    input_total=1,\n",
    "    max_train_length=300,\n",
    "    #output_dims=10\n",
    ")\n",
    "ts2_model.load('phone_600_wind1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "all_repr = ts2_model.encode(\n",
    "        data,\n",
    "        causal=False,\n",
    "        sliding_length=1,\n",
    "        sliding_padding= 200,\n",
    "        batch_size=256,\n",
    "        #encoding_window='multiscale'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2d = all_repr.reshape(7352, 320)\n",
    "test_2d=test_2d[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.from_numpy(test_2d).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルのパス\n",
    "file_path = 'datasets/phone/y_train.txt'\n",
    "\n",
    "# ファイルを読み込む\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# データをリストに変換\n",
    "data = []\n",
    "for line in lines:\n",
    "    # 行の空白を削除し、カンマで分割して浮動小数点数に変換\n",
    "    numbers = [float(num) for num in line.strip().split(',')]\n",
    "    data.append(numbers)\n",
    "\n",
    "# リストをPyTorchのテンソルに変換\n",
    "tensor_data = torch.tensor(data)[:1000]\n",
    "tensor_data=tensor_data.reshape(1000)\n",
    "y=tensor_data\n",
    "#print(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 320])\n",
      "torch.Size([1000])\n",
      "NaN in X: tensor(False)\n",
      "NaN in y: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "# # ベクトルデータの定義\n",
    "# input_size = 320  # 入力ベクトルの次元\n",
    "# X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "# y = torch.randint(0, 5, (50,))  # 0から4までのランダムなラベル\n",
    "print(X.size())\n",
    "print(y.size())\n",
    "print(\"NaN in X:\", torch.isnan(X).any())\n",
    "print(\"NaN in y:\", torch.isnan(y).any())\n",
    "\n",
    "class CustomVectorDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# データセットとデータローダーの作成\n",
    "dataset = CustomVectorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=opt.batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのロード\n",
    "def load_data(data_loader):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for vectors, lbls in data_loader:\n",
    "        data.append(vectors)\n",
    "        labels.append(lbls)\n",
    "    data = torch.cat(data)\n",
    "    labels = torch.cat(labels)\n",
    "    return data, labels\n",
    "\n",
    "train_data, train_labels = load_data(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        2., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 4., 4., 4., 4., 4., 4., 4.,\n",
      "        4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 6., 6., 6., 6.,\n",
      "        6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.,\n",
      "        6., 6., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 2., 2., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
      "        5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# if opt.centralize_input == 1:\n",
    "#     train_data -= train_data.mean(dim=0, keepdim=True)\n",
    "\n",
    "test_data = train_data.clone()\n",
    "test_labels = train_labels.clone()\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "affinity = Affinity()\n",
    "evaluate = Evaluate()\n",
    "agg_clustering = AggClustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(input_size):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_size, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 10)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> configuring model\n"
     ]
    }
   ],
   "source": [
    "# Initialize CNN models and variables\n",
    "print('==> configuring model')\n",
    "num_networks = opt.num_nets\n",
    "network_table = []\n",
    "optimizer_table = []\n",
    "criterion_triplet = TripletEmbeddingCriterion(opt.margin_tr, opt.gamma_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_networks):\n",
    "    model = load_model(input_size)\n",
    "    model.apply(init_weights)\n",
    "    network_table.append(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.learningRate, weight_decay=opt.weightDecay, momentum=opt.momentum)\n",
    "    optimizer_table.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_gt_table_table = []\n",
    "label_pre_table_table = []\n",
    "label_pre_tensor_table = []\n",
    "target_nclusters_table = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989], [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 296, 297, 298, 299, 300, 301, 302, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967], [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919]]\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "def cvt2TableLabels(labels):\n",
    "    unique_labels = torch.unique(labels)\n",
    "    label_table = {label.item(): [] for label in unique_labels}\n",
    "    for idx, label in enumerate(labels):\n",
    "        label_table[label.item()].append(idx)\n",
    "    return list(label_table.values())\n",
    "\n",
    "print(cvt2TableLabels(test_labels))\n",
    "for _ in range(num_networks):\n",
    "    label_gt_table_table.append(cvt2TableLabels(test_labels))\n",
    "    label_pre_table_table.append([])\n",
    "    label_pre_tensor_table.append([])\n",
    "    target_nclusters_table.append(len(label_gt_table_table[-1]))\n",
    "print(target_nclusters_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch_reset_labels = [0] * num_networks\n",
    "\n",
    "def getnClusters(label_pre):\n",
    "    nClusters = 0\n",
    "    for cluster in label_pre:\n",
    "        if len(cluster) > 0:\n",
    "            nClusters += 1\n",
    "    return nClusters\n",
    "\n",
    "def update_labels(features, label_pre, target_clusters, iter):\n",
    "    # print(\"compute affinity, \", features.size())\n",
    "    d, ind, W = affinity.compute(features, opt.K_s)  # sigma_l not used here\n",
    "    # sigma = sigma_l\n",
    "    if iter == 0:\n",
    "        print(\"initialize clusters...\")\n",
    "        # print(\"ind\", ind.size())\n",
    "        label_pre = agg_clustering.init(ind)\n",
    "        # print(\"nclusters: \", getnClusters(label_pre))\n",
    "        return label_pre\n",
    "\n",
    "    print(\"nclusters: \", getnClusters(label_pre))\n",
    "    A_us, A_s, label_pre = affinity.compute4cluster(features, W, label_pre, getnClusters(label_pre), target_clusters)\n",
    "    # print(\"nclusters affinity_compute: \", getnClusters(label_pre))\n",
    "    n_clusters = getnClusters(label_pre)\n",
    "    # print(\"A_s\", A_s.size())\n",
    "    # print(\"new_n_clusters\", n_clusters)\n",
    "    #12\n",
    "    print(\"run agglomerative clustering...\")\n",
    "\n",
    "    # Convert n_clusters to tensor\n",
    "    n_clusters_tensor = torch.tensor(n_clusters, dtype=torch.float32)\n",
    "    unfold_iter = torch.ceil(n_clusters_tensor * opt.eta).item()\n",
    "    unfold_valid_iter = n_clusters - target_clusters\n",
    "    iterations = min(unfold_iter, unfold_valid_iter)\n",
    "\n",
    "    if iterations <= 0:\n",
    "        print(\"nclusters:1Time: \", getnClusters(label_pre))\n",
    "        return label_pre\n",
    "\n",
    "    label_pre = agg_clustering.run(W, A_us, A_s, label_pre, iterations, opt.K_c, opt.use_fast)\n",
    "    return label_pre\n",
    "\n",
    "def extract_features(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = []\n",
    "        for batch in DataLoader(CustomVectorDataset(data, torch.zeros(len(data))), batch_size=opt.batchSize, shuffle=False):\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs\n",
    "            outputs = model(inputs)\n",
    "            features.append(outputs.cpu())\n",
    "    return torch.cat(features)\n",
    "\n",
    "def cvt2TensorLabels(labels):\n",
    "    tensor_labels = torch.zeros(sum(len(l) for l in labels), dtype=torch.long)\n",
    "    for cluster_id, cluster in enumerate(labels):\n",
    "        tensor_labels[cluster] = cluster_id + 1\n",
    "    return tensor_labels.unsqueeze(1)\n",
    "\n",
    "def merge_labels(network_table, epoch_reset_labels, train_data):\n",
    "    for i, model in enumerate(network_table):\n",
    "        if epoch_reset_labels[i] == 0 or opt.updateCNN == 0:\n",
    "            features = train_data\n",
    "        else:\n",
    "            features = extract_features(model, train_data)\n",
    "        \n",
    "        if opt.centralize_feature == 1:\n",
    "            features -= features.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if opt.normalize == 1:\n",
    "            features = nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        print(\"feature dims:\", features.size())\n",
    "        label_pre_table_table[i] = update_labels(features, label_pre_table_table[i], target_nclusters_table[i], epoch_reset_labels[i])\n",
    "        epoch_reset_labels[i] += 1\n",
    "        nclusters = len(label_pre_table_table[i])\n",
    "        print(\"nclusters:\", nclusters)\n",
    "        label_pre_tensor_table[i] = cvt2TensorLabels(label_pre_table_table[i])\n",
    "\n",
    "def merge_labels_final():\n",
    "    for i, model in enumerate(network_table):\n",
    "        features = extract_features(model, train_data)\n",
    "        if opt.centralize_feature == 1:\n",
    "            features -= features.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if opt.normalize == 1:\n",
    "            features = nn.functional.normalize(features, p=2, dim=1)\n",
    "\n",
    "        label_pre_table_table[i] = update_labels(features, label_pre_table_table[i], target_nclusters_table[i], epoch_reset_labels[i])\n",
    "        epoch_reset_labels[i] += 1\n",
    "        nclusters = len(label_pre_table_table[i])\n",
    "        print(\"nclusters:\", nclusters)\n",
    "        label_pre_tensor_table[i] = cvt2TensorLabels(label_pre_table_table[i])\n",
    "\n",
    "def organize_samples(X, y):\n",
    "    num_s = X.size(0)\n",
    "    y_table = cvt2TableLabels(y)\n",
    "    nclusters = len(y_table)\n",
    "    if nclusters == 1:\n",
    "        return None, None\n",
    "    num_neg_sampling = min(opt.num_nsampling, nclusters - 1)\n",
    "    num_triplet = sum(len(cluster) * (len(cluster) - 1) * num_neg_sampling // 2 for cluster in y_table if len(cluster) > 1)\n",
    "    if num_triplet == 0:\n",
    "        return None, None\n",
    "\n",
    "    A = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    B = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    C = torch.zeros(num_triplet, X.size(1), device=X.device)\n",
    "    A_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    B_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    C_ind = torch.zeros(num_triplet, dtype=torch.long)\n",
    "    id_triplet = 0\n",
    "\n",
    "    for i, cluster in enumerate(y_table):\n",
    "        if len(cluster) > 1:\n",
    "            for m in range(len(cluster)):\n",
    "                for n in range(m + 1, len(cluster)):\n",
    "                    is_chosen = torch.zeros(num_s, dtype=torch.bool)\n",
    "                    chosen_count = 0\n",
    "                    while chosen_count < num_neg_sampling:\n",
    "                        id_s = random.randint(0, num_s - 1)\n",
    "                        if not is_chosen[id_s] and y[id_s] != y[cluster[m]]:\n",
    "                            A_ind[id_triplet] = cluster[m]\n",
    "                            B_ind[id_triplet] = cluster[n]\n",
    "                            C_ind[id_triplet] = id_s\n",
    "                            is_chosen[id_s] = True\n",
    "                            chosen_count += 1\n",
    "                            id_triplet += 1\n",
    "\n",
    "    A.copy_(X[A_ind])\n",
    "    B.copy_(X[B_ind])\n",
    "    C.copy_(X[C_ind])\n",
    "    return [A, B, C], [A_ind, B_ind, C_ind]\n",
    "\n",
    "def cvt2df_do(df_do, df_dtriplets, triplets_ind):\n",
    "    df_do.index_add_(0, triplets_ind[0], df_dtriplets[0])\n",
    "    df_do.index_add_(0, triplets_ind[1], df_dtriplets[1])\n",
    "    df_do.index_add_(0, triplets_ind[2], df_dtriplets[2])\n",
    "    return df_do\n",
    "\n",
    "# def update_CNN():\n",
    "#     for model, optimizer in zip(network_table, optimizer_table):\n",
    "#         model.train()\n",
    "#     epoch = 1\n",
    "#     print(f'==> online epoch # {epoch} [batchSize = {opt.batchSize}] [learningRate = {opt.learningRate}]')\n",
    "#     indices = torch.randperm(len(train_data)).split(opt.batchSize)\n",
    "\n",
    "#     for t, v in enumerate(indices, 1):\n",
    "#         iter = epoch * len(indices) + t - 1\n",
    "#         learning_rate = opt.learningRate * (1 + opt.gamma_lr * iter) ** -opt.power_lr\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = learning_rate\n",
    "        \n",
    "#         inputs = train_data[v]\n",
    "#         targets = label_pre_tensor_table[0][v].squeeze()  # Change this if you have multiple networks\n",
    "        \n",
    "#         for model, optimizer in zip(network_table, optimizer_table):\n",
    "#             def closure():\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(inputs)\n",
    "#                 triplets, triplets_ind = organize_samples(outputs, targets)\n",
    "#                 loss = torch.tensor(0)\n",
    "#                 if triplets:\n",
    "#                     anchor, positive, negative = triplets\n",
    "#                     loss = criterion_triplet(anchor, positive, negative)\n",
    "#                     loss.backward()\n",
    "#                 if t % 10 == 0:\n",
    "#                     print(\"loss:\", loss.item())\n",
    "#                 return loss\n",
    "            \n",
    "#             optimizer.step(closure)\n",
    "#     epoch += 1\n",
    "\n",
    "def update_CNN():\n",
    "    for model in network_table:\n",
    "        model.train()\n",
    "    \n",
    "    global epoch\n",
    "    epoch = epoch if 'epoch' in globals() else 1\n",
    "    print(f'==> online epoch # {epoch} [batchSize = {opt.batchSize}] [learningRate = {opt.learningRate}]')\n",
    "    \n",
    "    indices = torch.randperm(len(train_data)).split(opt.batchSize)\n",
    "    \n",
    "    for t, v in enumerate(indices):\n",
    "        iter = epoch * len(indices) + t\n",
    "        learning_rate = opt.learningRate * (1 + opt.gamma_lr * iter) ** (-opt.power_lr)\n",
    "        \n",
    "        inputs = train_data[v]\n",
    "        \n",
    "        for i, (model, optimizer) in enumerate(zip(network_table, optimizer_table)):\n",
    "            targets = label_pre_tensor_table[i][v]\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                triplets, triplets_ind = organize_samples(outputs, targets.float())\n",
    "                #loss = torch.tensor(0.0)\n",
    "                if triplets is not None:\n",
    "                    anchor, positive, negative = triplets\n",
    "                    loss = criterion_triplet(anchor, positive, negative)\n",
    "                    loss.backward()\n",
    "                \n",
    "                    if t % 10 == 0:\n",
    "                        print(\"loss:\", loss.item())\n",
    "                    return loss\n",
    "                else:\n",
    "                    return torch.tensor(0.0)\n",
    "            \n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "            \n",
    "            optimizer.step(closure)\n",
    "    \n",
    "    epoch += 1\n",
    "\n",
    "def eval_perf():\n",
    "    for model in network_table:\n",
    "        model.eval()\n",
    "    print('==> testing')\n",
    "    for i, model in enumerate(network_table):\n",
    "        nmi = Evaluate().NMI(label_gt_table_table[i], label_pre_table_table[i])\n",
    "        print('NMI:', nmi)\n",
    "        print(\" \")\n",
    "\n",
    "def is_allfinished():\n",
    "    for label_pre, target_nclusters in zip(label_pre_table_table, target_nclusters_table):\n",
    "        if len(label_pre) > target_nclusters:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_reset_labels = [0] * num_networks\n",
    "optimState = {'learningRate': opt.learningRate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dims: torch.Size([1000, 320])\n",
      "sigma: tensor(0.6083, dtype=torch.float64)\n",
      "initialize clusters...\n",
      "nclusters: 220\n",
      "==> testing\n",
      "NMI: 0.544594407081604\n",
      " \n",
      "==> online epoch # 321 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998672604560852\n",
      "loss: 0.1999921053647995\n",
      "loss: 0.19999709725379944\n",
      "==> online epoch # 322 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19997325539588928\n",
      "==> online epoch # 323 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999583065509796\n",
      "loss: 0.1999949961900711\n",
      "loss: 0.19999182224273682\n",
      "loss: 0.1999925822019577\n",
      "==> online epoch # 324 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999924302101135\n",
      "loss: 0.19998779892921448\n",
      "loss: 0.19999729096889496\n",
      "==> online epoch # 325 [batchSize = 10] [learningRate = 0.01]\n",
      "==> online epoch # 326 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999435544013977\n",
      "loss: 0.199991375207901\n",
      "loss: 0.1999967098236084\n",
      "loss: 0.19999602437019348\n",
      "==> online epoch # 327 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999562203884125\n",
      "loss: 0.19999562203884125\n",
      "loss: 0.19999216496944427\n",
      "loss: 0.19999153912067413\n",
      "loss: 0.1999904364347458\n",
      "==> online epoch # 328 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999849796295166\n",
      "loss: 0.1999901533126831\n",
      "loss: 0.19999463856220245\n",
      "loss: 0.19999809563159943\n",
      "==> online epoch # 329 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999973475933075\n",
      "loss: 0.19999447464942932\n",
      "loss: 0.1999901384115219\n",
      "==> online epoch # 330 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999886929988861\n",
      "loss: 0.1999904215335846\n",
      "==> online epoch # 331 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999797523021698\n",
      "==> online epoch # 332 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998376071453094\n",
      "loss: 0.19999772310256958\n",
      "loss: 0.19999650120735168\n",
      "loss: 0.19999153912067413\n",
      "==> online epoch # 333 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999954730272293\n",
      "loss: 0.19999924302101135\n",
      "loss: 0.19994893670082092\n",
      "==> online epoch # 334 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999489188194275\n",
      "loss: 0.19999749958515167\n",
      "loss: 0.19999626278877258\n",
      "==> online epoch # 335 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999828964471817\n",
      "loss: 0.19999665021896362\n",
      "loss: 0.19998688995838165\n",
      "loss: 0.19998252391815186\n",
      "loss: 0.19999383389949799\n",
      "==> online epoch # 336 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998446106910706\n",
      "loss: 0.19996798038482666\n",
      "==> online epoch # 337 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999298453330994\n",
      "loss: 0.19998221099376678\n",
      "loss: 0.19999516010284424\n",
      "==> online epoch # 338 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998696446418762\n",
      "loss: 0.1999669373035431\n",
      "loss: 0.19999459385871887\n",
      "loss: 0.19999630749225616\n",
      "loss: 0.1999925971031189\n",
      "==> online epoch # 339 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999635219573975\n",
      "loss: 0.19998686015605927\n",
      "loss: 0.19997462630271912\n",
      "loss: 0.1999971717596054\n",
      "==> online epoch # 340 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999198615550995\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.4973, dtype=torch.float64)\n",
      "nclusters:  220\n",
      "merge tensor(31) 105\n",
      "Time elapsed for computing cluster affinity: 16.900758028030396 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 175\n",
      "==> testing\n",
      "NMI: 0.544308066368103\n",
      " \n",
      "==> online epoch # 341 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999215006828308\n",
      "loss: 0.1999930441379547\n",
      "loss: 0.19997861981391907\n",
      "loss: 0.19996769726276398\n",
      "==> online epoch # 342 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999387860298157\n",
      "loss: 0.19999398291110992\n",
      "loss: 0.19997286796569824\n",
      "loss: 0.19997760653495789\n",
      "==> online epoch # 343 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999960064888\n",
      "loss: 0.19998423755168915\n",
      "loss: 0.1999901384115219\n",
      "loss: 0.19999325275421143\n",
      "==> online epoch # 344 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998887181282043\n",
      "==> online epoch # 345 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999886929988861\n",
      "loss: 0.19998875260353088\n",
      "loss: 0.1999949812889099\n",
      "==> online epoch # 346 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999906420707703\n",
      "loss: 0.1999886929988861\n",
      "==> online epoch # 347 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.199990913271904\n",
      "loss: 0.19997835159301758\n",
      "loss: 0.19998642802238464\n",
      "==> online epoch # 348 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999938040971756\n",
      "==> online epoch # 349 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999706745147705\n",
      "loss: 0.19998884201049805\n",
      "loss: 0.1999388039112091\n",
      "loss: 0.19998621940612793\n",
      "==> online epoch # 350 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998681545257568\n",
      "==> online epoch # 351 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998161494731903\n",
      "loss: 0.19998398423194885\n",
      "==> online epoch # 352 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999803602695465\n",
      "loss: 0.19998054206371307\n",
      "==> online epoch # 353 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999250769615173\n",
      "loss: 0.19997760653495789\n",
      "loss: 0.19996413588523865\n",
      "loss: 0.19997574388980865\n",
      "==> online epoch # 354 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998089969158173\n",
      "loss: 0.19998173415660858\n",
      "==> online epoch # 355 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19996967911720276\n",
      "loss: 0.1999654769897461\n",
      "loss: 0.19997482001781464\n",
      "loss: 0.1999368816614151\n",
      "loss: 0.1999937742948532\n",
      "loss: 0.1999833881855011\n",
      "loss: 0.19998745620250702\n",
      "==> online epoch # 356 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998224079608917\n",
      "loss: 0.19997331500053406\n",
      "loss: 0.1999804526567459\n",
      "loss: 0.19998079538345337\n",
      "loss: 0.19999468326568604\n",
      "==> online epoch # 357 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19988951086997986\n",
      "loss: 0.19995540380477905\n",
      "loss: 0.19999511539936066\n",
      "==> online epoch # 358 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999765932559967\n",
      "loss: 0.1999955177307129\n",
      "loss: 0.19999273121356964\n",
      "loss: 0.19998595118522644\n",
      "==> online epoch # 359 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19996711611747742\n",
      "loss: 0.1999897062778473\n",
      "loss: 0.199992373585701\n",
      "loss: 0.19999101758003235\n",
      "loss: 0.19998407363891602\n",
      "==> online epoch # 360 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19997678697109222\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.4394, dtype=torch.float64)\n",
      "nclusters:  175\n",
      "merge 70 tensor(139)\n",
      "merge 133 tensor(172)\n",
      "merge 139 tensor(154)\n",
      "merge 146 tensor(171)\n",
      "merge tensor(114) 154\n",
      "merge tensor(145) 171\n",
      "merge tensor(133) 172\n",
      "Time elapsed for computing cluster affinity: 16.318477153778076 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 138\n",
      "==> testing\n",
      "NMI: 0.5420671105384827\n",
      " \n",
      "==> online epoch # 361 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19994832575321198\n",
      "loss: 0.19996356964111328\n",
      "==> online epoch # 362 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19994375109672546\n",
      "loss: 0.19998033344745636\n",
      "loss: 0.19997051358222961\n",
      "loss: 0.1999911218881607\n",
      "loss: 0.1999884992837906\n",
      "==> online epoch # 363 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999833732843399\n",
      "loss: 0.19997641444206238\n",
      "loss: 0.19998295605182648\n",
      "==> online epoch # 364 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19996866583824158\n",
      "loss: 0.19990260899066925\n",
      "==> online epoch # 365 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19999906420707703\n",
      "loss: 0.19995790719985962\n",
      "loss: 0.19997240602970123\n",
      "loss: 0.1999041736125946\n",
      "loss: 0.19986645877361298\n",
      "loss: 0.19993537664413452\n",
      "loss: 0.19992373883724213\n",
      "==> online epoch # 366 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19998440146446228\n",
      "loss: 0.19998042285442352\n",
      "loss: 0.1999712884426117\n",
      "loss: 0.19997984170913696\n",
      "loss: 0.19997873902320862\n",
      "==> online epoch # 367 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999838650226593\n",
      "loss: 0.1999252885580063\n",
      "loss: 0.19989421963691711\n",
      "loss: 0.1999763697385788\n",
      "==> online epoch # 368 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19991426169872284\n",
      "loss: 0.19992855191230774\n",
      "loss: 0.1999528557062149\n",
      "loss: 0.19997797906398773\n",
      "==> online epoch # 369 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19986844062805176\n",
      "loss: 0.19996817409992218\n",
      "loss: 0.1999785453081131\n",
      "loss: 0.19989007711410522\n",
      "loss: 0.19994980096817017\n",
      "==> online epoch # 370 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19994640350341797\n",
      "loss: 0.19997474551200867\n",
      "loss: 0.19989731907844543\n",
      "loss: 0.1999044120311737\n",
      "loss: 0.19996163249015808\n",
      "loss: 0.19994519650936127\n",
      "loss: 0.19990597665309906\n",
      "==> online epoch # 371 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19986189901828766\n",
      "loss: 0.1995873749256134\n",
      "loss: 0.19996997714042664\n",
      "loss: 0.19991210103034973\n",
      "==> online epoch # 372 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1999833881855011\n",
      "loss: 0.19991719722747803\n",
      "loss: 0.19996008276939392\n",
      "loss: 0.1999698430299759\n",
      "loss: 0.1999398171901703\n",
      "loss: 0.19996216893196106\n",
      "loss: 0.19998523592948914\n",
      "==> online epoch # 373 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19973258674144745\n",
      "loss: 0.1999647468328476\n",
      "loss: 0.1999792456626892\n",
      "loss: 0.19990547001361847\n",
      "loss: 0.19992393255233765\n",
      "loss: 0.19997231662273407\n",
      "==> online epoch # 374 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19995364546775818\n",
      "loss: 0.19998332858085632\n",
      "loss: 0.199905663728714\n",
      "loss: 0.19979766011238098\n",
      "==> online epoch # 375 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19987118244171143\n",
      "==> online epoch # 376 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1995536983013153\n",
      "loss: 0.19992700219154358\n",
      "loss: 0.1999312937259674\n",
      "loss: 0.1995614916086197\n",
      "==> online epoch # 377 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19929571449756622\n",
      "loss: 0.19946928322315216\n",
      "loss: 0.19952188432216644\n",
      "loss: 0.19956350326538086\n",
      "==> online epoch # 378 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19991286098957062\n",
      "loss: 0.19968315958976746\n",
      "loss: 0.19996851682662964\n",
      "loss: 0.19997663795948029\n",
      "loss: 0.19980685412883759\n",
      "==> online epoch # 379 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1989799588918686\n",
      "loss: 0.19963981211185455\n",
      "==> online epoch # 380 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19977527856826782\n",
      "loss: 0.19973483681678772\n",
      "loss: 0.1986621916294098\n",
      "loss: 0.19979976117610931\n",
      "loss: 0.19967897236347198\n",
      "loss: 0.19995206594467163\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.3032, dtype=torch.float64)\n",
      "nclusters:  138\n",
      "Time elapsed for computing cluster affinity: 15.766297101974487 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 110\n",
      "==> testing\n",
      "NMI: 0.550606369972229\n",
      " \n",
      "==> online epoch # 381 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19944651424884796\n",
      "loss: 0.19967928528785706\n",
      "loss: 0.1996164321899414\n",
      "==> online epoch # 382 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1998964101076126\n",
      "loss: 0.1996891349554062\n",
      "loss: 0.19968269765377045\n",
      "loss: 0.19963863492012024\n",
      "loss: 0.19968649744987488\n",
      "loss: 0.1993439495563507\n",
      "loss: 0.19870826601982117\n",
      "==> online epoch # 383 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19982019066810608\n",
      "loss: 0.1997172087430954\n",
      "loss: 0.19821889698505402\n",
      "loss: 0.19974343478679657\n",
      "loss: 0.1986214965581894\n",
      "==> online epoch # 384 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19939008355140686\n",
      "loss: 0.19957487285137177\n",
      "loss: 0.19849911332130432\n",
      "loss: 0.19774939119815826\n",
      "==> online epoch # 385 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19892819225788116\n",
      "loss: 0.19563330709934235\n",
      "==> online epoch # 386 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1989298164844513\n",
      "loss: 0.1997225284576416\n",
      "loss: 0.19689898192882538\n",
      "loss: 0.1978815495967865\n",
      "loss: 0.1982601135969162\n",
      "loss: 0.1968364119529724\n",
      "loss: 0.1998831033706665\n",
      "==> online epoch # 387 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19302983582019806\n",
      "loss: 0.19287094473838806\n",
      "loss: 0.19658485054969788\n",
      "loss: 0.196611225605011\n",
      "loss: 0.1985221803188324\n",
      "loss: 0.19589102268218994\n",
      "==> online epoch # 388 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.19980360567569733\n",
      "loss: 0.19045543670654297\n",
      "loss: 0.18407614529132843\n",
      "loss: 0.1844043731689453\n",
      "loss: 0.18124160170555115\n",
      "==> online epoch # 389 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.181636244058609\n",
      "loss: 0.07972771674394608\n",
      "loss: 0.038899365812540054\n",
      "loss: 0.03035339154303074\n",
      "loss: 0.15175127983093262\n",
      "==> online epoch # 390 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0686742439866066\n",
      "loss: 0.08198919147253036\n",
      "loss: 0.10696544498205185\n",
      "==> online epoch # 391 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06987682729959488\n",
      "loss: 0.11884278059005737\n",
      "loss: 0.11771319061517715\n",
      "loss: 0.119343101978302\n",
      "loss: 0.073267862200737\n",
      "==> online epoch # 392 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.03227765113115311\n",
      "loss: 0.08834435045719147\n",
      "loss: 0.07767995446920395\n",
      "==> online epoch # 393 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.009159376844763756\n",
      "loss: 0.04426662623882294\n",
      "loss: 0.04267782345414162\n",
      "loss: 0.0\n",
      "loss: 0.04878934472799301\n",
      "loss: 0.030121713876724243\n",
      "==> online epoch # 394 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.078711599111557\n",
      "loss: 0.008297714404761791\n",
      "loss: 0.079941526055336\n",
      "loss: 0.14691577851772308\n",
      "loss: 0.0\n",
      "loss: 0.0013649642933160067\n",
      "loss: 0.0799732580780983\n",
      "==> online epoch # 395 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07145798951387405\n",
      "loss: 0.03940706327557564\n",
      "loss: 0.02893003262579441\n",
      "loss: 0.058558739721775055\n",
      "loss: 0.0402679406106472\n",
      "loss: 0.04777070879936218\n",
      "==> online epoch # 396 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06033017486333847\n",
      "loss: 0.08247975260019302\n",
      "loss: 0.0\n",
      "loss: 0.12421679496765137\n",
      "loss: 0.06590145081281662\n",
      "loss: 0.04187756031751633\n",
      "loss: 0.045009318739175797\n",
      "==> online epoch # 397 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0327085480093956\n",
      "loss: 0.07175884395837784\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.1595916897058487\n",
      "loss: 0.04085954278707504\n",
      "loss: 0.10426578670740128\n",
      "==> online epoch # 398 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.11500869691371918\n",
      "loss: 0.025034183636307716\n",
      "loss: 0.0\n",
      "loss: 0.03261379152536392\n",
      "loss: 0.12100306898355484\n",
      "loss: 0.038787178695201874\n",
      "==> online epoch # 399 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07924534380435944\n",
      "loss: 0.07793277502059937\n",
      "loss: 0.03014407493174076\n",
      "loss: 0.21586517989635468\n",
      "==> online epoch # 400 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.15814802050590515\n",
      "loss: 0.14074937999248505\n",
      "loss: 0.09541194885969162\n",
      "loss: 0.04663046821951866\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0949, dtype=torch.float64)\n",
      "nclusters:  110\n",
      "Time elapsed for computing cluster affinity: 15.305031776428223 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 88\n",
      "==> testing\n",
      "NMI: 0.5588505268096924\n",
      " \n",
      "==> online epoch # 401 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.11063645780086517\n",
      "loss: 0.039272356778383255\n",
      "loss: 0.01726788468658924\n",
      "loss: 0.07967205345630646\n",
      "loss: 0.012403585948050022\n",
      "==> online epoch # 402 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03690405562520027\n",
      "loss: 0.07795009762048721\n",
      "loss: 0.1666014939546585\n",
      "loss: 0.13827477395534515\n",
      "loss: 0.11293091624975204\n",
      "==> online epoch # 403 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.058713823556900024\n",
      "loss: 0.0\n",
      "loss: 0.028950970619916916\n",
      "loss: 0.002158406423404813\n",
      "loss: 0.08048995584249496\n",
      "loss: 0.11747926473617554\n",
      "loss: 0.039984799921512604\n",
      "==> online epoch # 404 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03583461791276932\n",
      "loss: 0.04418842867016792\n",
      "loss: 0.018440714105963707\n",
      "loss: 0.060475219041109085\n",
      "==> online epoch # 405 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02877771481871605\n",
      "loss: 0.15479955077171326\n",
      "loss: 0.024734675884246826\n",
      "loss: 0.10574772208929062\n",
      "loss: 0.07247922569513321\n",
      "==> online epoch # 406 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.20308654010295868\n",
      "loss: 0.0\n",
      "loss: 0.053453899919986725\n",
      "loss: 0.0\n",
      "loss: 0.16321542859077454\n",
      "loss: 0.0\n",
      "==> online epoch # 407 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03964855521917343\n",
      "loss: 0.09441141784191132\n",
      "loss: 0.15302997827529907\n",
      "loss: 0.09732504189014435\n",
      "loss: 0.02832832932472229\n",
      "loss: 0.026303116232156754\n",
      "==> online epoch # 408 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.020762773230671883\n",
      "loss: 0.021377237513661385\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.014865907840430737\n",
      "loss: 0.02752773091197014\n",
      "loss: 0.04029374569654465\n",
      "==> online epoch # 409 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.014779512770473957\n",
      "loss: 0.11904153972864151\n",
      "loss: 0.04094666242599487\n",
      "loss: 0.0381399430334568\n",
      "loss: 0.056705962866544724\n",
      "loss: 0.055221766233444214\n",
      "loss: 0.0048562390729784966\n",
      "==> online epoch # 410 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.045652683824300766\n",
      "loss: 0.026561807841062546\n",
      "loss: 0.1553882658481598\n",
      "loss: 0.0\n",
      "loss: 0.09319750964641571\n",
      "loss: 0.0\n",
      "loss: 0.0724683329463005\n",
      "loss: 0.018659835681319237\n",
      "loss: 0.15697051584720612\n",
      "==> online epoch # 411 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.013530537486076355\n",
      "loss: 0.1338847130537033\n",
      "loss: 0.12071050703525543\n",
      "loss: 0.0\n",
      "loss: 0.1350536048412323\n",
      "==> online epoch # 412 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.04031134769320488\n",
      "loss: 0.11790738254785538\n",
      "loss: 0.04572560638189316\n",
      "loss: 0.0034450285602360964\n",
      "==> online epoch # 413 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.20395883917808533\n",
      "loss: 0.04178464785218239\n",
      "loss: 0.04371880739927292\n",
      "loss: 0.19383257627487183\n",
      "loss: 0.057286642491817474\n",
      "loss: 0.0\n",
      "==> online epoch # 414 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.037454862147569656\n",
      "loss: 0.06719431281089783\n",
      "loss: 0.058548975735902786\n",
      "loss: 0.1030331626534462\n",
      "loss: 0.15804703533649445\n",
      "loss: 0.08020709455013275\n",
      "loss: 0.03380303829908371\n",
      "loss: 0.07743460685014725\n",
      "==> online epoch # 415 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.11801435798406601\n",
      "loss: 0.10733582824468613\n",
      "loss: 0.06625061482191086\n",
      "loss: 0.012850883416831493\n",
      "==> online epoch # 416 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04857226088643074\n",
      "loss: 0.07685825973749161\n",
      "loss: 0.07499263435602188\n",
      "loss: 0.04502665624022484\n",
      "loss: 0.05428243800997734\n",
      "==> online epoch # 417 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06880336999893188\n",
      "loss: 0.026668738573789597\n",
      "loss: 0.18280622363090515\n",
      "loss: 0.04919726774096489\n",
      "loss: 0.2940450608730316\n",
      "loss: 0.02882024273276329\n",
      "loss: 0.1054072380065918\n",
      "loss: 0.07884225994348526\n",
      "==> online epoch # 418 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0532752200961113\n",
      "loss: 0.023067014291882515\n",
      "loss: 0.048462819308042526\n",
      "loss: 0.020400794222950935\n",
      "loss: 0.043512970209121704\n",
      "loss: 0.12669113278388977\n",
      "==> online epoch # 419 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.051530320197343826\n",
      "loss: 0.0\n",
      "loss: 0.034515172243118286\n",
      "==> online epoch # 420 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1752866953611374\n",
      "loss: 0.03462807089090347\n",
      "loss: 0.12394130229949951\n",
      "loss: 0.06415943801403046\n",
      "loss: 0.03262840956449509\n",
      "loss: 0.009442539885640144\n",
      "loss: 0.016811687499284744\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0842, dtype=torch.float64)\n",
      "nclusters:  88\n",
      "Time elapsed for computing cluster affinity: 15.132970809936523 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 70\n",
      "==> testing\n",
      "NMI: 0.5564253330230713\n",
      " \n",
      "==> online epoch # 421 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.14549903571605682\n",
      "loss: 0.03979150205850601\n",
      "loss: 0.037854619324207306\n",
      "loss: 0.053978703916072845\n",
      "loss: 0.058178968727588654\n",
      "loss: 0.023353632539510727\n",
      "loss: 0.0\n",
      "loss: 0.07083811610937119\n",
      "loss: 0.0\n",
      "==> online epoch # 422 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07655073702335358\n",
      "loss: 0.10354582965373993\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.2804122269153595\n",
      "loss: 0.0\n",
      "==> online epoch # 423 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.575739324092865\n",
      "loss: 0.0\n",
      "loss: 0.05343717709183693\n",
      "loss: 0.0742683932185173\n",
      "loss: 0.042380932718515396\n",
      "loss: 0.003567412495613098\n",
      "==> online epoch # 424 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.01901748776435852\n",
      "loss: 0.039957545697689056\n",
      "loss: 0.11589667946100235\n",
      "loss: 0.0449560210108757\n",
      "loss: 0.09177536517381668\n",
      "loss: 0.03132786601781845\n",
      "loss: 0.020288091152906418\n",
      "==> online epoch # 425 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02448556013405323\n",
      "loss: 0.05907588452100754\n",
      "loss: 0.04210786148905754\n",
      "loss: 0.1270793080329895\n",
      "loss: 0.024671383202075958\n",
      "loss: 0.0\n",
      "loss: 0.009715116582810879\n",
      "loss: 0.25323906540870667\n",
      "==> online epoch # 426 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.08923988789319992\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.028123848140239716\n",
      "loss: 0.12110872566699982\n",
      "loss: 0.03283151239156723\n",
      "==> online epoch # 427 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0296640582382679\n",
      "loss: 0.009605254977941513\n",
      "loss: 0.0\n",
      "loss: 0.009554114192724228\n",
      "loss: 0.02985401451587677\n",
      "loss: 0.03449773043394089\n",
      "loss: 0.08437260240316391\n",
      "==> online epoch # 428 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.05207597091794014\n",
      "loss: 0.10819945484399796\n",
      "loss: 0.0\n",
      "loss: 0.1044740080833435\n",
      "loss: 0.08450660854578018\n",
      "loss: 0.04358598217368126\n",
      "loss: 0.04234883934259415\n",
      "loss: 0.03041677549481392\n",
      "==> online epoch # 429 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08292107284069061\n",
      "loss: 0.030617928132414818\n",
      "loss: 0.03867805376648903\n",
      "loss: 0.15315274894237518\n",
      "loss: 0.10490106046199799\n",
      "loss: 0.07912689447402954\n",
      "loss: 0.03432057797908783\n",
      "loss: 0.0164182186126709\n",
      "==> online epoch # 430 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02103498950600624\n",
      "loss: 0.08667968213558197\n",
      "loss: 0.039878878742456436\n",
      "loss: 0.0\n",
      "loss: 0.012096406891942024\n",
      "loss: 0.03167560696601868\n",
      "loss: 0.11647150665521622\n",
      "loss: 0.028222760185599327\n",
      "loss: 0.0028148810379207134\n",
      "loss: 0.07690034061670303\n",
      "==> online epoch # 431 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.059802714735269547\n",
      "loss: 0.007153117563575506\n",
      "loss: 0.11021920293569565\n",
      "loss: 0.012992626056075096\n",
      "loss: 0.12691035866737366\n",
      "loss: 0.0\n",
      "loss: 0.030832942575216293\n",
      "loss: 0.0395045280456543\n",
      "==> online epoch # 432 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.053339723497629166\n",
      "loss: 0.07126422971487045\n",
      "loss: 0.03200209140777588\n",
      "loss: 0.1616811454296112\n",
      "loss: 0.0726616382598877\n",
      "loss: 0.026314888149499893\n",
      "loss: 0.07080234587192535\n",
      "loss: 0.11622712761163712\n",
      "loss: 0.10405884683132172\n",
      "==> online epoch # 433 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08102035522460938\n",
      "loss: 0.03852757811546326\n",
      "loss: 0.0808585062623024\n",
      "loss: 0.0\n",
      "loss: 0.0153347821906209\n",
      "loss: 0.09680536389350891\n",
      "loss: 0.022300731390714645\n",
      "==> online epoch # 434 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07149045169353485\n",
      "loss: 0.10000886023044586\n",
      "loss: 0.03333069756627083\n",
      "loss: 0.10616304725408554\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.012750500813126564\n",
      "loss: 0.01530755590647459\n",
      "loss: 0.014926747418940067\n",
      "==> online epoch # 435 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.18014828860759735\n",
      "loss: 0.0\n",
      "loss: 0.0023024468682706356\n",
      "loss: 0.0\n",
      "loss: 0.09614327549934387\n",
      "loss: 0.035533927381038666\n",
      "==> online epoch # 436 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.019923120737075806\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.039961159229278564\n",
      "loss: 0.0417267382144928\n",
      "loss: 0.03956839069724083\n",
      "loss: 0.0\n",
      "loss: 0.025164738297462463\n",
      "loss: 0.03446401283144951\n",
      "==> online epoch # 437 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03758396580815315\n",
      "loss: 0.059733856469392776\n",
      "loss: 0.008818532340228558\n",
      "loss: 0.042783066630363464\n",
      "loss: 0.009177230298519135\n",
      "loss: 0.04668574780225754\n",
      "loss: 0.033625178039073944\n",
      "loss: 0.1140633076429367\n",
      "loss: 0.03577134758234024\n",
      "loss: 0.0641479566693306\n",
      "==> online epoch # 438 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1805298775434494\n",
      "loss: 0.0288019347935915\n",
      "loss: 0.009367541410028934\n",
      "loss: 0.04317714273929596\n",
      "loss: 0.1221449226140976\n",
      "loss: 0.0\n",
      "loss: 0.10355304181575775\n",
      "==> online epoch # 439 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.012155568227171898\n",
      "loss: 0.0002913743373937905\n",
      "loss: 0.05483003705739975\n",
      "loss: 0.21246501803398132\n",
      "loss: 0.06321422755718231\n",
      "loss: 0.1894097626209259\n",
      "loss: 0.05255601927638054\n",
      "==> online epoch # 440 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.03173218294978142\n",
      "loss: 0.07197277247905731\n",
      "loss: 0.09334143996238708\n",
      "loss: 0.02026265487074852\n",
      "loss: 0.048650454729795456\n",
      "loss: 0.0\n",
      "loss: 0.006185316946357489\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0778, dtype=torch.float64)\n",
      "nclusters:  70\n",
      "Time elapsed for computing cluster affinity: 14.632567167282104 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 56\n",
      "==> testing\n",
      "NMI: 0.5563482046127319\n",
      " \n",
      "==> online epoch # 441 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0428355373442173\n",
      "loss: 0.028242170810699463\n",
      "loss: 0.029980182647705078\n",
      "loss: 0.07717530429363251\n",
      "loss: 0.0\n",
      "loss: 0.001356719876639545\n",
      "loss: 0.07557982206344604\n",
      "loss: 0.10851983726024628\n",
      "==> online epoch # 442 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03550194948911667\n",
      "loss: 0.10096289217472076\n",
      "loss: 0.11249241977930069\n",
      "loss: 0.10655774176120758\n",
      "loss: 0.0862320065498352\n",
      "loss: 0.09693291038274765\n",
      "loss: 0.03600968420505524\n",
      "loss: 0.03935573250055313\n",
      "loss: 0.09600753337144852\n",
      "loss: 0.005253559444099665\n",
      "==> online epoch # 443 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03593224659562111\n",
      "loss: 0.10862217098474503\n",
      "loss: 0.0201070848852396\n",
      "loss: 0.0\n",
      "loss: 0.01724289357662201\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.026857083663344383\n",
      "loss: 0.1311618983745575\n",
      "loss: 0.1251337230205536\n",
      "==> online epoch # 444 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.11653389036655426\n",
      "loss: 0.03577372059226036\n",
      "loss: 0.03195304796099663\n",
      "loss: 0.010742115788161755\n",
      "loss: 0.10718266665935516\n",
      "loss: 0.09200458973646164\n",
      "loss: 0.0969650149345398\n",
      "loss: 0.02048208937048912\n",
      "==> online epoch # 445 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.041713617742061615\n",
      "loss: 0.03017285093665123\n",
      "loss: 0.0\n",
      "loss: 0.03884761407971382\n",
      "loss: 0.01096432190388441\n",
      "loss: 0.03630247712135315\n",
      "loss: 0.03668753057718277\n",
      "loss: 0.0\n",
      "loss: 0.08420741558074951\n",
      "loss: 0.053244154900312424\n",
      "==> online epoch # 446 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03747141361236572\n",
      "loss: 0.011010969057679176\n",
      "loss: 0.12281296402215958\n",
      "loss: 0.05596817284822464\n",
      "loss: 0.09872527420520782\n",
      "loss: 0.03790166601538658\n",
      "loss: 0.09687316417694092\n",
      "loss: 0.12458102405071259\n",
      "loss: 0.010881222784519196\n",
      "==> online epoch # 447 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.050991471856832504\n",
      "loss: 0.010838769376277924\n",
      "loss: 0.019460298120975494\n",
      "loss: 0.04056191071867943\n",
      "loss: 0.048253707587718964\n",
      "loss: 0.17526310682296753\n",
      "loss: 0.0\n",
      "loss: 0.05173773691058159\n",
      "loss: 0.0\n",
      "==> online epoch # 448 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.038631465286016464\n",
      "loss: 0.045162927359342575\n",
      "loss: 0.13792350888252258\n",
      "loss: 0.023145625367760658\n",
      "loss: 0.0\n",
      "loss: 0.03841853886842728\n",
      "loss: 0.05756673961877823\n",
      "loss: 0.04030053690075874\n",
      "loss: 0.0\n",
      "==> online epoch # 449 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.002520102309063077\n",
      "loss: 0.0593177005648613\n",
      "loss: 0.0005585700273513794\n",
      "loss: 0.010187000036239624\n",
      "loss: 0.05270805209875107\n",
      "loss: 0.13018161058425903\n",
      "loss: 0.08920728415250778\n",
      "loss: 0.14043688774108887\n",
      "loss: 0.06761288642883301\n",
      "==> online epoch # 450 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0004409760294947773\n",
      "loss: 0.022420791909098625\n",
      "loss: 0.08430945128202438\n",
      "loss: 0.09446312487125397\n",
      "loss: 0.10302171856164932\n",
      "loss: 0.01475907675921917\n",
      "loss: 0.09659884870052338\n",
      "loss: 0.11816633492708206\n",
      "loss: 0.06085749715566635\n",
      "==> online epoch # 451 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06702705472707748\n",
      "loss: 0.026548225432634354\n",
      "loss: 0.04380530118942261\n",
      "loss: 0.045597560703754425\n",
      "loss: 0.00874013639986515\n",
      "loss: 0.04197847843170166\n",
      "loss: 0.0\n",
      "loss: 0.025728559121489525\n",
      "loss: 0.061696045100688934\n",
      "loss: 0.040122516453266144\n",
      "==> online epoch # 452 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06221942976117134\n",
      "loss: 0.037703048437833786\n",
      "loss: 0.09781945496797562\n",
      "loss: 0.0\n",
      "loss: 0.02110656537115574\n",
      "loss: 0.010185941122472286\n",
      "loss: 0.051288776099681854\n",
      "loss: 0.10143160074949265\n",
      "loss: 0.08449739962816238\n",
      "==> online epoch # 453 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.015958217903971672\n",
      "loss: 0.11418656259775162\n",
      "loss: 0.021114185452461243\n",
      "loss: 0.03705320507287979\n",
      "loss: 0.0\n",
      "loss: 0.02773371711373329\n",
      "loss: 0.11028897762298584\n",
      "loss: 0.036426302045583725\n",
      "loss: 0.0\n",
      "loss: 0.017169227823615074\n",
      "==> online epoch # 454 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08962807059288025\n",
      "loss: 0.09822597354650497\n",
      "loss: 0.02489592134952545\n",
      "loss: 0.0\n",
      "loss: 0.028702225536108017\n",
      "loss: 0.04103124141693115\n",
      "loss: 0.02777079865336418\n",
      "loss: 0.0\n",
      "loss: 0.07089217752218246\n",
      "==> online epoch # 455 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06497605890035629\n",
      "loss: 0.04488443583250046\n",
      "loss: 0.14386698603630066\n",
      "loss: 0.04034996032714844\n",
      "loss: 0.012556495144963264\n",
      "loss: 0.026353955268859863\n",
      "loss: 0.06013767048716545\n",
      "loss: 0.04717778041958809\n",
      "loss: 0.035457395017147064\n",
      "loss: 0.011797058396041393\n",
      "==> online epoch # 456 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07435965538024902\n",
      "loss: 0.03870543837547302\n",
      "loss: 0.29392939805984497\n",
      "loss: 0.05548977851867676\n",
      "loss: 0.03888378292322159\n",
      "loss: 0.1442052125930786\n",
      "loss: 0.026813501492142677\n",
      "loss: 0.07086929678916931\n",
      "loss: 0.09444133937358856\n",
      "loss: 0.1523921638727188\n",
      "==> online epoch # 457 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.05855204537510872\n",
      "loss: 0.04133286699652672\n",
      "loss: 0.023320475593209267\n",
      "loss: 0.0\n",
      "loss: 0.011394248344004154\n",
      "loss: 0.07470062375068665\n",
      "loss: 0.0\n",
      "loss: 0.11130630970001221\n",
      "loss: 0.03843823820352554\n",
      "loss: 0.025184601545333862\n",
      "==> online epoch # 458 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.022165345028042793\n",
      "loss: 0.022828802466392517\n",
      "loss: 0.019739685580134392\n",
      "loss: 0.0\n",
      "loss: 0.0865619108080864\n",
      "loss: 0.03848002851009369\n",
      "loss: 0.0\n",
      "loss: 0.18732860684394836\n",
      "==> online epoch # 459 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.005983797833323479\n",
      "loss: 0.06992965936660767\n",
      "loss: 0.02884772978723049\n",
      "loss: 0.12510521709918976\n",
      "loss: 0.034072715789079666\n",
      "loss: 0.013126930221915245\n",
      "loss: 0.03820347413420677\n",
      "loss: 0.019195158034563065\n",
      "loss: 0.09934516996145248\n",
      "==> online epoch # 460 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.15034952759742737\n",
      "loss: 0.0074308873154222965\n",
      "loss: 0.0\n",
      "loss: 0.04628242924809456\n",
      "loss: 0.08851899951696396\n",
      "loss: 0.032655976712703705\n",
      "loss: 0.05122556909918785\n",
      "loss: 0.049191735684871674\n",
      "loss: 0.03600432351231575\n",
      "loss: 0.04443812370300293\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0709, dtype=torch.float64)\n",
      "nclusters:  56\n",
      "Time elapsed for computing cluster affinity: 14.63437294960022 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 44\n",
      "==> testing\n",
      "NMI: 0.5545336008071899\n",
      " \n",
      "==> online epoch # 461 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.03424898535013199\n",
      "loss: 0.0\n",
      "loss: 0.03953111171722412\n",
      "loss: 0.05275736004114151\n",
      "loss: 0.06765227019786835\n",
      "loss: 0.01875227503478527\n",
      "loss: 0.01682945527136326\n",
      "loss: 0.13693439960479736\n",
      "loss: 0.06650599092245102\n",
      "==> online epoch # 462 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0027936070691794157\n",
      "loss: 0.025835145264863968\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.015439566224813461\n",
      "loss: 0.04003334790468216\n",
      "loss: 0.05444807931780815\n",
      "loss: 0.0\n",
      "loss: 0.16022846102714539\n",
      "==> online epoch # 463 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0834270715713501\n",
      "loss: 0.07789523899555206\n",
      "loss: 0.02538992092013359\n",
      "loss: 0.06189941614866257\n",
      "loss: 0.020369671285152435\n",
      "loss: 0.12971490621566772\n",
      "loss: 0.026510322466492653\n",
      "loss: 0.1083797961473465\n",
      "loss: 0.010673454962670803\n",
      "loss: 0.13378405570983887\n",
      "==> online epoch # 464 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06993059813976288\n",
      "loss: 0.10299209505319595\n",
      "loss: 0.113275445997715\n",
      "loss: 0.0560881569981575\n",
      "loss: 0.083596371114254\n",
      "loss: 0.029262984171509743\n",
      "loss: 0.02625408209860325\n",
      "loss: 0.035649046301841736\n",
      "loss: 0.0\n",
      "==> online epoch # 465 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1156662255525589\n",
      "loss: 0.012666736729443073\n",
      "loss: 0.0\n",
      "loss: 0.02770327962934971\n",
      "loss: 0.018211806192994118\n",
      "loss: 0.1206115260720253\n",
      "loss: 0.044241975992918015\n",
      "loss: 0.0\n",
      "loss: 0.07606880366802216\n",
      "loss: 0.12271425873041153\n",
      "==> online epoch # 466 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.10174062103033066\n",
      "loss: 0.0058730486780405045\n",
      "loss: 0.003117077983915806\n",
      "loss: 0.04945860058069229\n",
      "loss: 0.04213457554578781\n",
      "loss: 0.10808901488780975\n",
      "loss: 0.0\n",
      "loss: 0.041013605892658234\n",
      "loss: 0.03787462040781975\n",
      "loss: 0.0\n",
      "==> online epoch # 467 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.020376920700073242\n",
      "loss: 0.13709260523319244\n",
      "loss: 0.09080246090888977\n",
      "loss: 0.004189381841570139\n",
      "loss: 0.01188068650662899\n",
      "loss: 0.051422249525785446\n",
      "loss: 0.031245458871126175\n",
      "loss: 0.00013006626977585256\n",
      "loss: 0.19005891680717468\n",
      "==> online epoch # 468 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03908541798591614\n",
      "loss: 0.06646070629358292\n",
      "loss: 0.012236037291586399\n",
      "loss: 0.01323387585580349\n",
      "loss: 0.06082319840788841\n",
      "loss: 0.019002210348844528\n",
      "loss: 0.02991955541074276\n",
      "loss: 0.031494855880737305\n",
      "loss: 0.04740481823682785\n",
      "loss: 0.09176668524742126\n",
      "==> online epoch # 469 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06619023531675339\n",
      "loss: 0.22432950139045715\n",
      "loss: 0.08008560538291931\n",
      "loss: 0.015048757195472717\n",
      "loss: 0.004279843531548977\n",
      "loss: 0.04779484495520592\n",
      "loss: 0.0038452919106930494\n",
      "loss: 0.028158148750662804\n",
      "==> online epoch # 470 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07921186834573746\n",
      "loss: 0.05747901275753975\n",
      "loss: 0.0\n",
      "loss: 0.019115809351205826\n",
      "loss: 0.08081702888011932\n",
      "loss: 0.0\n",
      "loss: 0.030346253886818886\n",
      "loss: 0.03284221515059471\n",
      "loss: 0.11790712177753448\n",
      "==> online epoch # 471 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.00698503851890564\n",
      "loss: 0.027649464085698128\n",
      "loss: 0.10990909487009048\n",
      "loss: 0.0316196009516716\n",
      "loss: 0.03734052926301956\n",
      "loss: 0.1717599332332611\n",
      "loss: 0.02328880876302719\n",
      "loss: 0.0686803013086319\n",
      "==> online epoch # 472 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.16609922051429749\n",
      "loss: 0.02974187396466732\n",
      "loss: 0.0031626136042177677\n",
      "loss: 0.04972577095031738\n",
      "loss: 0.03305550292134285\n",
      "loss: 0.010271596722304821\n",
      "loss: 0.0\n",
      "loss: 0.045948393642902374\n",
      "loss: 0.04337318614125252\n",
      "loss: 0.018755672499537468\n",
      "==> online epoch # 473 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06514273583889008\n",
      "loss: 0.026762062683701515\n",
      "loss: 0.024736560881137848\n",
      "loss: 0.037165675312280655\n",
      "loss: 0.02874934673309326\n",
      "loss: 0.16436466574668884\n",
      "loss: 0.0628708228468895\n",
      "loss: 0.0007516116020269692\n",
      "==> online epoch # 474 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04463822394609451\n",
      "loss: 0.04661119729280472\n",
      "loss: 0.0\n",
      "loss: 0.04414479807019234\n",
      "loss: 0.18926751613616943\n",
      "loss: 0.06992641091346741\n",
      "loss: 0.0\n",
      "loss: 0.020316293463110924\n",
      "==> online epoch # 475 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.06431248784065247\n",
      "loss: 0.06321559846401215\n",
      "loss: 0.09584368765354156\n",
      "loss: 0.0\n",
      "loss: 0.09263824671506882\n",
      "loss: 0.0\n",
      "loss: 0.03341120108962059\n",
      "loss: 0.014874795451760292\n",
      "==> online epoch # 476 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.02618267759680748\n",
      "loss: 0.034565526992082596\n",
      "loss: 0.03915974497795105\n",
      "loss: 0.020278071984648705\n",
      "loss: 0.10308367013931274\n",
      "loss: 0.04510447755455971\n",
      "loss: 0.036686066538095474\n",
      "loss: 0.05142327770590782\n",
      "==> online epoch # 477 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03238943591713905\n",
      "loss: 0.0\n",
      "loss: 0.1172768697142601\n",
      "loss: 0.0\n",
      "loss: 0.035958267748355865\n",
      "loss: 0.1037348061800003\n",
      "loss: 0.056980524212121964\n",
      "loss: 0.05152282863855362\n",
      "loss: 0.05615413188934326\n",
      "loss: 0.09545380622148514\n",
      "==> online epoch # 478 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03304986283183098\n",
      "loss: 0.03343306481838226\n",
      "loss: 0.04711434990167618\n",
      "loss: 0.03878576681017876\n",
      "loss: 0.020510276779532433\n",
      "loss: 0.042579974979162216\n",
      "loss: 0.08372637629508972\n",
      "loss: 0.013089917600154877\n",
      "loss: 0.10279824584722519\n",
      "loss: 0.016585638746619225\n",
      "==> online epoch # 479 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02540489099919796\n",
      "loss: 0.05786437541246414\n",
      "loss: 0.0\n",
      "loss: 0.09733718633651733\n",
      "loss: 0.08010409027338028\n",
      "loss: 0.04770097881555557\n",
      "loss: 0.015474868007004261\n",
      "loss: 0.028891418129205704\n",
      "loss: 0.0792483240365982\n",
      "loss: 0.0655205100774765\n",
      "==> online epoch # 480 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.027098704129457474\n",
      "loss: 0.0\n",
      "loss: 0.09603390097618103\n",
      "loss: 0.02920759841799736\n",
      "loss: 0.0513077974319458\n",
      "loss: 0.24743318557739258\n",
      "loss: 0.04154520854353905\n",
      "loss: 0.4925549626350403\n",
      "loss: 0.005876900162547827\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0764, dtype=torch.float64)\n",
      "nclusters:  44\n",
      "Time elapsed for computing cluster affinity: 14.240924835205078 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 35\n",
      "==> testing\n",
      "NMI: 0.5615835785865784\n",
      " \n",
      "==> online epoch # 481 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.121445432305336\n",
      "loss: 0.03741810470819473\n",
      "loss: 0.08073326200246811\n",
      "loss: 0.06680849194526672\n",
      "loss: 0.010900825262069702\n",
      "loss: 0.057242751121520996\n",
      "loss: 0.10890360921621323\n",
      "loss: 0.16284826397895813\n",
      "loss: 0.07444238662719727\n",
      "loss: 0.02359049767255783\n",
      "==> online epoch # 482 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0413094237446785\n",
      "loss: 0.1041405200958252\n",
      "loss: 0.03380294516682625\n",
      "loss: 0.04650668054819107\n",
      "loss: 0.10855772346258163\n",
      "loss: 0.0\n",
      "loss: 0.020596245303750038\n",
      "loss: 0.04440261423587799\n",
      "loss: 0.07880475372076035\n",
      "loss: 0.08130570501089096\n",
      "==> online epoch # 483 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.025141417980194092\n",
      "loss: 0.08426533639431\n",
      "loss: 0.050339531153440475\n",
      "loss: 0.018549133092164993\n",
      "loss: 0.04103357717394829\n",
      "loss: 0.029975272715091705\n",
      "loss: 0.15541517734527588\n",
      "loss: 0.05651359632611275\n",
      "loss: 0.014744602143764496\n",
      "loss: 0.07250896841287613\n",
      "==> online epoch # 484 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.10378186404705048\n",
      "loss: 0.004953406751155853\n",
      "loss: 0.0\n",
      "loss: 0.0332016721367836\n",
      "loss: 0.08338429778814316\n",
      "loss: 0.03931722044944763\n",
      "==> online epoch # 485 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.022681357339024544\n",
      "loss: 0.050465453416109085\n",
      "loss: 0.0\n",
      "loss: 0.01923590712249279\n",
      "loss: 0.023566970601677895\n",
      "loss: 0.04951537773013115\n",
      "loss: 0.07121650129556656\n",
      "loss: 0.06084531545639038\n",
      "loss: 0.0951002836227417\n",
      "==> online epoch # 486 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.16895520687103271\n",
      "loss: 0.0\n",
      "loss: 0.2104577273130417\n",
      "loss: 0.5181906819343567\n",
      "loss: 0.054672662168741226\n",
      "loss: 0.08467171341180801\n",
      "loss: 0.0\n",
      "loss: 0.1671467125415802\n",
      "loss: 0.05767253041267395\n",
      "loss: 0.0784599632024765\n",
      "==> online epoch # 487 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.15080176293849945\n",
      "loss: 0.024222146719694138\n",
      "loss: 0.08394637703895569\n",
      "loss: 0.0169705580919981\n",
      "loss: 0.09397823363542557\n",
      "loss: 0.09185105562210083\n",
      "loss: 0.05909904092550278\n",
      "loss: 0.02241515927016735\n",
      "loss: 0.1098594218492508\n",
      "loss: 0.0037856728304177523\n",
      "==> online epoch # 488 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07390554249286652\n",
      "loss: 0.0475616455078125\n",
      "loss: 0.006584117189049721\n",
      "loss: 0.08571415394544601\n",
      "loss: 0.09079395979642868\n",
      "loss: 0.06403366476297379\n",
      "loss: 0.08027692884206772\n",
      "loss: 0.037084996700286865\n",
      "loss: 0.08070418983697891\n",
      "==> online epoch # 489 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1545000821352005\n",
      "loss: 0.009854698553681374\n",
      "loss: 0.019163060933351517\n",
      "loss: 0.09500626474618912\n",
      "loss: 0.04844005033373833\n",
      "loss: 0.0699552670121193\n",
      "loss: 0.0\n",
      "loss: 0.07288642972707748\n",
      "loss: 0.018936801701784134\n",
      "loss: 0.02242947183549404\n",
      "==> online epoch # 490 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07993003726005554\n",
      "loss: 0.009404084645211697\n",
      "loss: 0.005312324035912752\n",
      "loss: 0.10769309103488922\n",
      "loss: 0.006925584282726049\n",
      "loss: 0.018578052520751953\n",
      "loss: 0.054180968552827835\n",
      "loss: 0.010595984756946564\n",
      "loss: 0.13691416382789612\n",
      "loss: 0.09110428392887115\n",
      "==> online epoch # 491 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03467492014169693\n",
      "loss: 0.020280975848436356\n",
      "loss: 0.0\n",
      "loss: 0.047309428453445435\n",
      "loss: 0.009325146675109863\n",
      "loss: 0.020062969997525215\n",
      "loss: 0.016848329454660416\n",
      "loss: 0.024596523493528366\n",
      "loss: 0.08806978911161423\n",
      "loss: 0.11861393600702286\n",
      "==> online epoch # 492 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.09194259345531464\n",
      "loss: 0.08104360103607178\n",
      "loss: 0.06706082075834274\n",
      "loss: 0.013116458430886269\n",
      "loss: 0.0\n",
      "loss: 0.2130066156387329\n",
      "loss: 0.06758328527212143\n",
      "loss: 0.1125166043639183\n",
      "loss: 0.0374758206307888\n",
      "==> online epoch # 493 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06283146142959595\n",
      "loss: 0.05467383190989494\n",
      "loss: 0.09561336040496826\n",
      "loss: 0.019025087356567383\n",
      "loss: 0.0390927828848362\n",
      "loss: 0.021212024614214897\n",
      "loss: 0.02827044017612934\n",
      "loss: 0.0044732172973454\n",
      "loss: 0.015328509733080864\n",
      "loss: 0.020751401782035828\n",
      "==> online epoch # 494 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0642622634768486\n",
      "loss: 0.019806183874607086\n",
      "loss: 0.1162848174571991\n",
      "loss: 0.04020916298031807\n",
      "loss: 0.0\n",
      "loss: 0.042295098304748535\n",
      "loss: 0.05442126840353012\n",
      "loss: 0.06796654313802719\n",
      "loss: 0.07236502319574356\n",
      "loss: 0.016558000817894936\n",
      "==> online epoch # 495 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0206017903983593\n",
      "loss: 0.006195047404617071\n",
      "loss: 0.030549604445695877\n",
      "loss: 0.01718735322356224\n",
      "loss: 0.146209716796875\n",
      "loss: 0.020747000351548195\n",
      "loss: 0.006362837739288807\n",
      "loss: 0.17596925795078278\n",
      "loss: 0.05807051807641983\n",
      "loss: 0.11269011348485947\n",
      "==> online epoch # 496 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.027623513713479042\n",
      "loss: 0.14184030890464783\n",
      "loss: 0.03506788611412048\n",
      "loss: 0.06866210699081421\n",
      "loss: 0.09652191400527954\n",
      "loss: 0.006865516304969788\n",
      "loss: 0.0\n",
      "loss: 0.09489577263593674\n",
      "loss: 0.08108280599117279\n",
      "loss: 0.05438793823122978\n",
      "==> online epoch # 497 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04310047999024391\n",
      "loss: 0.005176729056984186\n",
      "loss: 0.051699891686439514\n",
      "loss: 0.055181778967380524\n",
      "loss: 0.030741453170776367\n",
      "loss: 0.06890333443880081\n",
      "loss: 0.05150787904858589\n",
      "loss: 0.005753904581069946\n",
      "loss: 0.026735762134194374\n",
      "loss: 0.0806700587272644\n",
      "==> online epoch # 498 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03702615201473236\n",
      "loss: 0.2198004275560379\n",
      "loss: 0.0\n",
      "loss: 0.028463352471590042\n",
      "loss: 0.039884086698293686\n",
      "loss: 0.07743240892887115\n",
      "loss: 0.06190170347690582\n",
      "loss: 0.0421576090157032\n",
      "loss: 0.07971061021089554\n",
      "loss: 0.04082401841878891\n",
      "==> online epoch # 499 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08996275812387466\n",
      "loss: 0.010212080553174019\n",
      "loss: 0.0920891985297203\n",
      "loss: 0.11242911219596863\n",
      "loss: 0.047750961035490036\n",
      "loss: 0.05652640387415886\n",
      "loss: 0.030998436734080315\n",
      "loss: 0.029615363106131554\n",
      "loss: 0.06975207477807999\n",
      "loss: 0.0\n",
      "==> online epoch # 500 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.1276581734418869\n",
      "loss: 0.0810149759054184\n",
      "loss: 0.09270083159208298\n",
      "loss: 0.045855820178985596\n",
      "loss: 0.13066522777080536\n",
      "loss: 0.039769046008586884\n",
      "loss: 0.013177032582461834\n",
      "loss: 0.08071360737085342\n",
      "loss: 0.031163373962044716\n",
      "loss: 0.0\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0730, dtype=torch.float64)\n",
      "nclusters:  35\n",
      "Time elapsed for computing cluster affinity: 14.217514991760254 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 28\n",
      "==> testing\n",
      "NMI: 0.5543019771575928\n",
      " \n",
      "==> online epoch # 501 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.008452936075627804\n",
      "loss: 0.012120528146624565\n",
      "loss: 0.03486722707748413\n",
      "loss: 0.09028802067041397\n",
      "loss: 0.03846261650323868\n",
      "loss: 0.027769584208726883\n",
      "loss: 0.0\n",
      "loss: 0.08186595886945724\n",
      "loss: 0.04947240278124809\n",
      "loss: 0.09005127847194672\n",
      "==> online epoch # 502 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.045270729809999466\n",
      "loss: 0.022638732567429543\n",
      "loss: 0.055953413248062134\n",
      "loss: 0.10455793142318726\n",
      "loss: 0.021684929728507996\n",
      "loss: 0.07716470956802368\n",
      "loss: 0.034571632742881775\n",
      "loss: 0.009938685223460197\n",
      "loss: 0.05194603279232979\n",
      "==> online epoch # 503 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.17699095606803894\n",
      "loss: 0.03607157617807388\n",
      "loss: 0.052646227180957794\n",
      "loss: 0.07548227161169052\n",
      "loss: 0.0\n",
      "loss: 0.006286808755248785\n",
      "loss: 0.05742649361491203\n",
      "loss: 0.07716266810894012\n",
      "loss: 0.08953376114368439\n",
      "loss: 0.04287836700677872\n",
      "==> online epoch # 504 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.026454709470272064\n",
      "loss: 0.07979877293109894\n",
      "loss: 0.031812891364097595\n",
      "loss: 0.016614243388175964\n",
      "loss: 0.036232560873031616\n",
      "loss: 0.11650785058736801\n",
      "loss: 0.02204071916639805\n",
      "loss: 0.018342403694987297\n",
      "loss: 0.0268937386572361\n",
      "==> online epoch # 505 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.007825462147593498\n",
      "loss: 0.011371621862053871\n",
      "loss: 0.15100429952144623\n",
      "loss: 0.0\n",
      "loss: 0.0932907834649086\n",
      "loss: 0.10969836264848709\n",
      "loss: 0.0\n",
      "loss: 0.05564550310373306\n",
      "loss: 0.05470243841409683\n",
      "loss: 0.010114723816514015\n",
      "==> online epoch # 506 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.10062544047832489\n",
      "loss: 0.17219232022762299\n",
      "loss: 0.0183364637196064\n",
      "loss: 0.008461357094347477\n",
      "loss: 0.033320099115371704\n",
      "loss: 0.03279596567153931\n",
      "loss: 0.12376236915588379\n",
      "loss: 0.02796316146850586\n",
      "loss: 0.020245317369699478\n",
      "loss: 0.1392803192138672\n",
      "==> online epoch # 507 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.010075938887894154\n",
      "loss: 0.08025066554546356\n",
      "loss: 0.04522518441081047\n",
      "loss: 0.035943374037742615\n",
      "loss: 0.010310428217053413\n",
      "loss: 0.0\n",
      "loss: 0.030173033475875854\n",
      "loss: 0.032613154500722885\n",
      "loss: 0.08372534811496735\n",
      "==> online epoch # 508 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.05546615645289421\n",
      "loss: 0.018278975039720535\n",
      "loss: 0.015794776380062103\n",
      "loss: 0.10979951173067093\n",
      "loss: 0.08104737848043442\n",
      "loss: 0.06271455436944962\n",
      "loss: 0.0\n",
      "loss: 0.03309336677193642\n",
      "loss: 0.0\n",
      "==> online epoch # 509 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.00754085648804903\n",
      "loss: 0.0465385876595974\n",
      "loss: 0.08372386544942856\n",
      "loss: 0.05785687640309334\n",
      "loss: 0.04127288609743118\n",
      "loss: 0.03923378884792328\n",
      "loss: 0.029931629076600075\n",
      "loss: 0.06794724613428116\n",
      "loss: 0.04303864389657974\n",
      "==> online epoch # 510 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.16998161375522614\n",
      "loss: 0.002857217099517584\n",
      "loss: 0.0\n",
      "loss: 0.022760964930057526\n",
      "loss: 0.053784020245075226\n",
      "loss: 0.08833365887403488\n",
      "loss: 0.07994964718818665\n",
      "loss: 0.01967576891183853\n",
      "loss: 0.009980251081287861\n",
      "==> online epoch # 511 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.20107710361480713\n",
      "loss: 0.060363542288541794\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0787431076169014\n",
      "loss: 0.006329563446342945\n",
      "loss: 0.06010039523243904\n",
      "loss: 0.04013066738843918\n",
      "loss: 0.06356343626976013\n",
      "==> online epoch # 512 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0885717049241066\n",
      "loss: 0.03171585500240326\n",
      "loss: 0.0\n",
      "loss: 0.015964075922966003\n",
      "loss: 0.054798275232315063\n",
      "loss: 0.04131750389933586\n",
      "loss: 0.017827946692705154\n",
      "loss: 0.0\n",
      "loss: 0.016245495527982712\n",
      "==> online epoch # 513 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.09530007094144821\n",
      "loss: 0.3899359703063965\n",
      "loss: 0.09791702032089233\n",
      "loss: 0.022227900102734566\n",
      "loss: 0.03836603835225105\n",
      "loss: 0.01894470304250717\n",
      "loss: 0.1238168478012085\n",
      "loss: 0.0\n",
      "loss: 0.06710703670978546\n",
      "==> online epoch # 514 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03567100316286087\n",
      "loss: 0.038939476013183594\n",
      "loss: 0.0370289571583271\n",
      "loss: 0.007875757291913033\n",
      "loss: 0.009165171533823013\n",
      "loss: 0.11220967769622803\n",
      "loss: 0.011412465013563633\n",
      "loss: 0.07795914262533188\n",
      "loss: 0.014750003814697266\n",
      "loss: 0.05158887431025505\n",
      "==> online epoch # 515 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08227324485778809\n",
      "loss: 0.05736347287893295\n",
      "loss: 0.0\n",
      "loss: 0.00842250045388937\n",
      "loss: 0.0555121973156929\n",
      "loss: 0.03557771444320679\n",
      "loss: 0.008712869137525558\n",
      "loss: 0.013176409527659416\n",
      "loss: 0.016070958226919174\n",
      "==> online epoch # 516 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.011728385463356972\n",
      "loss: 0.06999292224645615\n",
      "loss: 0.0\n",
      "loss: 0.006497151218354702\n",
      "loss: 0.09928986430168152\n",
      "loss: 0.038770973682403564\n",
      "loss: 0.0209428109228611\n",
      "loss: 0.01962622068822384\n",
      "loss: 0.009836015291512012\n",
      "loss: 0.2807874381542206\n",
      "==> online epoch # 517 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.05706397816538811\n",
      "loss: 0.09876760095357895\n",
      "loss: 0.02517803944647312\n",
      "loss: 0.044913697987794876\n",
      "loss: 0.0399850569665432\n",
      "loss: 0.02084825001657009\n",
      "loss: 0.08635033667087555\n",
      "loss: 0.06190953403711319\n",
      "loss: 0.04438582807779312\n",
      "loss: 0.06812940537929535\n",
      "==> online epoch # 518 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.004154942464083433\n",
      "loss: 0.019538646563887596\n",
      "loss: 0.0485394187271595\n",
      "loss: 0.025381291285157204\n",
      "loss: 0.016793813556432724\n",
      "loss: 0.059117261320352554\n",
      "loss: 0.05060476437211037\n",
      "loss: 0.0\n",
      "loss: 0.005926665849983692\n",
      "loss: 0.07552246004343033\n",
      "==> online epoch # 519 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.011139271780848503\n",
      "loss: 0.00746601028367877\n",
      "loss: 0.04562996327877045\n",
      "loss: 0.024603793397545815\n",
      "loss: 0.10165712982416153\n",
      "loss: 0.017587881535291672\n",
      "loss: 0.07510226219892502\n",
      "loss: 0.04967654496431351\n",
      "loss: 0.11423449963331223\n",
      "loss: 0.016948241740465164\n",
      "==> online epoch # 520 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.023625047877430916\n",
      "loss: 0.05439210683107376\n",
      "loss: 0.04091532155871391\n",
      "loss: 0.0212718416005373\n",
      "loss: 0.04825969040393829\n",
      "loss: 0.07148884236812592\n",
      "loss: 0.07567651569843292\n",
      "loss: 0.028007978573441505\n",
      "loss: 0.1026298850774765\n",
      "loss: 0.0022146350238472223\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0760, dtype=torch.float64)\n",
      "nclusters:  28\n",
      "Time elapsed for computing cluster affinity: 13.967010974884033 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 22\n",
      "==> testing\n",
      "NMI: 0.550230860710144\n",
      " \n",
      "==> online epoch # 521 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.025203239172697067\n",
      "loss: 0.09353788197040558\n",
      "loss: 0.04473285377025604\n",
      "loss: 0.008670591749250889\n",
      "loss: 0.11062300205230713\n",
      "loss: 0.010817510075867176\n",
      "loss: 0.0067567965015769005\n",
      "loss: 0.009182382375001907\n",
      "loss: 0.026709794998168945\n",
      "loss: 0.0426003597676754\n",
      "==> online epoch # 522 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03361764922738075\n",
      "loss: 0.04944916442036629\n",
      "loss: 0.06700731813907623\n",
      "loss: 0.09588544070720673\n",
      "loss: 0.02149299718439579\n",
      "loss: 0.0022663669660687447\n",
      "loss: 0.011919572949409485\n",
      "loss: 0.05758579447865486\n",
      "loss: 0.03800147771835327\n",
      "loss: 0.03887450322508812\n",
      "==> online epoch # 523 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04352594166994095\n",
      "loss: 0.04263397678732872\n",
      "loss: 0.0416252426803112\n",
      "loss: 0.059100307524204254\n",
      "loss: 0.05026467517018318\n",
      "loss: 0.048667095601558685\n",
      "loss: 0.051781654357910156\n",
      "loss: 0.06941554695367813\n",
      "loss: 0.0726107805967331\n",
      "==> online epoch # 524 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0003089621604885906\n",
      "loss: 0.05979560315608978\n",
      "loss: 0.01617611013352871\n",
      "loss: 0.004194289445877075\n",
      "loss: 0.045962680131196976\n",
      "loss: 0.10337746888399124\n",
      "loss: 0.0\n",
      "loss: 0.09698104113340378\n",
      "loss: 0.08190026879310608\n",
      "==> online epoch # 525 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.07832510024309158\n",
      "loss: 0.025625532492995262\n",
      "loss: 0.1015065386891365\n",
      "loss: 0.043528441339731216\n",
      "loss: 0.0\n",
      "loss: 0.003540987614542246\n",
      "loss: 0.024423811584711075\n",
      "loss: 0.0\n",
      "loss: 0.06102394312620163\n",
      "loss: 0.026852834969758987\n",
      "==> online epoch # 526 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.034118227660655975\n",
      "loss: 0.03650103881955147\n",
      "loss: 0.04163307696580887\n",
      "loss: 0.09847412258386612\n",
      "loss: 0.013920062221586704\n",
      "loss: 0.012137103825807571\n",
      "loss: 0.00813795905560255\n",
      "loss: 0.06579569727182388\n",
      "loss: 0.050258148461580276\n",
      "==> online epoch # 527 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.20952290296554565\n",
      "loss: 0.052821628749370575\n",
      "loss: 0.0\n",
      "loss: 0.10212355107069016\n",
      "loss: 0.10167574137449265\n",
      "loss: 0.019041558727622032\n",
      "loss: 0.04731591418385506\n",
      "loss: 0.050744395703077316\n",
      "loss: 0.028492916375398636\n",
      "loss: 0.0\n",
      "==> online epoch # 528 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.037392184138298035\n",
      "loss: 0.04792498052120209\n",
      "loss: 0.032455265522003174\n",
      "loss: 0.057374048978090286\n",
      "loss: 0.012679880484938622\n",
      "loss: 0.0013989672297611833\n",
      "loss: 0.04112779721617699\n",
      "loss: 0.03917790204286575\n",
      "loss: 0.09981600195169449\n",
      "loss: 0.029642704874277115\n",
      "==> online epoch # 529 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.018570221960544586\n",
      "loss: 0.09978922456502914\n",
      "loss: 0.07222766429185867\n",
      "loss: 0.052828334271907806\n",
      "loss: 0.012238521128892899\n",
      "loss: 0.020991699770092964\n",
      "loss: 0.002277188468724489\n",
      "loss: 0.0\n",
      "loss: 0.005790234077721834\n",
      "loss: 0.035535018891096115\n",
      "==> online epoch # 530 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08191759139299393\n",
      "loss: 0.021441472694277763\n",
      "loss: 0.04470708593726158\n",
      "loss: 0.14191263914108276\n",
      "loss: 0.06290633976459503\n",
      "loss: 0.00033892839564941823\n",
      "loss: 0.08742912113666534\n",
      "loss: 0.1152845025062561\n",
      "loss: 0.01922169141471386\n",
      "loss: 0.02768089435994625\n",
      "==> online epoch # 531 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0021607684902846813\n",
      "loss: 0.08191961795091629\n",
      "loss: 0.015545761212706566\n",
      "loss: 0.04129665344953537\n",
      "loss: 0.04152631759643555\n",
      "loss: 0.01859547570347786\n",
      "loss: 0.03728461265563965\n",
      "loss: 0.06464187800884247\n",
      "loss: 0.02520047128200531\n",
      "loss: 0.0312060359865427\n",
      "==> online epoch # 532 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.011404085904359818\n",
      "loss: 0.03989734128117561\n",
      "loss: 0.0\n",
      "loss: 0.03070410154759884\n",
      "loss: 0.15724630653858185\n",
      "loss: 0.0\n",
      "loss: 0.01571575365960598\n",
      "loss: 0.0\n",
      "loss: 0.049226488918066025\n",
      "loss: 0.009508328512310982\n",
      "==> online epoch # 533 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.07384800165891647\n",
      "loss: 0.05742935091257095\n",
      "loss: 0.029874389991164207\n",
      "loss: 0.057893287390470505\n",
      "loss: 0.10373616218566895\n",
      "loss: 0.011759083718061447\n",
      "loss: 0.051307518035173416\n",
      "loss: 0.024400293827056885\n",
      "loss: 0.027993688359856606\n",
      "==> online epoch # 534 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.03338540717959404\n",
      "loss: 0.06386261433362961\n",
      "loss: 0.12096833437681198\n",
      "loss: 0.049313150346279144\n",
      "loss: 0.0590498261153698\n",
      "loss: 0.010654326528310776\n",
      "loss: 0.09122715145349503\n",
      "loss: 0.09894787520170212\n",
      "==> online epoch # 535 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03808746486902237\n",
      "loss: 0.009339174255728722\n",
      "loss: 0.027410829439759254\n",
      "loss: 0.01775788888335228\n",
      "loss: 0.004996319301426411\n",
      "loss: 0.0\n",
      "loss: 0.0566132552921772\n",
      "loss: 0.01876690424978733\n",
      "loss: 0.028209000825881958\n",
      "loss: 0.019777661189436913\n",
      "==> online epoch # 536 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03566070273518562\n",
      "loss: 0.08615124970674515\n",
      "loss: 0.06194803863763809\n",
      "loss: 0.028072472661733627\n",
      "loss: 0.09762553125619888\n",
      "loss: 0.027421439066529274\n",
      "loss: 0.04913327470421791\n",
      "loss: 0.030230525881052017\n",
      "loss: 0.03706607222557068\n",
      "loss: 0.05284501984715462\n",
      "==> online epoch # 537 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0024497173726558685\n",
      "loss: 0.03302959352731705\n",
      "loss: 0.04943772405385971\n",
      "loss: 0.03208597004413605\n",
      "loss: 0.01597224362194538\n",
      "loss: 0.01907423511147499\n",
      "loss: 0.07985012233257294\n",
      "loss: 0.04829952493309975\n",
      "loss: 0.030650489032268524\n",
      "loss: 0.08356703817844391\n",
      "==> online epoch # 538 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.03084336407482624\n",
      "loss: 0.04471052438020706\n",
      "loss: 0.03434627503156662\n",
      "loss: 0.01838444359600544\n",
      "loss: 0.03887646645307541\n",
      "loss: 0.04491540044546127\n",
      "loss: 0.05157928541302681\n",
      "loss: 0.10011765360832214\n",
      "loss: 0.03197583556175232\n",
      "==> online epoch # 539 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0685674175620079\n",
      "loss: 0.004448242951184511\n",
      "loss: 0.11995244771242142\n",
      "loss: 0.15841467678546906\n",
      "loss: 0.11264868825674057\n",
      "loss: 0.0\n",
      "loss: 0.027291107922792435\n",
      "loss: 0.09865956008434296\n",
      "loss: 0.07089859247207642\n",
      "loss: 0.015142425894737244\n",
      "==> online epoch # 540 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.039795882999897\n",
      "loss: 0.03311241790652275\n",
      "loss: 0.0\n",
      "loss: 0.007622636388987303\n",
      "loss: 0.0\n",
      "loss: 0.038354333490133286\n",
      "loss: 0.05533290654420853\n",
      "loss: 0.01916557550430298\n",
      "loss: 0.047379691153764725\n",
      "loss: 0.020367145538330078\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0776, dtype=torch.float64)\n",
      "nclusters:  22\n",
      "Time elapsed for computing cluster affinity: 13.77299690246582 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 17\n",
      "==> testing\n",
      "NMI: 0.5518248677253723\n",
      " \n",
      "==> online epoch # 541 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.054018277674913406\n",
      "loss: 0.08633565902709961\n",
      "loss: 0.029689129441976547\n",
      "loss: 0.0809105709195137\n",
      "loss: 0.030139191076159477\n",
      "loss: 0.03527423366904259\n",
      "loss: 0.015250610187649727\n",
      "loss: 0.03012341633439064\n",
      "loss: 0.036026082932949066\n",
      "loss: 0.011376414448022842\n",
      "==> online epoch # 542 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02729831449687481\n",
      "loss: 0.03390532359480858\n",
      "loss: 0.043929655104875565\n",
      "loss: 0.06938669085502625\n",
      "loss: 0.08544940501451492\n",
      "loss: 0.06403562426567078\n",
      "loss: 0.010462812148034573\n",
      "loss: 0.042700719088315964\n",
      "loss: 0.04919334128499031\n",
      "loss: 0.034776944667100906\n",
      "==> online epoch # 543 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.008487474173307419\n",
      "loss: 0.05956912040710449\n",
      "loss: 0.08434310555458069\n",
      "loss: 0.01569862850010395\n",
      "loss: 0.00524037005379796\n",
      "loss: 0.037178780883550644\n",
      "loss: 0.005055889952927828\n",
      "loss: 0.14210405945777893\n",
      "loss: 0.025618281215429306\n",
      "loss: 0.04662913456559181\n",
      "==> online epoch # 544 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.05527560040354729\n",
      "loss: 0.022983083501458168\n",
      "loss: 0.07436913251876831\n",
      "loss: 0.14367522299289703\n",
      "loss: 0.056558866053819656\n",
      "loss: 0.11526588350534439\n",
      "loss: 0.016676168888807297\n",
      "loss: 0.041366323828697205\n",
      "loss: 0.14931254088878632\n",
      "loss: 0.11514873802661896\n",
      "==> online epoch # 545 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.081644207239151\n",
      "loss: 0.00047544686822220683\n",
      "loss: 0.023534703999757767\n",
      "loss: 0.10215325653553009\n",
      "loss: 0.024501029402017593\n",
      "loss: 0.059952106326818466\n",
      "loss: 0.09805909544229507\n",
      "loss: 0.03621789813041687\n",
      "loss: 0.02388601563870907\n",
      "loss: 0.02711227536201477\n",
      "==> online epoch # 546 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.024878760799765587\n",
      "loss: 0.03024030290544033\n",
      "loss: 0.054554034024477005\n",
      "loss: 0.034117113798856735\n",
      "loss: 0.06727395206689835\n",
      "loss: 0.029489168897271156\n",
      "loss: 0.024601170793175697\n",
      "loss: 0.04547237604856491\n",
      "loss: 0.0013758731074631214\n",
      "loss: 0.08014675974845886\n",
      "==> online epoch # 547 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.06730827689170837\n",
      "loss: 0.008033518679440022\n",
      "loss: 0.02988409623503685\n",
      "loss: 0.00975923240184784\n",
      "loss: 0.01543809287250042\n",
      "loss: 0.02079595997929573\n",
      "loss: 0.03212615102529526\n",
      "loss: 0.013673235662281513\n",
      "loss: 0.12701961398124695\n",
      "==> online epoch # 548 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.09454566985368729\n",
      "loss: 0.026182131841778755\n",
      "loss: 0.05284464731812477\n",
      "loss: 0.08102340996265411\n",
      "loss: 0.04083188995718956\n",
      "loss: 0.05487257242202759\n",
      "loss: 0.04599843546748161\n",
      "loss: 0.060946352779865265\n",
      "loss: 0.002592685865238309\n",
      "loss: 0.1143740564584732\n",
      "==> online epoch # 549 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.09901884943246841\n",
      "loss: 0.004182242322713137\n",
      "loss: 0.04663776606321335\n",
      "loss: 0.007615414448082447\n",
      "loss: 0.07166001200675964\n",
      "loss: 0.014267787337303162\n",
      "loss: 0.024362673982977867\n",
      "loss: 0.050047703087329865\n",
      "loss: 0.08506939560174942\n",
      "loss: 0.0\n",
      "==> online epoch # 550 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.029170455411076546\n",
      "loss: 0.01906108483672142\n",
      "loss: 0.10073119401931763\n",
      "loss: 0.04926057904958725\n",
      "loss: 0.03517896682024002\n",
      "loss: 0.052196286618709564\n",
      "loss: 0.0395498052239418\n",
      "loss: 0.009665020741522312\n",
      "loss: 0.08167698234319687\n",
      "loss: 0.05712147429585457\n",
      "==> online epoch # 551 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.026235783472657204\n",
      "loss: 0.05050086975097656\n",
      "loss: 0.026080453768372536\n",
      "loss: 0.053934793919324875\n",
      "loss: 0.0\n",
      "loss: 0.04286450520157814\n",
      "loss: 0.05659015104174614\n",
      "loss: 0.18208256363868713\n",
      "loss: 0.0007405310752801597\n",
      "loss: 0.02462334930896759\n",
      "==> online epoch # 552 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.08207681030035019\n",
      "loss: 0.1230655163526535\n",
      "loss: 0.0\n",
      "loss: 0.023902513086795807\n",
      "loss: 0.034440528601408005\n",
      "loss: 0.004828355275094509\n",
      "loss: 0.0212510135024786\n",
      "loss: 0.05196382477879524\n",
      "loss: 0.021691076457500458\n",
      "loss: 0.06485026329755783\n",
      "==> online epoch # 553 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.01358737237751484\n",
      "loss: 0.04211840033531189\n",
      "loss: 0.009141745045781136\n",
      "loss: 0.04864136874675751\n",
      "loss: 0.03930847346782684\n",
      "loss: 0.06640549749135971\n",
      "loss: 0.05048780515789986\n",
      "loss: 0.03692784905433655\n",
      "loss: 0.03692927211523056\n",
      "loss: 0.035366035997867584\n",
      "==> online epoch # 554 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.024105947464704514\n",
      "loss: 0.03980044648051262\n",
      "loss: 0.08604878932237625\n",
      "loss: 0.0031354338862001896\n",
      "loss: 0.045302532613277435\n",
      "loss: 0.03367114067077637\n",
      "loss: 0.024708900600671768\n",
      "loss: 0.06916721910238266\n",
      "loss: 0.02818831242620945\n",
      "loss: 0.03861400857567787\n",
      "==> online epoch # 555 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06588457524776459\n",
      "loss: 0.03233475983142853\n",
      "loss: 0.059258222579956055\n",
      "loss: 0.04593170806765556\n",
      "loss: 0.03378976136445999\n",
      "loss: 0.0870940163731575\n",
      "loss: 0.007291002664715052\n",
      "loss: 0.0\n",
      "loss: 0.06120849773287773\n",
      "loss: 0.03850597143173218\n",
      "==> online epoch # 556 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04476146772503853\n",
      "loss: 0.07837150245904922\n",
      "loss: 0.10260610282421112\n",
      "loss: 0.010162116028368473\n",
      "loss: 0.050061147660017014\n",
      "loss: 0.03970202058553696\n",
      "loss: 0.055898986756801605\n",
      "loss: 0.014304384589195251\n",
      "loss: 0.0\n",
      "loss: 0.0057969046756625175\n",
      "==> online epoch # 557 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.021958421915769577\n",
      "loss: 0.004471702501177788\n",
      "loss: 0.032554883509874344\n",
      "loss: 0.030075645074248314\n",
      "loss: 0.034953922033309937\n",
      "loss: 0.07320130616426468\n",
      "loss: 0.02483230084180832\n",
      "loss: 0.1916155219078064\n",
      "loss: 0.022956321015954018\n",
      "loss: 0.04495799541473389\n",
      "==> online epoch # 558 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02185639552772045\n",
      "loss: 0.04094971716403961\n",
      "loss: 0.08412974327802658\n",
      "loss: 0.016406826674938202\n",
      "loss: 0.04168101027607918\n",
      "loss: 0.01954207755625248\n",
      "loss: 0.07760392874479294\n",
      "loss: 0.05518808960914612\n",
      "loss: 0.09772377461194992\n",
      "loss: 0.0898682177066803\n",
      "==> online epoch # 559 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.018475964665412903\n",
      "loss: 0.0160754956305027\n",
      "loss: 0.04328421130776405\n",
      "loss: 0.016398046165704727\n",
      "loss: 0.0\n",
      "loss: 0.14665056765079498\n",
      "loss: 0.1650327891111374\n",
      "loss: 0.08415109664201736\n",
      "loss: 0.019164914265275\n",
      "loss: 0.02551157958805561\n",
      "==> online epoch # 560 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03129568323493004\n",
      "loss: 0.013094344176352024\n",
      "loss: 0.008650093339383602\n",
      "loss: 0.030194897204637527\n",
      "loss: 0.06001322716474533\n",
      "loss: 0.04801080375909805\n",
      "loss: 0.0926956981420517\n",
      "loss: 0.08224344998598099\n",
      "loss: 0.0772726908326149\n",
      "loss: 0.049604982137680054\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0786, dtype=torch.float64)\n",
      "nclusters:  17\n",
      "Time elapsed for computing cluster affinity: 13.711092948913574 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 13\n",
      "==> testing\n",
      "NMI: 0.5738661289215088\n",
      " \n",
      "==> online epoch # 561 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.04035192355513573\n",
      "loss: 0.0\n",
      "loss: 0.02181389555335045\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.04935046657919884\n",
      "loss: 0.0\n",
      "==> online epoch # 562 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0035496579948812723\n",
      "loss: 0.0036540392320603132\n",
      "loss: 0.0\n",
      "loss: 0.010162523947656155\n",
      "loss: 0.0\n",
      "loss: 0.03314640000462532\n",
      "loss: 0.0045256237499415874\n",
      "loss: 0.0\n",
      "loss: 0.00027866626624017954\n",
      "loss: 0.0\n",
      "==> online epoch # 563 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.02853543497622013\n",
      "loss: 0.028645532205700874\n",
      "loss: 0.0\n",
      "loss: 0.003603505901992321\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.021514639258384705\n",
      "loss: 0.05062266066670418\n",
      "loss: 0.025667576119303703\n",
      "==> online epoch # 564 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.002048370661213994\n",
      "loss: 0.00956361461430788\n",
      "loss: 0.004277699161320925\n",
      "loss: 0.0\n",
      "loss: 0.007041386794298887\n",
      "loss: 0.0\n",
      "loss: 0.003163072280585766\n",
      "loss: 5.2215797040844336e-05\n",
      "==> online epoch # 565 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0019286781316623092\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.001332536805421114\n",
      "loss: 0.02733035571873188\n",
      "loss: 0.0\n",
      "loss: 0.007004359737038612\n",
      "loss: 0.01889871247112751\n",
      "loss: 0.00242064637131989\n",
      "loss: 0.027006564661860466\n",
      "==> online epoch # 566 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.005517513491213322\n",
      "loss: 0.042285263538360596\n",
      "loss: 0.01056645903736353\n",
      "loss: 0.0019040894694626331\n",
      "loss: 0.0\n",
      "loss: 0.050119247287511826\n",
      "loss: 0.0\n",
      "loss: 0.022960372269153595\n",
      "loss: 0.0\n",
      "loss: 0.035587865859270096\n",
      "==> online epoch # 567 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0021875211969017982\n",
      "loss: 0.0011525372974574566\n",
      "loss: 0.02406919002532959\n",
      "loss: 0.009236046113073826\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 568 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02059110812842846\n",
      "loss: 0.03259905055165291\n",
      "loss: 0.07244549691677094\n",
      "loss: 0.0034897662699222565\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.013487753458321095\n",
      "loss: 0.13219228386878967\n",
      "loss: 0.006944486405700445\n",
      "loss: 0.0\n",
      "==> online epoch # 569 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.004551490303128958\n",
      "loss: 0.03297368064522743\n",
      "loss: 0.001501288264989853\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.011040299199521542\n",
      "loss: 0.005910777021199465\n",
      "loss: 0.007888398133218288\n",
      "loss: 0.004500956740230322\n",
      "loss: 0.012961209751665592\n",
      "==> online epoch # 570 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.004573805257678032\n",
      "loss: 0.004850332625210285\n",
      "loss: 0.0021533791441470385\n",
      "loss: 0.25722160935401917\n",
      "loss: 0.011174441315233707\n",
      "loss: 0.0\n",
      "loss: 0.01998799294233322\n",
      "loss: 0.0\n",
      "loss: 0.03287827596068382\n",
      "loss: 0.0\n",
      "==> online epoch # 571 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.005621162708848715\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.018741456791758537\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.06336965411901474\n",
      "==> online epoch # 572 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.006688422989100218\n",
      "loss: 0.021413443610072136\n",
      "loss: 0.0\n",
      "loss: 0.03777291625738144\n",
      "loss: 0.004491116385906935\n",
      "loss: 0.0\n",
      "loss: 0.009052824229001999\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 573 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.004131717141717672\n",
      "loss: 0.001989340642467141\n",
      "loss: 0.01303620170801878\n",
      "loss: 0.0\n",
      "loss: 0.10224352031946182\n",
      "loss: 0.0\n",
      "loss: 0.01967809721827507\n",
      "loss: 0.10355368256568909\n",
      "loss: 0.0\n",
      "loss: 0.010807540267705917\n",
      "==> online epoch # 574 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.004285544157028198\n",
      "loss: 0.0004760061565320939\n",
      "loss: 0.005693795159459114\n",
      "loss: 0.0\n",
      "loss: 0.007096883375197649\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 575 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.04107845947146416\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.02033247798681259\n",
      "loss: 0.005527990870177746\n",
      "loss: 0.03532090038061142\n",
      "loss: 0.0\n",
      "loss: 0.0011900182580575347\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 576 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.009869925677776337\n",
      "loss: 0.21098002791404724\n",
      "loss: 0.0032739618327468634\n",
      "loss: 0.006101645529270172\n",
      "loss: 0.0\n",
      "loss: 0.04295413941144943\n",
      "loss: 0.22675871849060059\n",
      "loss: 0.029399530962109566\n",
      "loss: 0.0\n",
      "loss: 0.010118535719811916\n",
      "==> online epoch # 577 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.05351271107792854\n",
      "loss: 0.006903032306581736\n",
      "loss: 0.01570652611553669\n",
      "loss: 0.006379921920597553\n",
      "loss: 0.004912175238132477\n",
      "loss: 0.03096049837768078\n",
      "loss: 0.005085833836346865\n",
      "loss: 0.0\n",
      "loss: 0.00405166856944561\n",
      "==> online epoch # 578 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0012495564296841621\n",
      "loss: 0.015298226848244667\n",
      "loss: 0.0030221755150705576\n",
      "loss: 0.0058476924896240234\n",
      "loss: 0.0373615100979805\n",
      "loss: 0.0\n",
      "loss: 0.004740681499242783\n",
      "==> online epoch # 579 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.014423983171582222\n",
      "loss: 0.08175496757030487\n",
      "loss: 0.0\n",
      "loss: 0.011327295564115047\n",
      "loss: 0.011283989995718002\n",
      "loss: 0.007698160596191883\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.00046185063547454774\n",
      "loss: 0.006294103804975748\n",
      "==> online epoch # 580 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.014333981089293957\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0003101710753981024\n",
      "loss: 0.003644698765128851\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0782, dtype=torch.float64)\n",
      "nclusters:  13\n",
      "Time elapsed for computing cluster affinity: 10.949334859848022 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 10\n",
      "==> testing\n",
      "NMI: 0.5503460764884949\n",
      " \n",
      "==> online epoch # 581 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.005325529258698225\n",
      "loss: 0.002672243397682905\n",
      "loss: 0.0\n",
      "loss: 0.17536862194538116\n",
      "loss: 0.022342117503285408\n",
      "loss: 0.20741534233093262\n",
      "loss: 0.0001100659355870448\n",
      "loss: 0.0\n",
      "==> online epoch # 582 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.06750165671110153\n",
      "loss: 0.0\n",
      "loss: 0.020247764885425568\n",
      "loss: 0.0\n",
      "loss: 0.08628860861063004\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.04282957687973976\n",
      "loss: 0.006243763025850058\n",
      "==> online epoch # 583 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.005469650030136108\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.02193324826657772\n",
      "loss: 0.0\n",
      "loss: 0.01676560379564762\n",
      "loss: 0.0\n",
      "loss: 0.013936988078057766\n",
      "==> online epoch # 584 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0029049203731119633\n",
      "loss: 0.0008486711885780096\n",
      "loss: 0.01015972625464201\n",
      "loss: 0.0\n",
      "loss: 0.01738259568810463\n",
      "loss: 0.0032791520934551954\n",
      "loss: 0.10265838354825974\n",
      "loss: 0.01930256001651287\n",
      "loss: 0.003150870092213154\n",
      "loss: 0.037452105432748795\n",
      "==> online epoch # 585 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.05425426363945007\n",
      "loss: 0.0\n",
      "loss: 0.0015350790927186608\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.06983235478401184\n",
      "loss: 0.0\n",
      "loss: 0.026393411681056023\n",
      "loss: 0.0\n",
      "==> online epoch # 586 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.020744265988469124\n",
      "loss: 0.04465257003903389\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.04056311026215553\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 587 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0056901853531599045\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.03123522363603115\n",
      "loss: 0.0\n",
      "loss: 0.03445472940802574\n",
      "loss: 0.0233396477997303\n",
      "loss: 0.007428862154483795\n",
      "loss: 0.019323067739605904\n",
      "loss: 0.0\n",
      "==> online epoch # 588 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.00694136880338192\n",
      "loss: 0.0\n",
      "loss: 0.0013730142964050174\n",
      "loss: 0.03916173800826073\n",
      "loss: 0.0\n",
      "loss: 0.013165563344955444\n",
      "loss: 0.0045123836025595665\n",
      "loss: 0.0056709349155426025\n",
      "loss: 0.0\n",
      "==> online epoch # 589 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.010926252231001854\n",
      "loss: 0.003951834049075842\n",
      "loss: 0.03420276194810867\n",
      "loss: 0.006183800287544727\n",
      "loss: 0.01725601591169834\n",
      "loss: 0.014975079335272312\n",
      "loss: 0.005028700456023216\n",
      "loss: 0.002258807886391878\n",
      "==> online epoch # 590 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.015922358259558678\n",
      "loss: 0.017258796840906143\n",
      "loss: 0.00435161218047142\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.018113624304533005\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 591 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0028345268219709396\n",
      "loss: 0.007598772179335356\n",
      "loss: 0.0050283330492675304\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.01071880292147398\n",
      "loss: 0.0\n",
      "loss: 0.00016807724023237824\n",
      "loss: 0.0\n",
      "==> online epoch # 592 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.001384597271680832\n",
      "loss: 0.0034084185026586056\n",
      "loss: 0.0003188409609720111\n",
      "loss: 0.02174558863043785\n",
      "loss: 0.0\n",
      "loss: 0.0035487187560647726\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.004694452974945307\n",
      "loss: 0.010330168530344963\n",
      "==> online epoch # 593 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0053702411241829395\n",
      "loss: 0.0\n",
      "loss: 0.00030144170159474015\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.102679543197155\n",
      "loss: 0.032549478113651276\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 594 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.01515414472669363\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0038163536228239536\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0072778272442519665\n",
      "loss: 0.008850614540278912\n",
      "loss: 0.004308054689317942\n",
      "==> online epoch # 595 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.023004479706287384\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.012243596836924553\n",
      "loss: 0.0483756959438324\n",
      "loss: 0.0\n",
      "loss: 0.002253599464893341\n",
      "loss: 0.0018518759170547128\n",
      "==> online epoch # 596 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.035291802138090134\n",
      "loss: 0.0002576742845121771\n",
      "loss: 0.05610626935958862\n",
      "loss: 0.015170968137681484\n",
      "loss: 0.05251277610659599\n",
      "loss: 0.0036836653016507626\n",
      "loss: 0.003049709601327777\n",
      "loss: 0.012734761461615562\n",
      "loss: 0.003713468089699745\n",
      "loss: 0.02309872955083847\n",
      "==> online epoch # 597 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0028281472623348236\n",
      "loss: 0.005030452273786068\n",
      "loss: 0.0\n",
      "loss: 0.004764708224684\n",
      "loss: 0.001113462494686246\n",
      "loss: 0.0\n",
      "loss: 0.008126427419483662\n",
      "==> online epoch # 598 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.02568896859884262\n",
      "loss: 0.0003003221354447305\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.004382926970720291\n",
      "loss: 0.010322962887585163\n",
      "loss: 0.004166233818978071\n",
      "loss: 0.013018122874200344\n",
      "loss: 0.010778422467410564\n",
      "loss: 0.011337787844240665\n",
      "==> online epoch # 599 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.03459828719496727\n",
      "loss: 0.01934429444372654\n",
      "loss: 0.011220724321901798\n",
      "loss: 0.03826894611120224\n",
      "loss: 0.0\n",
      "loss: 0.011799520812928677\n",
      "loss: 0.0054931361228227615\n",
      "loss: 0.0\n",
      "loss: 0.014535382390022278\n",
      "loss: 0.013837237842381\n",
      "==> online epoch # 600 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0024745019618421793\n",
      "loss: 0.01657485030591488\n",
      "loss: 0.010432809591293335\n",
      "loss: 0.13185754418373108\n",
      "loss: 0.0\n",
      "loss: 0.017130279913544655\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0845, dtype=torch.float64)\n",
      "nclusters:  10\n",
      "Time elapsed for computing cluster affinity: 10.763442993164062 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 8\n",
      "==> testing\n",
      "NMI: 0.5480398535728455\n",
      " \n",
      "==> online epoch # 601 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.00621193228289485\n",
      "loss: 0.009608802385628223\n",
      "loss: 0.0\n",
      "loss: 0.015616379678249359\n",
      "loss: 0.0\n",
      "loss: 0.029926426708698273\n",
      "loss: 0.010836558416485786\n",
      "loss: 0.09213583171367645\n",
      "loss: 0.015438406728208065\n",
      "==> online epoch # 602 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0004604269634000957\n",
      "loss: 0.0315532349050045\n",
      "loss: 0.018186379224061966\n",
      "loss: 0.008428430184721947\n",
      "loss: 0.0\n",
      "loss: 0.003544249339029193\n",
      "loss: 0.0\n",
      "loss: 0.004519139416515827\n",
      "loss: 0.029367422685027122\n",
      "==> online epoch # 603 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.10091196000576019\n",
      "loss: 0.0059613375924527645\n",
      "loss: 0.006355924066156149\n",
      "loss: 0.02686234563589096\n",
      "loss: 0.0\n",
      "loss: 0.0020912201143801212\n",
      "loss: 0.005738836247473955\n",
      "loss: 0.031699039041996\n",
      "loss: 0.0243903249502182\n",
      "loss: 0.0\n",
      "==> online epoch # 604 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.008345365524291992\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.013576369732618332\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0024161064065992832\n",
      "==> online epoch # 605 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.004203394521027803\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.10172371566295624\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.007426009979099035\n",
      "loss: 0.0\n",
      "loss: 0.005711042787879705\n",
      "==> online epoch # 606 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.011857286095619202\n",
      "loss: 0.0\n",
      "loss: 0.051245324313640594\n",
      "loss: 0.003586242673918605\n",
      "loss: 0.058277204632759094\n",
      "loss: 0.0\n",
      "loss: 0.032095834612846375\n",
      "loss: 0.0459301583468914\n",
      "loss: 0.047877952456474304\n",
      "loss: 0.0\n",
      "==> online epoch # 607 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.016514528542757034\n",
      "loss: 0.01008938904851675\n",
      "loss: 0.0\n",
      "loss: 0.0050640772096812725\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.03044215589761734\n",
      "loss: 0.003687355201691389\n",
      "loss: 0.001629329752177\n",
      "==> online epoch # 608 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.006753772031515837\n",
      "loss: 0.0038522062823176384\n",
      "loss: 0.01196125615388155\n",
      "loss: 0.03325430676341057\n",
      "loss: 0.008784649893641472\n",
      "loss: 0.08317705988883972\n",
      "loss: 0.0\n",
      "loss: 0.056346338242292404\n",
      "loss: 0.0064149401150643826\n",
      "loss: 0.002499346388503909\n",
      "==> online epoch # 609 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.026411743834614754\n",
      "loss: 0.0\n",
      "loss: 0.003166500013321638\n",
      "loss: 0.008005376905202866\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.0055130054242908955\n",
      "loss: 0.0\n",
      "loss: 0.013967384584248066\n",
      "loss: 0.0070539494045078754\n",
      "==> online epoch # 610 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.016590557992458344\n",
      "loss: 0.0\n",
      "loss: 0.009201579727232456\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.19477440416812897\n",
      "loss: 0.03787504509091377\n",
      "loss: 0.020872628316283226\n",
      "loss: 0.013998826034367085\n",
      "loss: 0.0\n",
      "==> online epoch # 611 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.009135372005403042\n",
      "loss: 0.0010814396664500237\n",
      "loss: 0.0115447286516428\n",
      "loss: 0.0\n",
      "loss: 0.06370425969362259\n",
      "loss: 0.0\n",
      "loss: 0.008751343935728073\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "==> online epoch # 612 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0022925129160284996\n",
      "loss: 0.05264410749077797\n",
      "loss: 0.0\n",
      "loss: 0.00497317872941494\n",
      "loss: 0.0035643689334392548\n",
      "loss: 0.00397124420851469\n",
      "loss: 0.11792879551649094\n",
      "loss: 0.015856945887207985\n",
      "loss: 0.0\n",
      "==> online epoch # 613 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.05723593011498451\n",
      "loss: 0.0\n",
      "loss: 0.008243048563599586\n",
      "loss: 0.029426518827676773\n",
      "loss: 0.041784342378377914\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.00992365088313818\n",
      "==> online epoch # 614 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.044733595103025436\n",
      "loss: 0.03413749486207962\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.01649162918329239\n",
      "loss: 0.015107858926057816\n",
      "loss: 0.0\n",
      "loss: 0.0028431520331650972\n",
      "loss: 0.0012088189832866192\n",
      "loss: 0.0\n",
      "==> online epoch # 615 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.010410040616989136\n",
      "loss: 0.00604740297421813\n",
      "loss: 0.006009945645928383\n",
      "loss: 0.07452612370252609\n",
      "loss: 0.07049821317195892\n",
      "loss: 0.0\n",
      "loss: 0.016582604497671127\n",
      "loss: 0.013347937725484371\n",
      "loss: 0.04077707231044769\n",
      "loss: 0.0\n",
      "==> online epoch # 616 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0609886534512043\n",
      "loss: 0.005769022274762392\n",
      "loss: 0.0\n",
      "loss: 0.031202100217342377\n",
      "loss: 0.001988557167351246\n",
      "loss: 0.0646565854549408\n",
      "loss: 0.006593579426407814\n",
      "loss: 0.0\n",
      "loss: 0.026774251833558083\n",
      "loss: 0.0049089547246694565\n",
      "==> online epoch # 617 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.000600870291236788\n",
      "loss: 0.012293688021600246\n",
      "loss: 0.0258803628385067\n",
      "loss: 0.0019798087887465954\n",
      "loss: 0.10696033388376236\n",
      "loss: 0.0016399398446083069\n",
      "loss: 0.0053460425697267056\n",
      "loss: 0.029173798859119415\n",
      "loss: 0.003975244704633951\n",
      "loss: 0.0034478590823709965\n",
      "==> online epoch # 618 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.0\n",
      "loss: 0.004281740169972181\n",
      "loss: 0.0\n",
      "loss: 0.007266179192811251\n",
      "loss: 0.09632246941328049\n",
      "loss: 0.0\n",
      "loss: 0.004528735298663378\n",
      "loss: 0.0\n",
      "loss: 0.014046497642993927\n",
      "loss: 0.01586814783513546\n",
      "==> online epoch # 619 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.01447139959782362\n",
      "loss: 0.008166179992258549\n",
      "loss: 0.01941838674247265\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.01720479689538479\n",
      "loss: 0.006636294536292553\n",
      "loss: 0.050874147564172745\n",
      "loss: 0.005116308107972145\n",
      "loss: 0.03897079452872276\n",
      "==> online epoch # 620 [batchSize = 10] [learningRate = 0.01]\n",
      "loss: 0.014088538475334644\n",
      "loss: 0.004246184136718512\n",
      "loss: 0.07539209723472595\n",
      "loss: 0.0\n",
      "loss: 0.007937191985547543\n",
      "loss: 0.0\n",
      "loss: 0.02555900812149048\n",
      "loss: 0.0\n",
      "loss: 0.0\n",
      "loss: 0.008962058462202549\n",
      "feature dims: torch.Size([1000, 10])\n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  8\n",
      "Time elapsed for computing cluster affinity: 10.534818887710571 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 6\n",
      "==> testing\n",
      "NMI: 0.5254263281822205\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "initialize clusters...\n",
      "nclusters: 239\n",
      "==> testing\n",
      "NMI: 0.43198564648628235\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  239\n",
      "merge tensor(144) 147\n",
      "Time elapsed for computing cluster affinity: 17.272058963775635 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 190\n",
      "==> testing\n",
      "NMI: 0.4300713539123535\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  190\n",
      "Time elapsed for computing cluster affinity: 16.521433115005493 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 152\n",
      "==> testing\n",
      "NMI: 0.4283043444156647\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  152\n",
      "Time elapsed for computing cluster affinity: 16.06380605697632 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 121\n",
      "==> testing\n",
      "NMI: 0.42949602007865906\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  121\n",
      "Time elapsed for computing cluster affinity: 15.663035154342651 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 96\n",
      "==> testing\n",
      "NMI: 0.42366325855255127\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  96\n",
      "Time elapsed for computing cluster affinity: 15.64797592163086 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 76\n",
      "==> testing\n",
      "NMI: 0.42244747281074524\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  76\n",
      "Time elapsed for computing cluster affinity: 15.070976972579956 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 60\n",
      "==> testing\n",
      "NMI: 0.42831000685691833\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  60\n",
      "Time elapsed for computing cluster affinity: 14.934237957000732 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 48\n",
      "==> testing\n",
      "NMI: 0.43019604682922363\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  48\n",
      "Time elapsed for computing cluster affinity: 14.630876064300537 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 38\n",
      "==> testing\n",
      "NMI: 0.4419941008090973\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  38\n",
      "Time elapsed for computing cluster affinity: 14.270711183547974 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 30\n",
      "==> testing\n",
      "NMI: 0.436086505651474\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  30\n",
      "Time elapsed for computing cluster affinity: 13.450489044189453 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 24\n",
      "==> testing\n",
      "NMI: 0.4401068389415741\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  24\n",
      "Time elapsed for computing cluster affinity: 12.589805126190186 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 19\n",
      "==> testing\n",
      "NMI: 0.45504051446914673\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  19\n",
      "Time elapsed for computing cluster affinity: 12.383321046829224 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 15\n",
      "==> testing\n",
      "NMI: 0.47428345680236816\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  15\n",
      "Time elapsed for computing cluster affinity: 10.201603889465332 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 12\n",
      "==> testing\n",
      "NMI: 0.4952477812767029\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  12\n",
      "Time elapsed for computing cluster affinity: 10.046306133270264 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 9\n",
      "==> testing\n",
      "NMI: 0.49383917450904846\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  9\n",
      "Time elapsed for computing cluster affinity: 9.88473916053772 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 7\n",
      "==> testing\n",
      "NMI: 0.5147845149040222\n",
      " \n",
      "sigma: tensor(0.0850, dtype=torch.float64)\n",
      "nclusters:  7\n",
      "Time elapsed for computing cluster affinity: 7.8291051387786865 seconds\n",
      "run agglomerative clustering...\n",
      "nclusters: 6\n",
      "==> testing\n",
      "NMI: 0.518498420715332\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Train multi-attribute discovery models\n",
    "for _ in range(opt.epoch_rnn):\n",
    "    for i in range(opt.epoch_max + 1):\n",
    "        if i % opt.epoch_pp == 0:\n",
    "            merge_labels(network_table, epoch_reset_labels, train_data)\n",
    "            eval_perf()\n",
    "            if is_allfinished():\n",
    "                break\n",
    "        if opt.updateCNN == 1:\n",
    "            update_CNN()\n",
    "    epoch_reset_labels = [0] * num_networks\n",
    "    while True:\n",
    "        merge_labels_final()\n",
    "        eval_perf()\n",
    "        if is_allfinished():\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: [[0, 177, 247, 350, 536, 773, 776, 768, 769, 771, 772, 774, 377, 998, 6, 358, 527, 609, 690, 694, 853, 992, 31, 180, 370, 700, 176, 995, 32, 67, 178, 246, 352, 357, 363, 369, 385, 693, 696, 996, 997, 179, 537, 689, 852, 353, 354, 355, 372, 553, 577, 15, 245, 381, 530, 533, 697, 712, 713, 849, 855, 367, 545, 718, 371, 541, 563, 858, 24, 34, 27, 380, 698, 889, 551, 701, 705, 869, 884, 887, 249, 434, 523, 564, 691, 778, 388, 539, 557, 558, 576, 526, 994, 183, 351, 368, 851, 25, 30, 73, 579, 703, 704, 714, 727, 730, 885, 62, 63, 74, 383, 384, 859, 863, 865, 72, 716, 71, 379, 386, 392, 724, 732, 864, 1, 7, 240, 528, 359, 854, 529, 695, 918, 2, 3, 241, 919, 360, 542, 856, 692, 848, 850, 993, 248, 349, 991, 4, 11, 75, 187, 204, 532, 182, 525, 565, 999, 396, 707, 876, 882, 20, 378, 397, 550, 723, 862, 877, 13, 181, 213, 524, 531, 775, 861, 47, 538, 741, 897, 191, 709, 356, 534, 546, 547, 568, 43, 48, 192, 742, 49, 586, 898, 190, 554, 373, 375, 562, 40, 428, 763, 195, 432, 752, 755, 912, 232, 739, 58, 411, 747, 901, 390, 399, 406, 581, 717, 733, 750, 866, 905, 66, 569, 711, 561, 720, 890, 184, 208, 189, 209, 214, 391, 549, 570, 593, 870, 874, 572, 702, 883, 395, 567, 578, 868, 878, 880, 21, 544, 559, 607, 737, 745, 857, 871, 892, 56, 235, 597, 608, 916, 221, 243, 760, 605, 761, 764, 22, 710, 721, 746, 904, 52, 226, 228, 738, 894, 403, 413, 893, 902, 427, 765, 205, 405, 595, 748, 903, 215, 414, 583, 404, 552, 556, 588, 725, 875, 26, 28, 37, 59, 61, 224, 407, 584, 900, 57, 230, 419, 420, 592, 412, 417, 426, 430, 600, 914, 421, 423, 60, 415, 582, 585, 587, 872, 36, 234, 416, 425, 591, 599, 881, 899, 53, 431, 589, 602, 227, 229, 389, 64, 210, 222, 735, 891, 895, 906, 909, 29, 382, 566, 729, 731, 364, 699, 886, 398, 409, 560, 571, 575, 580, 603, 715, 734, 736, 757, 860, 44, 45, 758, 418, 424, 604, 422, 756, 910, 911, 233, 574, 726, 873, 429, 594, 598, 601, 596, 606, 753, 759, 915, 917, 410, 749, 879, 206, 223, 5, 9, 33, 201, 35, 38, 42, 194, 212, 394, 535, 908, 10, 51, 55, 387, 770, 18, 217, 193, 200, 211, 219, 393, 400, 401, 408, 744, 41, 50, 242, 743, 590, 907, 913, 39, 231, 402, 751, 762, 896, 46, 199, 216, 220, 555, 188, 207, 548, 719, 867, 186, 203, 225, 365, 706, 722, 14, 23, 362, 366, 433, 540, 766, 888, 8, 16, 17, 65, 68, 69, 70, 185, 197, 244, 361, 376, 543, 12, 19, 374, 708, 740, 754, 54, 196, 198, 218, 202, 573, 728, 76, 77, 236, 767, 78, 435, 777, 174, 237, 250, 781, 239, 348], [168, 172, 255], [269, 277, 438, 441, 81, 256, 93, 108, 273, 790, 151, 157, 325, 328, 263, 437, 452, 680, 920, 152, 521, 154, 310, 313, 319, 681, 684, 307, 308, 86, 159, 326, 686, 329, 492, 496, 989, 84, 85, 611, 682, 679, 683, 158, 306, 312, 265, 272, 275, 317, 324, 347, 847, 264, 266, 80, 276, 780, 258, 449, 688, 444, 612, 617, 624, 445, 447, 613, 622, 623, 685, 443, 614, 446, 448, 107, 783, 625, 633, 117, 508, 675, 846, 112, 124, 454, 494, 495, 511, 512, 630, 635, 786, 628, 629, 637, 670, 116, 122, 287, 615, 114, 289, 513, 514, 515, 785, 791, 793, 92, 678, 104, 105, 500, 504, 509, 516, 290, 674, 98, 115, 283, 676, 292, 510, 673, 845, 88, 110, 280, 518, 91, 498, 517, 106, 632, 787, 501, 503, 271, 305, 311, 318, 101, 102, 631, 784, 925, 103, 288, 497, 792, 794, 923, 123, 304, 493, 921, 522, 687, 990, 82, 96, 119, 282, 285, 90, 109, 505, 95, 274, 278, 506, 253, 279, 442, 618, 621, 788, 789, 89, 94, 111, 284, 286, 616, 620, 924, 118, 261, 281, 309, 315, 316, 507, 153, 260, 270, 322, 323, 83, 150, 97, 262, 267, 268, 519, 619, 120, 121, 490, 167, 254, 436, 440, 163, 164, 165, 166, 321, 520, 155, 160, 161, 320, 156, 327, 87, 252, 259, 439, 162, 171, 450, 782, 79, 251, 257, 314, 301, 502, 666, 667, 795, 844, 668, 669, 113, 672, 149, 302, 345, 489, 664, 665, 671, 988, 291, 303, 499, 677, 125, 296, 297, 330, 334, 638, 639, 640, 646, 647, 648, 649, 643, 644, 645, 148, 343, 344, 126, 146, 339, 127, 129, 130, 133, 135, 136, 139, 140, 144, 145, 335, 336, 663, 134, 138, 141, 142, 337, 338, 340, 131, 147, 331, 332, 341, 128, 333, 342, 468, 652, 132, 143, 642, 137, 467, 472, 641, 650, 651, 477, 478, 298, 465, 466, 469, 470, 475, 488, 655, 660, 796, 945, 471, 476, 659, 653, 656, 661, 798, 801, 808, 809, 473, 474, 654, 657, 799, 800, 804, 805, 806, 807, 813, 658, 944, 818, 946, 961, 966, 951, 954, 960, 965, 482, 483, 950, 957, 963, 964, 968, 481, 934, 949, 959, 952, 955, 956, 962, 969, 947, 953, 958, 967, 823, 935, 480, 484, 487, 948, 810, 814, 816, 817, 819, 802, 803, 812, 820, 821, 811, 815, 479, 662, 822, 975, 453, 634, 99, 100, 293, 346, 456, 463, 491, 626, 927, 460, 461, 458, 459, 294, 295, 455, 922, 627, 636, 457, 462, 464, 926, 299, 300, 797, 839, 840, 929, 824, 831, 833, 841, 928, 930, 837, 842, 931, 940, 941, 943, 985, 986, 987, 825, 827, 826, 828, 829, 835, 838, 832, 834, 843, 830, 836, 932, 942, 978, 984, 936, 937, 971, 977, 979, 938, 939, 972, 980, 981, 485, 486, 974, 933, 982, 970, 973, 976, 983], [173, 451], [175, 238, 610], [169, 170, 779]]\n"
     ]
    }
   ],
   "source": [
    "# 最終的なクラスタリング結果を取得\n",
    "final_clusters = label_pre_table_table\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "for cluster_id, cluster in enumerate(final_clusters):\n",
    "    print(f\"Cluster {cluster_id}: {cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945], [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989], [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 296, 297, 298, 299, 300, 301, 302, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967], [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999], [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919]]]\n"
     ]
    }
   ],
   "source": [
    "#正解ラベルの表示\n",
    "print(label_gt_table_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted list 0: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 781, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 991, 992, 993, 994, 995, 996, 997, 998, 999]\n",
      "Sorted list 1: [168, 172, 255]\n",
      "Sorted list 2: [79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 171, 251, 252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 780, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990]\n",
      "Sorted list 3: [173, 451]\n",
      "Sorted list 4: [175, 238, 610]\n",
      "Sorted list 5: [169, 170, 779]\n"
     ]
    }
   ],
   "source": [
    "sorted_data = []\n",
    "#print(final_clusters)\n",
    "for sublist in final_clusters[0]:\n",
    "    #print(sublist)\n",
    "    if isinstance(sublist, list):\n",
    "        sorted_data.append(sorted(sublist))\n",
    "#print(sorted_data)\n",
    "\n",
    "#予測結果の表示\n",
    "for i, sublist in enumerate(sorted_data):\n",
    "    print(f\"Sorted list {i}: {sublist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "print(target_nclusters_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
