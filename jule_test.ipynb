{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31865705 0.4338777  0.21977522 ... 0.89287638 0.03629183 0.7796883 ]\n",
      " [0.47630695 0.09403516 0.53630307 ... 0.7681944  0.59515131 0.77691244]\n",
      " [0.15424405 0.9565707  0.46636573 ... 0.91019906 0.45007233 0.53799707]\n",
      " ...\n",
      " [0.63843677 0.60129168 0.25473161 ... 0.53179527 0.33739622 0.10772266]\n",
      " [0.74774086 0.30308011 0.69932211 ... 0.90903129 0.17290222 0.0379777 ]\n",
      " [0.23531209 0.67237018 0.63000951 ... 0.1058342  0.4084622  0.7980099 ]]\n",
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 5 4 4 3 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = np.zeros((ns, ns))\n",
    "     # σ^2の計算\n",
    "    sigma_squared = (a / (ns * Ks)) * np.sum(\n",
    "        [np.sum(np.linalg.norm(X[i] - X[j])**2) for i in range(ns) for j in np.argsort(np.linalg.norm(X[i] - X, axis=1))[1:Ks+1]]\n",
    "    )\n",
    "    \n",
    "    for i in range(ns):\n",
    "        distances = np.linalg.norm(X[i] - X, axis=1)\n",
    "        nearest_indices = np.argsort(distances)[1:Ks+1]\n",
    "        #print(nearest_indices)\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = np.exp(-distances[j]**2 / (sigma_squared))\n",
    "    \n",
    "    return W\n",
    "\n",
    "def agglomerative_clustering(X, n_clusters,a):\n",
    "    # アフィニティ行列を計算する\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks,a)\n",
    "    \n",
    "    # 凝集型クラスタリングを実行する\n",
    "    Z = linkage(W, method='average')\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "def compute_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 使用例\n",
    "X = np.random.rand(100, 50)  # 100サンプル、各50特徴量\n",
    "print(X)\n",
    "n_clusters = 5\n",
    "a = 1.0  # パラメータa\n",
    "labels = agglomerative_clustering(X, n_clusters,a)\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argsort(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m Kc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     21\u001b[0m λ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mλ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m損失: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(X, clusters, W, Kc, λ)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Ci \u001b[38;5;129;01min\u001b[39;00m clusters:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_i \u001b[38;5;129;01min\u001b[39;00m Ci:\n\u001b[0;32m---> 38\u001b[0m         N_Kc_Ci \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39mKc:]  \u001b[38;5;66;03m# Top Kc nearest clusters\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         nearest_cluster \u001b[38;5;241m=\u001b[39m N_Kc_Ci[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;66;03m# Compute loss (7a)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: argsort(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 10   # クラスタ数（出力サイズ）\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "# 使用例\n",
    "#X = np.random.rand(100, 50)  # 100サンプル、各50特徴量\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "\n",
    "# クラスタ (ここでは仮に設定)\n",
    "clusters = [list(range(10)), list(range(10, 20)), list(range(20, 30)), list(range(30, 40)), list(range(40, 50))]\n",
    "\n",
    "# アフィニティ行列を計算\n",
    "W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "# 損失を計算 (式 7a, 7b)\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "loss = compute_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "print(f\"損失: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: -4.44038724899292\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    \n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    \n",
    "    return W\n",
    "\n",
    "def compute_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 10   # クラスタ数（出力サイズ）\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "\n",
    "# クラスタ (ここでは仮に設定)\n",
    "clusters = [list(range(10)), list(range(10, 20)), list(range(20, 30)), list(range(30, 40)), list(range(40, 50))]\n",
    "\n",
    "# アフィニティ行列を計算\n",
    "W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "# 損失を計算 (式 7a, 7b)\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "loss = compute_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "print(f\"損失: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 37 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m λ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# アルゴリズム1の実行\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m fcnn, rnn \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mλ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 114\u001b[0m, in \u001b[0;36mjoint_optimization\u001b[0;34m(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters)\u001b[0m\n\u001b[1;32m    111\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([label \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m clusters \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m cluster])\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# ステップ6: 累積損失を計算 (式6)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m time_step_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# ステップ7: クラスタ損失を計算 (式7)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m cluster_loss \u001b[38;5;241m=\u001b[39m compute_cluster_loss(X, clusters, W, Kc, λ)\n",
      "Cell \u001b[0;32mIn[28], line 51\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(outputs, targets, criterion)\u001b[0m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, targets):\n\u001b[0;32m---> 51\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/tstest/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tstest/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tstest/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 37 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.numpy(), method='average')\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters\n",
    "\n",
    "# アルゴリズム1に基づく最適化\n",
    "def joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters):\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # ステップ1: 深層表現を取得 (式5a)\n",
    "        X_t = fcnn(X)\n",
    "\n",
    "        # ステップ2: 初期隠れ状態を生成\n",
    "        batch_size = X.size(0)\n",
    "        hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "        # ステップ3: 時系列ごとの出力を収集\n",
    "        outputs = []\n",
    "        for t in range(batch_size):\n",
    "            output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "        W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "        # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "        clusters = update_clusters(X.detach(), num_clusters, a)\n",
    "        \n",
    "        # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "        targets = torch.tensor([label for cluster in clusters for label in cluster])\n",
    "\n",
    "        # ステップ6: 累積損失を計算 (式6)\n",
    "        time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "        # ステップ7: クラスタ損失を計算 (式7)\n",
    "        cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "        # ステップ8: 総合損失を計算 (式8)\n",
    "        total_loss = time_step_loss + cluster_loss\n",
    "\n",
    "        # ステップ9: パラメータの更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    return fcnn, rnn\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 5    # クラスタ数（出力サイズ）\n",
    "num_iterations = 100\n",
    "num_clusters = 5\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# アルゴリズム1の実行\n",
    "fcnn, rnn = joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, Total Loss: 76.28885650634766\n",
      "Iteration 2/100, Total Loss: 72.27485656738281\n",
      "Iteration 3/100, Total Loss: 68.57183837890625\n",
      "Iteration 4/100, Total Loss: 64.54376983642578\n",
      "Iteration 5/100, Total Loss: 59.83450698852539\n",
      "Iteration 6/100, Total Loss: 54.423988342285156\n",
      "Iteration 7/100, Total Loss: 48.74696350097656\n",
      "Iteration 8/100, Total Loss: 44.14325714111328\n",
      "Iteration 9/100, Total Loss: 42.14908218383789\n",
      "Iteration 10/100, Total Loss: 40.39509582519531\n",
      "Iteration 11/100, Total Loss: 36.87460708618164\n",
      "Iteration 12/100, Total Loss: 32.56963348388672\n",
      "Iteration 13/100, Total Loss: 28.73149871826172\n",
      "Iteration 14/100, Total Loss: 25.6488037109375\n",
      "Iteration 15/100, Total Loss: 22.779682159423828\n",
      "Iteration 16/100, Total Loss: 19.77930450439453\n",
      "Iteration 17/100, Total Loss: 16.701156616210938\n",
      "Iteration 18/100, Total Loss: 13.72177791595459\n",
      "Iteration 19/100, Total Loss: 10.99393081665039\n",
      "Iteration 20/100, Total Loss: 8.526632308959961\n",
      "Iteration 21/100, Total Loss: 6.307470321655273\n",
      "Iteration 22/100, Total Loss: 4.315258979797363\n",
      "Iteration 23/100, Total Loss: 2.5080299377441406\n",
      "Iteration 24/100, Total Loss: 0.8963117599487305\n",
      "Iteration 25/100, Total Loss: -0.48142576217651367\n",
      "Iteration 26/100, Total Loss: -1.609738826751709\n",
      "Iteration 27/100, Total Loss: -2.4908206462860107\n",
      "Iteration 28/100, Total Loss: -3.1171560287475586\n",
      "Iteration 29/100, Total Loss: -3.532209873199463\n",
      "Iteration 30/100, Total Loss: -3.7963948249816895\n",
      "Iteration 31/100, Total Loss: -3.9640963077545166\n",
      "Iteration 32/100, Total Loss: -4.079644680023193\n",
      "Iteration 33/100, Total Loss: -4.16151237487793\n",
      "Iteration 34/100, Total Loss: -4.21628999710083\n",
      "Iteration 35/100, Total Loss: -4.247507572174072\n",
      "Iteration 36/100, Total Loss: -4.26269006729126\n",
      "Iteration 37/100, Total Loss: -4.26894998550415\n",
      "Iteration 38/100, Total Loss: -4.272974967956543\n",
      "Iteration 39/100, Total Loss: -4.280724048614502\n",
      "Iteration 40/100, Total Loss: -4.290773391723633\n",
      "Iteration 41/100, Total Loss: -4.2983551025390625\n",
      "Iteration 42/100, Total Loss: -4.3030104637146\n",
      "Iteration 43/100, Total Loss: -4.30574893951416\n",
      "Iteration 44/100, Total Loss: -4.307219982147217\n",
      "Iteration 45/100, Total Loss: -4.307828426361084\n",
      "Iteration 46/100, Total Loss: -4.307853698730469\n",
      "Iteration 47/100, Total Loss: -4.30742883682251\n",
      "Iteration 48/100, Total Loss: -4.306735515594482\n",
      "Iteration 49/100, Total Loss: -4.306083679199219\n",
      "Iteration 50/100, Total Loss: -4.3058881759643555\n",
      "Iteration 51/100, Total Loss: -4.306386947631836\n",
      "Iteration 52/100, Total Loss: -4.307398796081543\n",
      "Iteration 53/100, Total Loss: -4.308505535125732\n",
      "Iteration 54/100, Total Loss: -4.309434413909912\n",
      "Iteration 55/100, Total Loss: -4.310115814208984\n",
      "Iteration 56/100, Total Loss: -4.310588836669922\n",
      "Iteration 57/100, Total Loss: -4.310912132263184\n",
      "Iteration 58/100, Total Loss: -4.311133861541748\n",
      "Iteration 59/100, Total Loss: -4.311286926269531\n",
      "Iteration 60/100, Total Loss: -4.311397075653076\n",
      "Iteration 61/100, Total Loss: -4.311476707458496\n",
      "Iteration 62/100, Total Loss: -4.311539173126221\n",
      "Iteration 63/100, Total Loss: -4.311586380004883\n",
      "Iteration 64/100, Total Loss: -4.311625003814697\n",
      "Iteration 65/100, Total Loss: -4.311657428741455\n",
      "Iteration 66/100, Total Loss: -4.311686992645264\n",
      "Iteration 67/100, Total Loss: -4.311712265014648\n",
      "Iteration 68/100, Total Loss: -4.311736583709717\n",
      "Iteration 69/100, Total Loss: -4.311758041381836\n",
      "Iteration 70/100, Total Loss: -4.311779022216797\n",
      "Iteration 71/100, Total Loss: -4.311797142028809\n",
      "Iteration 72/100, Total Loss: -4.31181526184082\n",
      "Iteration 73/100, Total Loss: -4.311831951141357\n",
      "Iteration 74/100, Total Loss: -4.31184720993042\n",
      "Iteration 75/100, Total Loss: -4.311861038208008\n",
      "Iteration 76/100, Total Loss: -4.311873435974121\n",
      "Iteration 77/100, Total Loss: -4.311885833740234\n",
      "Iteration 78/100, Total Loss: -4.311897277832031\n",
      "Iteration 79/100, Total Loss: -4.311906814575195\n",
      "Iteration 80/100, Total Loss: -4.311916828155518\n",
      "Iteration 81/100, Total Loss: -4.311925888061523\n",
      "Iteration 82/100, Total Loss: -4.311933994293213\n",
      "Iteration 83/100, Total Loss: -4.311941146850586\n",
      "Iteration 84/100, Total Loss: -4.311947822570801\n",
      "Iteration 85/100, Total Loss: -4.311953544616699\n",
      "Iteration 86/100, Total Loss: -4.311959266662598\n",
      "Iteration 87/100, Total Loss: -4.31196403503418\n",
      "Iteration 88/100, Total Loss: -4.311968803405762\n",
      "Iteration 89/100, Total Loss: -4.3119730949401855\n",
      "Iteration 90/100, Total Loss: -4.311976432800293\n",
      "Iteration 91/100, Total Loss: -4.311980247497559\n",
      "Iteration 92/100, Total Loss: -4.311983108520508\n",
      "Iteration 93/100, Total Loss: -4.311986446380615\n",
      "Iteration 94/100, Total Loss: -4.311988353729248\n",
      "Iteration 95/100, Total Loss: -4.311990737915039\n",
      "Iteration 96/100, Total Loss: -4.311992645263672\n",
      "Iteration 97/100, Total Loss: -4.311995029449463\n",
      "Iteration 98/100, Total Loss: -4.311996936798096\n",
      "Iteration 99/100, Total Loss: -4.3119988441467285\n",
      "Iteration 100/100, Total Loss: -4.312000274658203\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.numpy(), method='average')\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters, cluster_labels - 1  # クラスタラベルは0始まりにする\n",
    "\n",
    "# アルゴリズム1に基づく最適化\n",
    "def joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters):\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # ステップ1: 深層表現を取得 (式5a)\n",
    "        X_t = fcnn(X)\n",
    "\n",
    "        # ステップ2: 初期隠れ状態を生成\n",
    "        batch_size = X.size(0)\n",
    "        hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "        # ステップ3: 時系列ごとの出力を収集\n",
    "        outputs = []\n",
    "        for t in range(batch_size):\n",
    "            output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "        W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "        # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "        clusters, cluster_labels = update_clusters(X.detach(), num_clusters, a)\n",
    "        \n",
    "        # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "        targets = torch.tensor(cluster_labels, dtype=torch.long)\n",
    "\n",
    "        # ステップ6: 累積損失を計算 (式6)\n",
    "        time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "        # ステップ7: クラスタ損失を計算 (式7)\n",
    "        cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "        # ステップ8: 総合損失を計算 (式8)\n",
    "        total_loss = time_step_loss + cluster_loss\n",
    "\n",
    "        # ステップ9: パラメータの更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    return fcnn, rnn\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 5    # クラスタ数（出力サイズ）\n",
    "num_iterations = 100\n",
    "num_clusters = 5\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# アルゴリズム1の実行\n",
    "fcnn, rnn = joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, Total Loss: 77.10551452636719\n",
      "Iteration 2/100, Total Loss: 73.82721710205078\n",
      "Iteration 3/100, Total Loss: 71.30651092529297\n",
      "Iteration 4/100, Total Loss: 68.907470703125\n",
      "Iteration 5/100, Total Loss: 66.2588882446289\n",
      "Iteration 6/100, Total Loss: 63.071083068847656\n",
      "Iteration 7/100, Total Loss: 59.18975830078125\n",
      "Iteration 8/100, Total Loss: 54.600196838378906\n",
      "Iteration 9/100, Total Loss: 49.40264129638672\n",
      "Iteration 10/100, Total Loss: 44.141056060791016\n",
      "Iteration 11/100, Total Loss: 39.61823654174805\n",
      "Iteration 12/100, Total Loss: 35.71752166748047\n",
      "Iteration 13/100, Total Loss: 31.749624252319336\n",
      "Iteration 14/100, Total Loss: 27.48515510559082\n",
      "Iteration 15/100, Total Loss: 22.59560203552246\n",
      "Iteration 16/100, Total Loss: 17.604249954223633\n",
      "Iteration 17/100, Total Loss: 13.344947814941406\n",
      "Iteration 18/100, Total Loss: 9.96843147277832\n",
      "Iteration 19/100, Total Loss: 7.280675411224365\n",
      "Iteration 20/100, Total Loss: 5.061440944671631\n",
      "Iteration 21/100, Total Loss: 3.12581729888916\n",
      "Iteration 22/100, Total Loss: 1.4102797508239746\n",
      "Iteration 23/100, Total Loss: -0.05917835235595703\n",
      "Iteration 24/100, Total Loss: -1.27335524559021\n",
      "Iteration 25/100, Total Loss: -2.234563112258911\n",
      "Iteration 26/100, Total Loss: -2.9928760528564453\n",
      "Iteration 27/100, Total Loss: -3.558177947998047\n",
      "Iteration 28/100, Total Loss: -3.9550845623016357\n",
      "Iteration 29/100, Total Loss: -4.21319580078125\n",
      "Iteration 30/100, Total Loss: -4.3686842918396\n",
      "Iteration 31/100, Total Loss: -4.455507755279541\n",
      "Iteration 32/100, Total Loss: -4.501408100128174\n",
      "Iteration 33/100, Total Loss: -4.524622440338135\n",
      "Iteration 34/100, Total Loss: -4.537415504455566\n",
      "Iteration 35/100, Total Loss: -4.547805309295654\n",
      "Iteration 36/100, Total Loss: -4.560103893280029\n",
      "Iteration 37/100, Total Loss: -4.574100494384766\n",
      "Iteration 38/100, Total Loss: -4.585317134857178\n",
      "Iteration 39/100, Total Loss: -4.592546463012695\n",
      "Iteration 40/100, Total Loss: -4.596330165863037\n",
      "Iteration 41/100, Total Loss: -4.5980353355407715\n",
      "Iteration 42/100, Total Loss: -4.599349021911621\n",
      "Iteration 43/100, Total Loss: -4.601522922515869\n",
      "Iteration 44/100, Total Loss: -4.603872776031494\n",
      "Iteration 45/100, Total Loss: -4.605147838592529\n",
      "Iteration 46/100, Total Loss: -4.6050238609313965\n",
      "Iteration 47/100, Total Loss: -4.604354381561279\n",
      "Iteration 48/100, Total Loss: -4.605301380157471\n",
      "Iteration 49/100, Total Loss: -4.607551574707031\n",
      "Iteration 50/100, Total Loss: -4.6092119216918945\n",
      "Iteration 51/100, Total Loss: -4.609986782073975\n",
      "Iteration 52/100, Total Loss: -4.610249996185303\n",
      "Iteration 53/100, Total Loss: -4.610254287719727\n",
      "Iteration 54/100, Total Loss: -4.610127925872803\n",
      "Iteration 55/100, Total Loss: -4.609943866729736\n",
      "Iteration 56/100, Total Loss: -4.609752178192139\n",
      "Iteration 57/100, Total Loss: -4.609603404998779\n",
      "Iteration 58/100, Total Loss: -4.609525203704834\n",
      "Iteration 59/100, Total Loss: -4.609532356262207\n",
      "Iteration 60/100, Total Loss: -4.609618663787842\n",
      "Iteration 61/100, Total Loss: -4.609766483306885\n",
      "Iteration 62/100, Total Loss: -4.609949111938477\n",
      "Iteration 63/100, Total Loss: -4.610142707824707\n",
      "Iteration 64/100, Total Loss: -4.610330104827881\n",
      "Iteration 65/100, Total Loss: -4.610499858856201\n",
      "Iteration 66/100, Total Loss: -4.610648155212402\n",
      "Iteration 67/100, Total Loss: -4.610772132873535\n",
      "Iteration 68/100, Total Loss: -4.610875129699707\n",
      "Iteration 69/100, Total Loss: -4.610958576202393\n",
      "Iteration 70/100, Total Loss: -4.611025333404541\n",
      "Iteration 71/100, Total Loss: -4.611079216003418\n",
      "Iteration 72/100, Total Loss: -4.61112117767334\n",
      "Iteration 73/100, Total Loss: -4.611154079437256\n",
      "Iteration 74/100, Total Loss: -4.611179351806641\n",
      "Iteration 75/100, Total Loss: -4.611198902130127\n",
      "Iteration 76/100, Total Loss: -4.611213684082031\n",
      "Iteration 77/100, Total Loss: -4.611225128173828\n",
      "Iteration 78/100, Total Loss: -4.611233234405518\n",
      "Iteration 79/100, Total Loss: -4.611238956451416\n",
      "Iteration 80/100, Total Loss: -4.611242771148682\n",
      "Iteration 81/100, Total Loss: -4.611245155334473\n",
      "Iteration 82/100, Total Loss: -4.611247539520264\n",
      "Iteration 83/100, Total Loss: -4.611248016357422\n",
      "Iteration 84/100, Total Loss: -4.611248970031738\n",
      "Iteration 85/100, Total Loss: -4.61124849319458\n",
      "Iteration 86/100, Total Loss: -4.61124849319458\n",
      "Iteration 87/100, Total Loss: -4.611248016357422\n",
      "Iteration 88/100, Total Loss: -4.611248016357422\n",
      "Iteration 89/100, Total Loss: -4.61124849319458\n",
      "Iteration 90/100, Total Loss: -4.611248970031738\n",
      "Iteration 91/100, Total Loss: -4.611248970031738\n",
      "Iteration 92/100, Total Loss: -4.611249923706055\n",
      "Iteration 93/100, Total Loss: -4.611250877380371\n",
      "Iteration 94/100, Total Loss: -4.611252307891846\n",
      "Iteration 95/100, Total Loss: -4.6112542152404785\n",
      "Iteration 96/100, Total Loss: -4.611255645751953\n",
      "Iteration 97/100, Total Loss: -4.611257553100586\n",
      "Iteration 98/100, Total Loss: -4.6112589836120605\n",
      "Iteration 99/100, Total Loss: -4.611261367797852\n",
      "Iteration 100/100, Total Loss: -4.611262321472168\n",
      "最終クラスタリング結果: [2 2 3 2 0 3 2 2 3 3 2 0 0 2 3 3 3 3 2 3 2 2 1 0 3 2 2 2 2 3 4 1 3 2 3 3 3\n",
      " 1 3 2 2 1 2 3 3 2 3 3 2 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.numpy(), method='average')\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters, cluster_labels - 1  # クラスタラベルは0始まりにする\n",
    "\n",
    "# アルゴリズム1に基づく最適化\n",
    "def joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters):\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "    final_clusters = None  # 最終クラスタリング結果を保存\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # ステップ1: 深層表現を取得 (式5a)\n",
    "        X_t = fcnn(X)\n",
    "\n",
    "        # ステップ2: 初期隠れ状態を生成\n",
    "        batch_size = X.size(0)\n",
    "        hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "        # ステップ3: 時系列ごとの出力を収集\n",
    "        outputs = []\n",
    "        for t in range(batch_size):\n",
    "            output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "        W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "        # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "        clusters, cluster_labels = update_clusters(X.detach(), num_clusters, a)\n",
    "        final_clusters = cluster_labels  # 最終クラスタリング結果を保存\n",
    "        \n",
    "        # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "        targets = torch.tensor(cluster_labels, dtype=torch.long)\n",
    "\n",
    "        # ステップ6: 累積損失を計算 (式6)\n",
    "        time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "        # ステップ7: クラスタ損失を計算 (式7)\n",
    "        cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "        # ステップ8: 総合損失を計算 (式8)\n",
    "        total_loss = time_step_loss + cluster_loss\n",
    "\n",
    "        # ステップ9: パラメータの更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    return fcnn, rnn, final_clusters\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 5    # クラスタ数（出力サイズ）\n",
    "num_iterations = 100\n",
    "num_clusters = 5\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# アルゴリズム1の実行\n",
    "fcnn, rnn, final_clusters = joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, num_clusters)\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "print(\"最終クラスタリング結果:\", final_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters determined: 5\n",
      "Iteration 1/100, Total Loss: 74.16905975341797\n",
      "Iteration 2/100, Total Loss: 69.88932037353516\n",
      "Iteration 3/100, Total Loss: 65.56550598144531\n",
      "Iteration 4/100, Total Loss: 60.562686920166016\n",
      "Iteration 5/100, Total Loss: 54.550880432128906\n",
      "Iteration 6/100, Total Loss: 47.29065704345703\n",
      "Iteration 7/100, Total Loss: 38.99746322631836\n",
      "Iteration 8/100, Total Loss: 31.132801055908203\n",
      "Iteration 9/100, Total Loss: 27.076061248779297\n",
      "Iteration 10/100, Total Loss: 28.47885513305664\n",
      "Iteration 11/100, Total Loss: 28.875816345214844\n",
      "Iteration 12/100, Total Loss: 26.326587677001953\n",
      "Iteration 13/100, Total Loss: 22.42275619506836\n",
      "Iteration 14/100, Total Loss: 18.799991607666016\n",
      "Iteration 15/100, Total Loss: 16.314483642578125\n",
      "Iteration 16/100, Total Loss: 14.936219215393066\n",
      "Iteration 17/100, Total Loss: 14.192179679870605\n",
      "Iteration 18/100, Total Loss: 13.52741527557373\n",
      "Iteration 19/100, Total Loss: 12.675494194030762\n",
      "Iteration 20/100, Total Loss: 11.589444160461426\n",
      "Iteration 21/100, Total Loss: 10.294286727905273\n",
      "Iteration 22/100, Total Loss: 8.879975318908691\n",
      "Iteration 23/100, Total Loss: 7.471163749694824\n",
      "Iteration 24/100, Total Loss: 6.125959396362305\n",
      "Iteration 25/100, Total Loss: 4.873162269592285\n",
      "Iteration 26/100, Total Loss: 3.720587730407715\n",
      "Iteration 27/100, Total Loss: 2.651477813720703\n",
      "Iteration 28/100, Total Loss: 1.6386938095092773\n",
      "Iteration 29/100, Total Loss: 0.6909866333007812\n",
      "Iteration 30/100, Total Loss: -0.1943964958190918\n",
      "Iteration 31/100, Total Loss: -1.0031161308288574\n",
      "Iteration 32/100, Total Loss: -1.7223098278045654\n",
      "Iteration 33/100, Total Loss: -2.3517515659332275\n",
      "Iteration 34/100, Total Loss: -2.877232074737549\n",
      "Iteration 35/100, Total Loss: -3.2977800369262695\n",
      "Iteration 36/100, Total Loss: -3.6212878227233887\n",
      "Iteration 37/100, Total Loss: -3.853161334991455\n",
      "Iteration 38/100, Total Loss: -4.016815662384033\n",
      "Iteration 39/100, Total Loss: -4.128136157989502\n",
      "Iteration 40/100, Total Loss: -4.2014360427856445\n",
      "Iteration 41/100, Total Loss: -4.2496161460876465\n",
      "Iteration 42/100, Total Loss: -4.28270959854126\n",
      "Iteration 43/100, Total Loss: -4.3067193031311035\n",
      "Iteration 44/100, Total Loss: -4.324792861938477\n",
      "Iteration 45/100, Total Loss: -4.338606357574463\n",
      "Iteration 46/100, Total Loss: -4.349386215209961\n",
      "Iteration 47/100, Total Loss: -4.3576340675354\n",
      "Iteration 48/100, Total Loss: -4.363845348358154\n",
      "Iteration 49/100, Total Loss: -4.368464469909668\n",
      "Iteration 50/100, Total Loss: -4.371872901916504\n",
      "Iteration 51/100, Total Loss: -4.37437629699707\n",
      "Iteration 52/100, Total Loss: -4.376206874847412\n",
      "Iteration 53/100, Total Loss: -4.377543926239014\n",
      "Iteration 54/100, Total Loss: -4.378527641296387\n",
      "Iteration 55/100, Total Loss: -4.379257678985596\n",
      "Iteration 56/100, Total Loss: -4.3798017501831055\n",
      "Iteration 57/100, Total Loss: -4.380210876464844\n",
      "Iteration 58/100, Total Loss: -4.380523681640625\n",
      "Iteration 59/100, Total Loss: -4.380762577056885\n",
      "Iteration 60/100, Total Loss: -4.3809494972229\n",
      "Iteration 61/100, Total Loss: -4.381098747253418\n",
      "Iteration 62/100, Total Loss: -4.381218910217285\n",
      "Iteration 63/100, Total Loss: -4.381319046020508\n",
      "Iteration 64/100, Total Loss: -4.381405830383301\n",
      "Iteration 65/100, Total Loss: -4.381484031677246\n",
      "Iteration 66/100, Total Loss: -4.381556987762451\n",
      "Iteration 67/100, Total Loss: -4.381626129150391\n",
      "Iteration 68/100, Total Loss: -4.381694793701172\n",
      "Iteration 69/100, Total Loss: -4.381762504577637\n",
      "Iteration 70/100, Total Loss: -4.381829738616943\n",
      "Iteration 71/100, Total Loss: -4.381897926330566\n",
      "Iteration 72/100, Total Loss: -4.381964683532715\n",
      "Iteration 73/100, Total Loss: -4.382030487060547\n",
      "Iteration 74/100, Total Loss: -4.382094383239746\n",
      "Iteration 75/100, Total Loss: -4.382157325744629\n",
      "Iteration 76/100, Total Loss: -4.382217884063721\n",
      "Iteration 77/100, Total Loss: -4.382275581359863\n",
      "Iteration 78/100, Total Loss: -4.382329940795898\n",
      "Iteration 79/100, Total Loss: -4.382380962371826\n",
      "Iteration 80/100, Total Loss: -4.382428169250488\n",
      "Iteration 81/100, Total Loss: -4.382473468780518\n",
      "Iteration 82/100, Total Loss: -4.3825154304504395\n",
      "Iteration 83/100, Total Loss: -4.382554054260254\n",
      "Iteration 84/100, Total Loss: -4.382588863372803\n",
      "Iteration 85/100, Total Loss: -4.382621765136719\n",
      "Iteration 86/100, Total Loss: -4.382651329040527\n",
      "Iteration 87/100, Total Loss: -4.382678508758545\n",
      "Iteration 88/100, Total Loss: -4.3827033042907715\n",
      "Iteration 89/100, Total Loss: -4.382726192474365\n",
      "Iteration 90/100, Total Loss: -4.382746696472168\n",
      "Iteration 91/100, Total Loss: -4.382765293121338\n",
      "Iteration 92/100, Total Loss: -4.382782936096191\n",
      "Iteration 93/100, Total Loss: -4.382798671722412\n",
      "Iteration 94/100, Total Loss: -4.382813453674316\n",
      "Iteration 95/100, Total Loss: -4.382827281951904\n",
      "Iteration 96/100, Total Loss: -4.382839202880859\n",
      "Iteration 97/100, Total Loss: -4.382850646972656\n",
      "Iteration 98/100, Total Loss: -4.3828606605529785\n",
      "Iteration 99/100, Total Loss: -4.382870197296143\n",
      "Iteration 100/100, Total Loss: -4.382879734039307\n",
      "最終クラスタリング結果: [3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 1 3 3 2 3 3 3 1 3 3 3 3 0 1 3 3 3 3\n",
      " 3 2 3 4 3 3 3 3 3 0 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.cpu().numpy(), method='average')  # .cpu()を追加してGPUからCPUに移動\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters, cluster_labels - 1  # クラスタラベルは0始まりにする\n",
    "\n",
    "# 最適なクラスタ数を決定\n",
    "def determine_optimal_clusters(X, max_clusters, a):\n",
    "    best_num_clusters = 2\n",
    "    best_silhouette_score = -1\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        clusters, cluster_labels = update_clusters(X, n_clusters, a)\n",
    "        if len(set(cluster_labels)) == 1:  # クラスタが一つに収束した場合はスキップ\n",
    "            continue\n",
    "        score = silhouette_score(X.cpu().numpy(), cluster_labels)  # .cpu()を追加してGPUからCPUに移動\n",
    "        if score > best_silhouette_score:\n",
    "            best_silhouette_score = score\n",
    "            best_num_clusters = n_clusters\n",
    "    return best_num_clusters\n",
    "\n",
    "# アルゴリズム1に基づく最適化\n",
    "def joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, max_clusters):\n",
    "    final_clusters = None  # 最終クラスタリング結果を保存\n",
    "\n",
    "    # 最適なクラスタ数を決定\n",
    "    #num_clusters = determine_optimal_clusters(X, max_clusters, a)\n",
    "    num_clusters =5\n",
    "    print(f\"Optimal number of clusters determined: {num_clusters}\")\n",
    "\n",
    "    # RNNの出力サイズを更新\n",
    "    rnn.i2o = nn.Linear(rnn.hidden_size, num_clusters)\n",
    "    rnn.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # ステップ1: 深層表現を取得 (式5a)\n",
    "        X_t = fcnn(X)\n",
    "\n",
    "        # ステップ2: 初期隠れ状態を生成\n",
    "        batch_size = X.size(0)\n",
    "        hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "        # ステップ3: 時系列ごとの出力を収集\n",
    "        outputs = []\n",
    "        for t in range(batch_size):\n",
    "            output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "        W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "        # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "        clusters, cluster_labels = update_clusters(X.detach(), num_clusters, a)\n",
    "        final_clusters = cluster_labels  # 最終クラスタリング結果を保存\n",
    "        \n",
    "        # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "        targets = torch.tensor(cluster_labels, dtype=torch.long)\n",
    "\n",
    "        # ステップ6: 累積損失を計算 (式6)\n",
    "        time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "        # ステップ7: クラスタ損失を計算 (式7)\n",
    "        cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "        # ステップ8: 総合損失を計算 (式8)\n",
    "        total_loss = time_step_loss + cluster_loss\n",
    "\n",
    "        # ステップ9: パラメータの更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    return fcnn, rnn, final_clusters\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "max_clusters = 10  # 最大クラスタ数\n",
    "num_iterations = 100\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "# 出力サイズは動的に設定されるため、初期値は仮設定\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=max_clusters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# アルゴリズム1の実行\n",
    "fcnn, rnn, final_clusters = joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, max_clusters)\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "print(\"最終クラスタリング結果:\", final_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of clusters: 5\n",
      "Period 1/5, Step 1/5, Total Loss: 77.14836883544922\n",
      "Period 1/5, Step 2/5, Total Loss: 154.29673767089844\n",
      "Period 1/5, Step 3/5, Total Loss: 231.44509887695312\n",
      "Period 1/5, Step 4/5, Total Loss: 308.5934753417969\n",
      "Period 1/5, Step 5/5, Total Loss: 385.7418518066406\n",
      "Period 2/5, Step 1/5, Total Loss: 74.82903289794922\n",
      "Period 2/5, Step 2/5, Total Loss: 149.65806579589844\n",
      "Period 2/5, Step 3/5, Total Loss: 224.48709106445312\n",
      "Period 2/5, Step 4/5, Total Loss: 299.3161315917969\n",
      "Period 2/5, Step 5/5, Total Loss: 374.1451721191406\n",
      "Period 3/5, Step 1/5, Total Loss: 76.27616882324219\n",
      "Period 3/5, Step 2/5, Total Loss: 152.55233764648438\n",
      "Period 3/5, Step 3/5, Total Loss: 228.82850646972656\n",
      "Period 3/5, Step 4/5, Total Loss: 305.10467529296875\n",
      "Period 3/5, Step 5/5, Total Loss: 381.380859375\n",
      "Period 4/5, Step 1/5, Total Loss: 74.69186401367188\n",
      "Period 4/5, Step 2/5, Total Loss: 149.38372802734375\n",
      "Period 4/5, Step 3/5, Total Loss: 224.07559204101562\n",
      "Period 4/5, Step 4/5, Total Loss: 298.7674560546875\n",
      "Period 4/5, Step 5/5, Total Loss: 373.4593200683594\n",
      "Period 5/5, Step 1/5, Total Loss: 75.38400268554688\n",
      "Period 5/5, Step 2/5, Total Loss: 150.76800537109375\n",
      "Period 5/5, Step 3/5, Total Loss: 226.15200805664062\n",
      "Period 5/5, Step 4/5, Total Loss: 301.5360107421875\n",
      "Period 5/5, Step 5/5, Total Loss: 376.9200134277344\n",
      "最終クラスタリング結果: [3 1 3 3 3 3 0 3 3 0 0 2 3 3 3 3 2 3 3 1 2 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 2 3 3 1 3 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.cpu().numpy(), method='average')  # .cpu()を追加してGPUからCPUに移動\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters, cluster_labels - 1  # クラスタラベルは0始まりにする\n",
    "\n",
    "# # 最適なクラスタ数を決定\n",
    "# def determine_optimal_clusters(X, max_clusters, a):\n",
    "#     best_num_clusters = 2\n",
    "#     best_silhouette_score = -1\n",
    "#     for n_clusters in range(2, max_clusters + 1):\n",
    "#         clusters, cluster_labels = update_clusters(X, n_clusters, a)\n",
    "#         if len(set(cluster_labels)) == 1:  # クラスタが一つに収束した場合はスキップ\n",
    "#             continue\n",
    "#         score = silhouette_score(X.cpu().numpy(), cluster_labels)  # .cpu()を追加してGPUからCPUに移動\n",
    "#         if score > best_silhouette_score:\n",
    "#             best_silhouette_score = score\n",
    "#             best_num_clusters = n_clusters\n",
    "#     return best_num_clusters\n",
    "\n",
    "# 部分展開の最適化\n",
    "def partial_unrolling_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_periods, unrolling_rate, max_clusters):\n",
    "    final_clusters = None  # 最終クラスタリング結果を保存\n",
    "\n",
    "    # 初期のクラスタ数\n",
    "    num_clusters = 5\n",
    "    print(f\"Initial number of clusters: {num_clusters}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "\n",
    "    period_length = lambda ns_c: int(np.ceil(unrolling_rate * ns_c))\n",
    "\n",
    "    for period in range(num_periods):\n",
    "        current_clusters, cluster_labels = update_clusters(X, num_clusters, a)\n",
    "        num_clusters = len(current_clusters)  # 現在のクラスタ数を更新\n",
    "\n",
    "        # RNNの出力サイズを更新\n",
    "        rnn.i2o = nn.Linear(rnn.hidden_size, num_clusters)\n",
    "        rnn.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # 順伝播：クラスタをマージ\n",
    "        total_loss = 0\n",
    "        for step in range(period_length(num_clusters)):\n",
    "            # ステップ1: 深層表現を取得 (式5a)\n",
    "            X_t = fcnn(X)\n",
    "\n",
    "            # ステップ2: 初期隠れ状態を生成\n",
    "            batch_size = X.size(0)\n",
    "            hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "            # ステップ3: 時系列ごとの出力を収集\n",
    "            outputs = []\n",
    "            for t in range(batch_size):\n",
    "                output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "                outputs.append(output)\n",
    "\n",
    "            # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "            W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "            # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "            clusters, cluster_labels = update_clusters(X.detach(), num_clusters, a)\n",
    "            final_clusters = cluster_labels  # 最終クラスタリング結果を保存\n",
    "            \n",
    "            # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "            targets = torch.tensor(cluster_labels, dtype=torch.long)\n",
    "\n",
    "            # ステップ6: 累積損失を計算 (式6)\n",
    "            time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "            # ステップ7: クラスタ損失を計算 (式7)\n",
    "            cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "            # ステップ8: 総合損失を計算 (式8)\n",
    "            total_loss += time_step_loss + cluster_loss\n",
    "\n",
    "            print(f\"Period {period + 1}/{num_periods}, Step {step + 1}/{period_length(num_clusters)}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        # 逆伝播：CNNパラメータを更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return fcnn, rnn, final_clusters\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "max_clusters = 10  # 最大クラスタ数\n",
    "num_periods = 5\n",
    "unrolling_rate = 0.9  # 部分展開の割合\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=max_clusters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# 部分展開最適化の実行\n",
    "fcnn, rnn, final_clusters = partial_unrolling_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_periods, unrolling_rate, max_clusters)\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "print(\"最終クラスタリング結果:\", final_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters determined: 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_labels() missing 1 required positional argument: 'iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 212\u001b[0m\n\u001b[1;32m    210\u001b[0m args\u001b[38;5;241m.\u001b[39mcentralize_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    211\u001b[0m args\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 212\u001b[0m fcnn, rnn, final_clusters \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mλ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# クラスタリング結果を表示\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最終クラスタリング結果:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_clusters)\n",
      "Cell \u001b[0;32mIn[34], line 184\u001b[0m, in \u001b[0;36mjoint_optimization\u001b[0;34m(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, max_clusters, args)\u001b[0m\n\u001b[1;32m    181\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# クラスラベルの更新とマージ\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     label_pre_tensor_table \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_reset_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_clusters\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fcnn, rnn, final_clusters\n",
      "Cell \u001b[0;32mIn[34], line 117\u001b[0m, in \u001b[0;36mmerge_labels\u001b[0;34m(network_table, epoch_reset_labels, target_nclusters_table, trainData_data, args)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    115\u001b[0m     features \u001b[38;5;241m=\u001b[39m features \u001b[38;5;241m/\u001b[39m features\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 117\u001b[0m label_pre_table_table\u001b[38;5;241m.\u001b[39mappend(\u001b[43mupdate_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_nclusters_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_reset_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    118\u001b[0m epoch_reset_labels[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    119\u001b[0m label_pre_tensor_table \u001b[38;5;241m=\u001b[39m [cvt2_tensor_labels(labels, \u001b[38;5;241m1\u001b[39m, trainData_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m label_pre_table_table]\n",
      "\u001b[0;31mTypeError\u001b[0m: update_labels() missing 1 required positional argument: 'iter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "# 式(3)と式(4)に基づくアフィニティ行列の計算\n",
    "def compute_affinity_matrix(X, Ks, a):\n",
    "    ns = X.shape[0]\n",
    "    W = torch.zeros((ns, ns))\n",
    "    for i in range(ns):\n",
    "        distances = torch.norm(X[i] - X, dim=1)\n",
    "        nearest_indices = torch.argsort(distances)[1:Ks+1]\n",
    "        for j in nearest_indices:\n",
    "            W[i, j] = torch.exp(-distances[j]**2 / (a * ns * Ks))\n",
    "    return W\n",
    "\n",
    "# 式(5)に基づくCNNとRNNの定義\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "# 累積損失の計算 (式6)\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# クラスタ間の損失の計算 (式7a, 7b)\n",
    "def compute_cluster_loss(X, clusters, W, Kc, λ):\n",
    "    loss = 0\n",
    "    for Ci in clusters:\n",
    "        for x_i in Ci:\n",
    "            N_Kc_Ci = torch.argsort(W[x_i])[-Kc:]  # Top Kc nearest clusters\n",
    "            nearest_cluster = N_Kc_Ci[0]\n",
    "            \n",
    "            # Compute loss (7a)\n",
    "            loss_7a = -W[x_i, nearest_cluster]\n",
    "            \n",
    "            # Compute loss (7b)\n",
    "            loss_7b = 0\n",
    "            for k in range(1, Kc):\n",
    "                loss_7b += (W[x_i, nearest_cluster] - W[x_i, N_Kc_Ci[k]])\n",
    "            loss_7b = -λ / (Kc - 1) * loss_7b\n",
    "            \n",
    "            loss += loss_7a + loss_7b\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 凝集型クラスタリングに基づくクラスタ割り当ての更新 (式11)\n",
    "def update_clusters(X, n_clusters, a):\n",
    "    Ks = 10  # 近傍数\n",
    "    W = compute_affinity_matrix(X, Ks, a)\n",
    "    Z = linkage(W.cpu().numpy(), method='average')  # .cpu()を追加してGPUからCPUに移動\n",
    "    cluster_labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    clusters = [[] for _ in range(n_clusters)]\n",
    "    for idx, label in enumerate(cluster_labels):\n",
    "        clusters[label - 1].append(idx)\n",
    "    return clusters, cluster_labels - 1  # クラスタラベルは0始まりにする\n",
    "\n",
    "# クラスラベルの更新\n",
    "def update_labels(features, label_pre, Ks, a, iter):\n",
    "    if features.shape[0] <= 1:\n",
    "        return label_pre\n",
    "    distances, indices, W = compute_affinity(features.numpy(), Ks, a)\n",
    "    W = csr_matrix(W)\n",
    "    if iter == 0:\n",
    "        print(\"initialize clusters...\")\n",
    "        n_components, labels = connected_components(csgraph=W, directed=False, return_labels=True)\n",
    "        label_pre = [np.where(labels == i)[0].tolist() for i in range(n_components)]\n",
    "        return label_pre\n",
    "    return label_pre\n",
    "\n",
    "# ラベルのマージ\n",
    "def merge_labels(network_table, epoch_reset_labels, target_nclusters_table, trainData_data, args):\n",
    "    label_pre_table_table = []\n",
    "    for i, model in enumerate(network_table):\n",
    "        if epoch_reset_labels[i] == 0 or args.updateCNN == 0:\n",
    "            features = trainData_data.float()\n",
    "        else:\n",
    "            features = extract_features(model, trainData_data)\n",
    "        \n",
    "        if args.centralize_feature == 1:\n",
    "            features -= features.mean(dim=0, keepdim=True)\n",
    "\n",
    "        if args.normalize == 1:\n",
    "            features = features / features.norm(p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        label_pre_table_table.append(update_labels(features, [], target_nclusters_table[i], epoch_reset_labels[i]))\n",
    "        epoch_reset_labels[i] += 1\n",
    "        label_pre_tensor_table = [cvt2_tensor_labels(labels, 1, trainData_data.size(0)) for labels in label_pre_table_table]\n",
    "    return label_pre_tensor_table\n",
    "\n",
    "# テンソルラベルへの変換\n",
    "def cvt2_tensor_labels(labels, ind_s, ind_e):\n",
    "    label_tensor = torch.zeros(ind_e - ind_s + 1, 1)\n",
    "    for i, label_group in enumerate(labels):\n",
    "        for j in label_group:\n",
    "            label_tensor[j, 0] = i + 1\n",
    "    return label_tensor\n",
    "\n",
    "# アルゴリズム1に基づく最適化\n",
    "def joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, max_clusters, args):\n",
    "    final_clusters = None  # 最終クラスタリング結果を保存\n",
    "    epoch_reset_labels = [0]\n",
    "\n",
    "    # 最適なクラスタ数を決定\n",
    "    num_clusters = determine_optimal_clusters(X, max_clusters, a)\n",
    "    print(f\"Optimal number of clusters determined: {num_clusters}\")\n",
    "\n",
    "    # RNNの出力サイズを更新\n",
    "    rnn.i2o = nn.Linear(rnn.hidden_size, num_clusters)\n",
    "    rnn.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(fcnn.parameters()) + list(rnn.parameters()))\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # ステップ1: 深層表現を取得 (式5a)\n",
    "        X_t = fcnn(X)\n",
    "\n",
    "        # ステップ2: 初期隠れ状態を生成\n",
    "        batch_size = X.size(0)\n",
    "        hidden = rnn.init_hidden(1)  # 修正：初期隠れ状態はバッチサイズ1で初期化\n",
    "\n",
    "        # ステップ3: 時系列ごとの出力を収集\n",
    "        outputs = []\n",
    "        for t in range(batch_size):\n",
    "            output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # ステップ4: アフィニティ行列を計算 (式3, 4)\n",
    "        W = compute_affinity_matrix(X, Ks, a)\n",
    "\n",
    "        # ステップ5: クラスタ割り当てを更新 (式11)\n",
    "        clusters, cluster_labels = update_clusters(X.detach(), num_clusters, a)\n",
    "        final_clusters = cluster_labels  # 最終クラスタリング結果を保存\n",
    "        \n",
    "        # ターゲットラベルを取得 (クラスタリングのラベルを仮定)\n",
    "        targets = torch.tensor(cluster_labels, dtype=torch.long)\n",
    "\n",
    "        # ステップ6: 累積損失を計算 (式6)\n",
    "        time_step_loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "        # ステップ7: クラスタ損失を計算 (式7)\n",
    "        cluster_loss = compute_cluster_loss(X, clusters, W, Kc, λ)\n",
    "\n",
    "        # ステップ8: 総合損失を計算 (式8)\n",
    "        total_loss = time_step_loss + cluster_loss\n",
    "\n",
    "        # ステップ9: パラメータの更新\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # クラスラベルの更新とマージ\n",
    "        label_pre_tensor_table = merge_labels([fcnn, rnn], epoch_reset_labels, [num_clusters], X, args)\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "    return fcnn, rnn, final_clusters\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "max_clusters = 10  # 最大クラスタ数\n",
    "num_iterations = 100\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "# 出力サイズは動的に設定されるため、初期値は仮設定\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=max_clusters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "Ks = 5  # 近傍数\n",
    "a = 1.0  # パラメータa\n",
    "Kc = 5\n",
    "λ = 1.0\n",
    "\n",
    "# アルゴリズム1の実行\n",
    "args = lambda: None\n",
    "args.updateCNN = 1\n",
    "args.centralize_feature = 1\n",
    "args.normalize = 1\n",
    "fcnn, rnn, final_clusters = joint_optimization(X, fcnn, rnn, criterion, Ks, a, Kc, λ, num_iterations, max_clusters, args)\n",
    "\n",
    "# クラスタリング結果を表示\n",
    "print(\"最終クラスタリング結果:\", final_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affinity(features, k_s):\n",
    "    n_samples = features.shape[0]\n",
    "    k_s = max(1, min(k_s, n_samples - 1))\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_s, algorithm='auto').fit(features)\n",
    "    distances, indices = nbrs.kneighbors(features)\n",
    "    \n",
    "    # Create a square matrix of shape (n_samples, n_samples)\n",
    "    W = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Fill the affinity matrix\n",
    "    for i in range(n_samples):\n",
    "        for j in range(k_s):\n",
    "            W[i, indices[i, j]] = np.exp(-distances[i, j] ** 2)\n",
    "    \n",
    "    return distances, indices, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 50 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 61\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_t: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_t\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tstest/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[0;32m---> 24\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2h(combined))\n\u001b[1;32m     26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2o(hidden))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 50 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "    \n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for t in range(len(outputs)):\n",
    "        loss += criterion(outputs[t], targets[t])\n",
    "    return loss\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 10   # クラスタ数（出力サイズ）\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "inputs = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "targets = torch.randint(0, output_size, (50,))  # 50個のターゲットラベル\n",
    "batch_size = inputs.size(0)\n",
    "\n",
    "# 時刻 t の初期隠れ状態を生成\n",
    "hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "# 時刻 t の処理 (式 5a)\n",
    "X_t = fcnn(inputs)\n",
    "\n",
    "# 時刻 t の隠れ状態を更新 (式 5b)\n",
    "outputs = []\n",
    "for t in range(batch_size):\n",
    "    output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "    outputs.append(output)\n",
    "\n",
    "print(f\"X_t: {X_t}\")\n",
    "print(f\"h_t: {hidden}\")\n",
    "print(f\"y_t: {outputs}\")\n",
    "# 損失の計算 (式 6)\n",
    "loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "print(f\"損失: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 50 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 各タイムステップでRNNを適用し、出力と隠れ状態を更新 (式 5b, 5c)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 63\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# ターゲットラベルを各タイムステップに対応する形に変換\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tstest/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, hidden):\n\u001b[0;32m---> 24\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2h(combined))\n\u001b[1;32m     26\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi2o(hidden))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 50 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.relu(self.i2h(combined))\n",
    "        output = self.softmax(self.i2o(hidden))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "def compute_loss(outputs, targets, criterion):\n",
    "    loss = 0\n",
    "    for output, target in zip(outputs, targets):\n",
    "        loss += criterion(output, target.unsqueeze(0))\n",
    "    return loss\n",
    "\n",
    "# 使用例\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "hidden_size = 128  # 隠れ層のサイズ\n",
    "output_size = 10   # クラスタ数（出力サイズ）\n",
    "\n",
    "fcnn = FCNN(input_size, hidden_size)\n",
    "rnn = RNN(input_size=hidden_size, hidden_size=hidden_size, output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 例の入力データ\n",
    "inputs = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "targets = torch.randint(0, output_size, (50,))  # 50個のターゲットラベル\n",
    "batch_size = inputs.size(0)\n",
    "\n",
    "# 初期隠れ状態を生成\n",
    "hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "# FCNNを通じて入力データを変換 (式 5a)\n",
    "X_t = fcnn(inputs)\n",
    "\n",
    "# タイムステップごとの出力を収集するリスト\n",
    "outputs = []\n",
    "\n",
    "# 各タイムステップでRNNを適用し、出力と隠れ状態を更新 (式 5b, 5c)\n",
    "for t in range(batch_size):\n",
    "    output, hidden = rnn(X_t[t].unsqueeze(0), hidden)\n",
    "    outputs.append(output)\n",
    "\n",
    "# ターゲットラベルを各タイムステップに対応する形に変換\n",
    "targets = targets.unsqueeze(1).expand(-1, batch_size)\n",
    "\n",
    "# 累積損失を計算 (式 6)\n",
    "loss = compute_loss(outputs, targets, criterion)\n",
    "\n",
    "print(f\"損失: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_c_a: 14 idx_c_b: 17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class AggClustering:\n",
    "    def __init__(self, K_c):\n",
    "        self.K_c = K_c\n",
    "\n",
    "    def search_clusters(self, A_s_t):\n",
    "        A_sorted, idx_sort = torch.sort(A_s_t, dim=0, descending=True)\n",
    "        aff = torch.zeros(1, A_sorted.size(1), dtype=torch.float32)\n",
    "\n",
    "        for i in range(A_sorted.size(1)):\n",
    "            aff[0, i] = A_sorted[0, i].item()\n",
    "            if A_sorted.size(1) > 100:\n",
    "                for k in range(1, self.K_c):\n",
    "                    aff[0, i] += (A_sorted[0, i] - A_sorted[k, i]).item() / (self.K_c - 1)\n",
    "\n",
    "        v_c, idx_c = torch.max(aff, dim=1)\n",
    "        idx_c_b = idx_c.item()\n",
    "        idx_c_a = idx_sort[0, idx_c_b].item()\n",
    "\n",
    "        if idx_c_a == idx_c_b:\n",
    "            raise ValueError(\"Error: idx_c_a and idx_c_b are the same\")\n",
    "        elif idx_c_a > idx_c_b:\n",
    "            idx_c_a, idx_c_b = idx_c_b, idx_c_a\n",
    "\n",
    "        return idx_c_a, idx_c_b\n",
    "\n",
    "# 使用例\n",
    "K_c = 10  # K_cの例\n",
    "agg_clustering = AggClustering(K_c)\n",
    "\n",
    "A_s_t = torch.rand(50, 50)  # アフィニティマトリックスの例\n",
    "idx_c_a, idx_c_b = agg_clustering.search_clusters(A_s_t)\n",
    "print(\"idx_c_a:\", idx_c_a, \"idx_c_b:\", idx_c_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 320])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# ベクトルデータの定義\n",
    "input_size = 320  # 入力ベクトルの次元\n",
    "X = torch.randn(50, input_size)  # 50個の320次元ベクトル\n",
    "y = torch.randint(0, 5, (50,))  # 0から4までのランダムなラベル\n",
    "\n",
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
